nohup: ignoring input
Tacotron2(
  (embedding): Embedding(37, 512)
  (gst): GST(
    (encoder): ReferenceEncoder(
      (convs): ModuleList(
        (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
      (bns): ModuleList(
        (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (gru): GRU(256, 256, batch_first=True)
    )
    (stl): StyleTokenLayer(
      (attention): MultiHeadAttention(
        (W_query): Linear(in_features=256, out_features=512, bias=False)
        (W_key): Linear(in_features=64, out_features=512, bias=False)
        (W_value): Linear(in_features=64, out_features=512, bias=False)
      )
    )
  )
  (ser): SER(
    (encoder): ReferenceEncoder(
      (convs): ModuleList(
        (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (1): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
        (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
      )
      (bns): ModuleList(
        (0): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (gru): GRU(256, 256, batch_first=True)
    )
    (stl): StyleTokenLayer(
      (attention): MultiHeadAttention(
        (W_query): Linear(in_features=256, out_features=512, bias=False)
        (W_key): Linear(in_features=64, out_features=512, bias=False)
        (W_value): Linear(in_features=64, out_features=512, bias=False)
      )
    )
    (classify): ClassifyLayer(
      (linear_layer): ModuleList(
        (0): Linear(in_features=512, out_features=5, bias=True)
      )
      (dropout_layer): ModuleList(
        (0): Dropout(p=0.2, inplace=False)
      )
    )
  )
  (encoder): Encoder(
    (convolutions): ModuleList(
      (0): Sequential(
        (0): ConvNorm(
          (conv): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))
        )
        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): Sequential(
        (0): ConvNorm(
          (conv): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))
        )
        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): Sequential(
        (0): ConvNorm(
          (conv): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))
        )
        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (lstm): LSTM(512, 256, batch_first=True, bidirectional=True)
  )
  (decoder): Decoder(
    (prenet): Prenet(
      (layers): ModuleList(
        (0): LinearNorm(
          (linear_layer): Linear(in_features=80, out_features=256, bias=False)
        )
        (1): LinearNorm(
          (linear_layer): Linear(in_features=256, out_features=256, bias=False)
        )
      )
    )
    (attention_rnn): LSTMCell(768, 1024)
    (attention_layer): Attention(
      (query_layer): LinearNorm(
        (linear_layer): Linear(in_features=1024, out_features=128, bias=False)
      )
      (memory_layer): LinearNorm(
        (linear_layer): Linear(in_features=512, out_features=128, bias=False)
      )
      (v): LinearNorm(
        (linear_layer): Linear(in_features=128, out_features=1, bias=False)
      )
      (location_layer): LocationLayer(
        (location_conv): ConvNorm(
          (conv): Conv1d(2, 32, kernel_size=(31,), stride=(1,), padding=(15,), bias=False)
        )
        (location_dense): LinearNorm(
          (linear_layer): Linear(in_features=32, out_features=128, bias=False)
        )
      )
    )
    (decoder_rnn): LSTMCell(1536, 1024, bias=1)
    (linear_projection): LinearNorm(
      (linear_layer): Linear(in_features=1536, out_features=80, bias=True)
    )
    (gate_layer): LinearNorm(
      (linear_layer): Linear(in_features=1536, out_features=1, bias=True)
    )
  )
  (postnet): Postnet(
    (convolutions): ModuleList(
      (0): Sequential(
        (0): ConvNorm(
          (conv): Conv1d(80, 512, kernel_size=(5,), stride=(1,), padding=(2,))
        )
        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (1): Sequential(
        (0): ConvNorm(
          (conv): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))
        )
        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): Sequential(
        (0): ConvNorm(
          (conv): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))
        )
        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): Sequential(
        (0): ConvNorm(
          (conv): Conv1d(512, 512, kernel_size=(5,), stride=(1,), padding=(2,))
        )
        (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (4): Sequential(
        (0): ConvNorm(
          (conv): Conv1d(512, 80, kernel_size=(5,), stride=(1,), padding=(2,))
        )
        (1): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
)
total params: 29897542
trainable params: 29897542
/media/cv516/8e2bd071-88a1-41bb-a3dc-b7eadc0c192e/cv516/ceiling_workspace/renjie/GST-Tacotron2/models/stft.py:62: FutureWarning: Pass size=1024 as keyword args. From version 0.10 passing these as positional arguments will result in an error
  fft_window = pad_center(fft_window, filter_length)
/media/cv516/8e2bd071-88a1-41bb-a3dc-b7eadc0c192e/cv516/ceiling_workspace/renjie/GST-Tacotron2/models/layers.py:62: FutureWarning: Pass sr=48000, n_fft=1024, n_mels=80, fmin=0.0, fmax=8000.0 as keyword args. From version 0.10 passing these as positional arguments will result in an error
  sampling_rate, filter_length, n_mel_channels, mel_fmin, mel_fmax)
/media/cv516/8e2bd071-88a1-41bb-a3dc-b7eadc0c192e/cv516/ceiling_workspace/renjie/GST-Tacotron2/utils.py:146: FutureWarning: Pass sr=48000 as keyword args. From version 0.10 passing these as positional arguments will result in an error
  y, sr = librosa.load(full_path, sampling_rate)
/media/cv516/8e2bd071-88a1-41bb-a3dc-b7eadc0c192e/cv516/ceiling_workspace/renjie/GST-Tacotron2/utils.py:146: FutureWarning: Pass sr=48000 as keyword args. From version 0.10 passing these as positional arguments will result in an error
  y, sr = librosa.load(full_path, sampling_rate)
/media/cv516/8e2bd071-88a1-41bb-a3dc-b7eadc0c192e/cv516/ceiling_workspace/renjie/GST-Tacotron2/utils.py:146: FutureWarning: Pass sr=48000 as keyword args. From version 0.10 passing these as positional arguments will result in an error
  y, sr = librosa.load(full_path, sampling_rate)
/media/cv516/8e2bd071-88a1-41bb-a3dc-b7eadc0c192e/cv516/ceiling_workspace/renjie/GST-Tacotron2/utils.py:146: FutureWarning: Pass sr=48000 as keyword args. From version 0.10 passing these as positional arguments will result in an error
  y, sr = librosa.load(full_path, sampling_rate)
2022-05-06 16:51:17,234 INFO 	Epoch: [0][0/309]	Batch time 7.3687 (7.3687)	Loss 50.60423 (50.60423)
2022-05-06 16:51:52,990 INFO 	Epoch: [0][10/309]	Batch time 3.1636 (3.9204)	Loss 6.79415 (15.60543)
2022-05-06 16:52:29,157 INFO 	Epoch: [0][20/309]	Batch time 3.6375 (3.7758)	Loss 5.47490 (10.80879)
2022-05-06 16:53:06,937 INFO 	Epoch: [0][30/309]	Batch time 3.8635 (3.7765)	Loss 5.00128 (8.88208)
2022-05-06 16:53:43,939 INFO 	Epoch: [0][40/309]	Batch time 3.6634 (3.7579)	Loss 4.05287 (7.77329)
2022-05-06 16:54:21,454 INFO 	Epoch: [0][50/309]	Batch time 3.0492 (3.7567)	Loss 3.44236 (6.93404)
2022-05-06 16:54:58,704 INFO 	Epoch: [0][60/309]	Batch time 3.3905 (3.7515)	Loss 2.93030 (6.29866)
2022-05-06 16:55:38,322 INFO 	Epoch: [0][70/309]	Batch time 4.0024 (3.7811)	Loss 2.59351 (5.79153)
2022-05-06 16:56:15,608 INFO 	Epoch: [0][80/309]	Batch time 3.4807 (3.7746)	Loss 2.78847 (5.41636)
2022-05-06 16:56:53,584 INFO 	Epoch: [0][90/309]	Batch time 3.4758 (3.7771)	Loss 2.83432 (5.10335)
2022-05-06 16:57:32,069 INFO 	Epoch: [0][100/309]	Batch time 3.9104 (3.7842)	Loss 2.51195 (4.85340)
2022-05-06 16:58:09,736 INFO 	Epoch: [0][110/309]	Batch time 3.6989 (3.7826)	Loss 2.23607 (4.63031)
2022-05-06 16:58:48,099 INFO 	Epoch: [0][120/309]	Batch time 4.3332 (3.7871)	Loss 2.10456 (4.43555)
2022-05-06 16:59:26,905 INFO 	Epoch: [0][130/309]	Batch time 4.0700 (3.7942)	Loss 2.51197 (4.27771)
2022-05-06 17:00:04,962 INFO 	Epoch: [0][140/309]	Batch time 4.1735 (3.7950)	Loss 2.27810 (4.14201)
2022-05-06 17:00:43,396 INFO 	Epoch: [0][150/309]	Batch time 4.3716 (3.7982)	Loss 2.14705 (4.02468)
2022-05-06 17:01:24,887 INFO 	Epoch: [0][160/309]	Batch time 3.8030 (3.8200)	Loss 2.04439 (3.91213)
2022-05-06 17:02:01,948 INFO 	Epoch: [0][170/309]	Batch time 4.1751 (3.8134)	Loss 2.09393 (3.81694)
2022-05-06 17:02:40,817 INFO 	Epoch: [0][180/309]	Batch time 3.7403 (3.8174)	Loss 2.31214 (3.72692)
2022-05-06 17:03:20,375 INFO 	Epoch: [0][190/309]	Batch time 3.6306 (3.8247)	Loss 2.19679 (3.64402)
2022-05-06 17:03:58,032 INFO 	Epoch: [0][200/309]	Batch time 3.7827 (3.8217)	Loss 2.19586 (3.56795)
2022-05-06 17:04:37,352 INFO 	Epoch: [0][210/309]	Batch time 3.7189 (3.8270)	Loss 2.21320 (3.49552)
2022-05-06 17:05:14,648 INFO 	Epoch: [0][220/309]	Batch time 3.7998 (3.8225)	Loss 1.85265 (3.42367)
2022-05-06 17:05:53,420 INFO 	Epoch: [0][230/309]	Batch time 4.0573 (3.8249)	Loss 1.92477 (3.35851)
2022-05-06 17:06:33,243 INFO 	Epoch: [0][240/309]	Batch time 4.1016 (3.8314)	Loss 1.87109 (3.29351)
2022-05-06 17:07:12,225 INFO 	Epoch: [0][250/309]	Batch time 3.9775 (3.8341)	Loss 1.76474 (3.23371)
2022-05-06 17:07:50,622 INFO 	Epoch: [0][260/309]	Batch time 3.6808 (3.8343)	Loss 1.84220 (3.17897)
2022-05-06 17:08:28,853 INFO 	Epoch: [0][270/309]	Batch time 3.6967 (3.8339)	Loss 1.69646 (3.12779)
2022-05-06 17:09:08,820 INFO 	Epoch: [0][280/309]	Batch time 3.8098 (3.8397)	Loss 1.71145 (3.07422)
2022-05-06 17:09:48,113 INFO 	Epoch: [0][290/309]	Batch time 3.9090 (3.8428)	Loss 1.66679 (3.02678)
2022-05-06 17:10:26,736 INFO 	Epoch: [0][300/309]	Batch time 3.8956 (3.8434)	Loss 1.56998 (2.98186)

Learning rate: 0.001
Step num: 309

/media/cv516/8e2bd071-88a1-41bb-a3dc-b7eadc0c192e/cv516/ceiling_workspace/renjie/GST-Tacotron2/utils.py:146: FutureWarning: Pass sr=48000 as keyword args. From version 0.10 passing these as positional arguments will result in an error
  y, sr = librosa.load(full_path, sampling_rate)
/media/cv516/8e2bd071-88a1-41bb-a3dc-b7eadc0c192e/cv516/ceiling_workspace/renjie/GST-Tacotron2/utils.py:146: FutureWarning: Pass sr=48000 as keyword args. From version 0.10 passing these as positional arguments will result in an error
  y, sr = librosa.load(full_path, sampling_rate)
/media/cv516/8e2bd071-88a1-41bb-a3dc-b7eadc0c192e/cv516/ceiling_workspace/renjie/GST-Tacotron2/utils.py:146: FutureWarning: Pass sr=48000 as keyword args. From version 0.10 passing these as positional arguments will result in an error
  y, sr = librosa.load(full_path, sampling_rate)
2022-05-06 17:11:04,119 INFO 	
Validation Loss 1.59061 (1.51803)

/media/cv516/8e2bd071-88a1-41bb-a3dc-b7eadc0c192e/cv516/ceiling_workspace/renjie/GST-Tacotron2/utils.py:146: FutureWarning: Pass sr=48000 as keyword args. From version 0.10 passing these as positional arguments will result in an error
  y, sr = librosa.load(full_path, sampling_rate)
Warning! Reached max decoder steps
/home/cv516/anaconda3/envs/renjie/lib/python3.7/site-packages/torch/serialization.py:656: SourceChangeWarning: source code of class 'torch.nn.modules.conv.ConvTranspose1d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.
  warnings.warn(msg, SourceChangeWarning)
/home/cv516/anaconda3/envs/renjie/lib/python3.7/site-packages/torch/serialization.py:656: SourceChangeWarning: source code of class 'torch.nn.modules.container.ModuleList' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.
  warnings.warn(msg, SourceChangeWarning)
/home/cv516/anaconda3/envs/renjie/lib/python3.7/site-packages/torch/serialization.py:656: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv1d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.
  warnings.warn(msg, SourceChangeWarning)
save mel done
2022-05-06 17:11:15,856 INFO 	Epoch: [1][0/309]	Batch time 6.7929 (6.7929)	Loss 1.49304 (1.49304)
2022-05-06 17:11:55,707 INFO 	Epoch: [1][10/309]	Batch time 3.4435 (4.2404)	Loss 1.67427 (1.55982)
2022-05-06 17:12:35,417 INFO 	Epoch: [1][20/309]	Batch time 3.8356 (4.1121)	Loss 1.60154 (1.57094)
2022-05-06 17:13:14,671 INFO 	Epoch: [1][30/309]	Batch time 3.8492 (4.0519)	Loss 1.41987 (1.58104)
2022-05-06 17:13:54,840 INFO 	Epoch: [1][40/309]	Batch time 4.0289 (4.0433)	Loss 1.49598 (1.57800)
2022-05-06 17:14:36,142 INFO 	Epoch: [1][50/309]	Batch time 4.9726 (4.0604)	Loss 1.18931 (1.56157)
2022-05-06 17:15:17,083 INFO 	Epoch: [1][60/309]	Batch time 4.0481 (4.0659)	Loss 1.71401 (1.55758)
2022-05-06 17:15:58,848 INFO 	Epoch: [1][70/309]	Batch time 4.2265 (4.0815)	Loss 1.49387 (1.53947)
2022-05-06 17:16:40,542 INFO 	Epoch: [1][80/309]	Batch time 3.8618 (4.0923)	Loss 1.66987 (1.52824)
2022-05-06 17:17:22,540 INFO 	Epoch: [1][90/309]	Batch time 4.1787 (4.1041)	Loss 1.49177 (1.51310)
2022-05-06 17:18:05,394 INFO 	Epoch: [1][100/309]	Batch time 3.5496 (4.1221)	Loss 1.60090 (1.50840)
2022-05-06 17:18:43,159 INFO 	Epoch: [1][110/309]	Batch time 3.8785 (4.0910)	Loss 1.28731 (1.50292)
2022-05-06 17:19:24,222 INFO 	Epoch: [1][120/309]	Batch time 4.2705 (4.0922)	Loss 1.33819 (1.49346)
2022-05-06 17:20:04,230 INFO 	Epoch: [1][130/309]	Batch time 4.0719 (4.0852)	Loss 1.33218 (1.49096)
2022-05-06 17:20:44,001 INFO 	Epoch: [1][140/309]	Batch time 4.0001 (4.0776)	Loss 1.52496 (1.48940)
2022-05-06 17:21:24,654 INFO 	Epoch: [1][150/309]	Batch time 3.9957 (4.0768)	Loss 1.26444 (1.48270)
2022-05-06 17:22:03,550 INFO 	Epoch: [1][160/309]	Batch time 4.3262 (4.0651)	Loss 1.17680 (1.47796)
2022-05-06 17:22:45,012 INFO 	Epoch: [1][170/309]	Batch time 4.1365 (4.0699)	Loss 1.34290 (1.46819)
2022-05-06 17:23:24,118 INFO 	Epoch: [1][180/309]	Batch time 3.7933 (4.0611)	Loss 1.56193 (1.46284)
2022-05-06 17:24:03,972 INFO 	Epoch: [1][190/309]	Batch time 3.6743 (4.0571)	Loss 1.34406 (1.45850)
2022-05-06 17:24:42,440 INFO 	Epoch: [1][200/309]	Batch time 3.9671 (4.0467)	Loss 1.27620 (1.45546)
2022-05-06 17:25:22,351 INFO 	Epoch: [1][210/309]	Batch time 4.0207 (4.0440)	Loss 1.23614 (1.44908)
2022-05-06 17:26:02,353 INFO 	Epoch: [1][220/309]	Batch time 4.0997 (4.0420)	Loss 1.22895 (1.44361)
2022-05-06 17:26:43,018 INFO 	Epoch: [1][230/309]	Batch time 3.4990 (4.0431)	Loss 1.45490 (1.43772)
2022-05-06 17:27:24,344 INFO 	Epoch: [1][240/309]	Batch time 3.8305 (4.0468)	Loss 1.29660 (1.43203)
2022-05-06 17:28:06,501 INFO 	Epoch: [1][250/309]	Batch time 3.9258 (4.0535)	Loss 1.41125 (1.42570)
2022-05-06 17:28:47,979 INFO 	Epoch: [1][260/309]	Batch time 4.5211 (4.0572)	Loss 1.04021 (1.42102)
2022-05-06 17:29:27,958 INFO 	Epoch: [1][270/309]	Batch time 3.9233 (4.0550)	Loss 1.30768 (1.41801)
2022-05-06 17:30:08,146 INFO 	Epoch: [1][280/309]	Batch time 4.1281 (4.0537)	Loss 1.15574 (1.41313)
2022-05-06 17:30:47,993 INFO 	Epoch: [1][290/309]	Batch time 4.2214 (4.0513)	Loss 1.18260 (1.40765)
2022-05-06 17:31:28,215 INFO 	Epoch: [1][300/309]	Batch time 3.6168 (4.0503)	Loss 1.30852 (1.40134)

Learning rate: 0.001
Step num: 618

2022-05-06 17:32:05,805 INFO 	
Validation Loss 1.25520 (1.19645)

Warning! Reached max decoder steps
save mel done
2022-05-06 17:32:21,087 INFO 	Epoch: [2][0/309]	Batch time 6.7731 (6.7731)	Loss 1.30354 (1.30354)
2022-05-06 17:33:02,230 INFO 	Epoch: [2][10/309]	Batch time 3.7652 (4.3560)	Loss 1.34115 (1.22271)
2022-05-06 17:33:42,141 INFO 	Epoch: [2][20/309]	Batch time 3.7986 (4.1822)	Loss 1.20790 (1.24015)
2022-05-06 17:34:23,487 INFO 	Epoch: [2][30/309]	Batch time 4.3013 (4.1669)	Loss 1.26700 (1.23637)
2022-05-06 17:35:05,423 INFO 	Epoch: [2][40/309]	Batch time 4.5589 (4.1734)	Loss 1.01949 (1.21277)
2022-05-06 17:35:43,188 INFO 	Epoch: [2][50/309]	Batch time 4.5106 (4.0956)	Loss 1.03470 (1.20945)
2022-05-06 17:36:24,226 INFO 	Epoch: [2][60/309]	Batch time 3.9119 (4.0969)	Loss 1.33819 (1.21175)
2022-05-06 17:37:04,194 INFO 	Epoch: [2][70/309]	Batch time 4.0896 (4.0828)	Loss 1.24214 (1.20857)
2022-05-06 17:37:44,769 INFO 	Epoch: [2][80/309]	Batch time 4.1579 (4.0797)	Loss 1.16092 (1.20343)
2022-05-06 17:38:26,023 INFO 	Epoch: [2][90/309]	Batch time 4.3870 (4.0847)	Loss 1.10139 (1.19308)
2022-05-06 17:39:05,528 INFO 	Epoch: [2][100/309]	Batch time 3.4182 (4.0714)	Loss 1.23858 (1.18970)
2022-05-06 17:39:44,913 INFO 	Epoch: [2][110/309]	Batch time 3.8494 (4.0594)	Loss 1.08486 (1.18657)
2022-05-06 17:40:23,706 INFO 	Epoch: [2][120/309]	Batch time 3.9409 (4.0446)	Loss 1.03481 (1.18868)
2022-05-06 17:41:03,977 INFO 	Epoch: [2][130/309]	Batch time 4.0576 (4.0432)	Loss 1.00530 (1.18481)
2022-05-06 17:41:43,068 INFO 	Epoch: [2][140/309]	Batch time 4.0204 (4.0337)	Loss 1.14960 (1.18132)
2022-05-06 17:42:23,130 INFO 	Epoch: [2][150/309]	Batch time 4.0955 (4.0319)	Loss 1.17645 (1.17782)
2022-05-06 17:43:03,792 INFO 	Epoch: [2][160/309]	Batch time 3.9662 (4.0340)	Loss 0.94744 (1.16896)
2022-05-06 17:43:44,011 INFO 	Epoch: [2][170/309]	Batch time 3.8511 (4.0333)	Loss 1.16400 (1.16598)
2022-05-06 17:44:24,667 INFO 	Epoch: [2][180/309]	Batch time 4.2828 (4.0351)	Loss 1.06357 (1.16036)
2022-05-06 17:45:05,148 INFO 	Epoch: [2][190/309]	Batch time 4.1048 (4.0358)	Loss 1.15535 (1.15615)
2022-05-06 17:45:46,972 INFO 	Epoch: [2][200/309]	Batch time 4.4314 (4.0431)	Loss 0.89247 (1.15051)
2022-05-06 17:46:27,786 INFO 	Epoch: [2][210/309]	Batch time 3.7678 (4.0449)	Loss 1.11960 (1.14636)
2022-05-06 17:47:09,106 INFO 	Epoch: [2][220/309]	Batch time 3.9073 (4.0488)	Loss 1.11815 (1.14234)
2022-05-06 17:47:49,791 INFO 	Epoch: [2][230/309]	Batch time 4.4184 (4.0497)	Loss 0.87676 (1.13891)
2022-05-06 17:48:31,782 INFO 	Epoch: [2][240/309]	Batch time 4.0055 (4.0559)	Loss 1.01457 (1.13454)
2022-05-06 17:49:12,096 INFO 	Epoch: [2][250/309]	Batch time 3.3985 (4.0549)	Loss 1.10747 (1.13040)
2022-05-06 17:49:52,325 INFO 	Epoch: [2][260/309]	Batch time 3.8734 (4.0537)	Loss 1.07225 (1.12659)
2022-05-06 17:50:33,041 INFO 	Epoch: [2][270/309]	Batch time 3.9564 (4.0543)	Loss 0.99271 (1.12268)
2022-05-06 17:51:14,275 INFO 	Epoch: [2][280/309]	Batch time 4.7441 (4.0568)	Loss 1.00911 (1.11987)
2022-05-06 17:51:54,825 INFO 	Epoch: [2][290/309]	Batch time 3.9605 (4.0567)	Loss 0.92274 (1.11533)
2022-05-06 17:52:33,012 INFO 	Epoch: [2][300/309]	Batch time 3.9553 (4.0488)	Loss 1.00666 (1.11167)

Learning rate: 0.001
Step num: 927

2022-05-06 17:53:10,128 INFO 	
Validation Loss 1.01453 (0.96095)

Warning! Reached max decoder steps
save mel done
2022-05-06 17:53:25,234 INFO 	Epoch: [3][0/309]	Batch time 6.6396 (6.6396)	Loss 1.05723 (1.05723)
2022-05-06 17:54:04,578 INFO 	Epoch: [3][10/309]	Batch time 3.8029 (4.1803)	Loss 1.17594 (1.04940)
2022-05-06 17:54:44,076 INFO 	Epoch: [3][20/309]	Batch time 3.7011 (4.0706)	Loss 1.18988 (1.03463)
2022-05-06 17:55:25,116 INFO 	Epoch: [3][30/309]	Batch time 4.4272 (4.0814)	Loss 0.85017 (1.02864)
2022-05-06 17:56:05,236 INFO 	Epoch: [3][40/309]	Batch time 3.6513 (4.0644)	Loss 1.00227 (1.01248)
2022-05-06 17:56:45,090 INFO 	Epoch: [3][50/309]	Batch time 3.7708 (4.0489)	Loss 1.06124 (1.01325)
2022-05-06 17:57:24,735 INFO 	Epoch: [3][60/309]	Batch time 3.9652 (4.0351)	Loss 0.99350 (1.00932)
2022-05-06 17:58:04,834 INFO 	Epoch: [3][70/309]	Batch time 3.9757 (4.0315)	Loss 0.96771 (1.00301)
2022-05-06 17:58:45,413 INFO 	Epoch: [3][80/309]	Batch time 4.7599 (4.0348)	Loss 0.79244 (0.99378)
2022-05-06 17:59:26,233 INFO 	Epoch: [3][90/309]	Batch time 4.1097 (4.0400)	Loss 0.97235 (0.98629)
2022-05-06 18:00:05,297 INFO 	Epoch: [3][100/309]	Batch time 3.7251 (4.0268)	Loss 0.94948 (0.98364)
2022-05-06 18:00:44,961 INFO 	Epoch: [3][110/309]	Batch time 4.2004 (4.0213)	Loss 0.98860 (0.98111)
2022-05-06 18:01:25,308 INFO 	Epoch: [3][120/309]	Batch time 3.5399 (4.0224)	Loss 0.99620 (0.97832)
2022-05-06 18:02:05,719 INFO 	Epoch: [3][130/309]	Batch time 4.4827 (4.0239)	Loss 0.78550 (0.97294)
2022-05-06 18:02:46,248 INFO 	Epoch: [3][140/309]	Batch time 4.0858 (4.0259)	Loss 0.96397 (0.97100)
2022-05-06 18:03:26,857 INFO 	Epoch: [3][150/309]	Batch time 3.9652 (4.0282)	Loss 1.01513 (0.97079)
2022-05-06 18:04:06,474 INFO 	Epoch: [3][160/309]	Batch time 3.6857 (4.0241)	Loss 1.08981 (0.97139)
2022-05-06 18:04:48,233 INFO 	Epoch: [3][170/309]	Batch time 4.0830 (4.0330)	Loss 0.96486 (0.96743)
2022-05-06 18:05:27,312 INFO 	Epoch: [3][180/309]	Batch time 4.1752 (4.0261)	Loss 0.88352 (0.96489)
2022-05-06 18:06:08,125 INFO 	Epoch: [3][190/309]	Batch time 3.7596 (4.0290)	Loss 1.01068 (0.96236)
2022-05-06 18:06:48,247 INFO 	Epoch: [3][200/309]	Batch time 3.7139 (4.0281)	Loss 1.08719 (0.96162)
2022-05-06 18:07:25,842 INFO 	Epoch: [3][210/309]	Batch time 4.3338 (4.0154)	Loss 0.75132 (0.96169)
2022-05-06 18:08:09,915 INFO 	Epoch: [3][220/309]	Batch time 4.5261 (4.0331)	Loss 0.80612 (0.95748)
2022-05-06 18:08:50,059 INFO 	Epoch: [3][230/309]	Batch time 3.9281 (4.0323)	Loss 0.96055 (0.95559)
2022-05-06 18:09:28,563 INFO 	Epoch: [3][240/309]	Batch time 3.9958 (4.0248)	Loss 0.93524 (0.95269)
2022-05-06 18:10:08,731 INFO 	Epoch: [3][250/309]	Batch time 4.0655 (4.0245)	Loss 0.85680 (0.95015)
2022-05-06 18:10:50,229 INFO 	Epoch: [3][260/309]	Batch time 4.6215 (4.0293)	Loss 0.81869 (0.94772)
2022-05-06 18:11:29,177 INFO 	Epoch: [3][270/309]	Batch time 3.6628 (4.0243)	Loss 0.97931 (0.94655)
2022-05-06 18:12:10,561 INFO 	Epoch: [3][280/309]	Batch time 3.5730 (4.0284)	Loss 0.93162 (0.94411)
2022-05-06 18:12:52,352 INFO 	Epoch: [3][290/309]	Batch time 4.0423 (4.0335)	Loss 0.83510 (0.94081)
2022-05-06 18:13:31,079 INFO 	Epoch: [3][300/309]	Batch time 3.7180 (4.0282)	Loss 0.95220 (0.93955)

Learning rate: 0.001
Step num: 1236

2022-05-06 18:14:06,851 INFO 	
Validation Loss 0.88410 (0.83633)

Warning! Reached max decoder steps
save mel done
2022-05-06 18:14:22,255 INFO 	Epoch: [4][0/309]	Batch time 6.8504 (6.8504)	Loss 0.88314 (0.88314)
2022-05-06 18:15:02,837 INFO 	Epoch: [4][10/309]	Batch time 3.7306 (4.3120)	Loss 0.85854 (0.85674)
2022-05-06 18:15:43,918 INFO 	Epoch: [4][20/309]	Batch time 4.4589 (4.2149)	Loss 0.74754 (0.86594)
2022-05-06 18:16:23,054 INFO 	Epoch: [4][30/309]	Batch time 4.0729 (4.1177)	Loss 0.92220 (0.87688)
2022-05-06 18:17:04,414 INFO 	Epoch: [4][40/309]	Batch time 3.9498 (4.1222)	Loss 0.81590 (0.86727)
2022-05-06 18:17:46,096 INFO 	Epoch: [4][50/309]	Batch time 3.8192 (4.1312)	Loss 0.90562 (0.85962)
2022-05-06 18:18:26,035 INFO 	Epoch: [4][60/309]	Batch time 4.1122 (4.1087)	Loss 0.85148 (0.85725)
2022-05-06 18:19:08,333 INFO 	Epoch: [4][70/309]	Batch time 3.9789 (4.1257)	Loss 0.85863 (0.85381)
2022-05-06 18:19:50,524 INFO 	Epoch: [4][80/309]	Batch time 3.9414 (4.1373)	Loss 0.94261 (0.85085)
2022-05-06 18:20:31,360 INFO 	Epoch: [4][90/309]	Batch time 4.0696 (4.1314)	Loss 0.82759 (0.84892)
2022-05-06 18:21:11,287 INFO 	Epoch: [4][100/309]	Batch time 3.9592 (4.1176)	Loss 0.85889 (0.84569)
2022-05-06 18:21:51,106 INFO 	Epoch: [4][110/309]	Batch time 5.0991 (4.1054)	Loss 0.70839 (0.84815)
2022-05-06 18:22:33,332 INFO 	Epoch: [4][120/309]	Batch time 4.6152 (4.1151)	Loss 0.78630 (0.84840)
2022-05-06 18:23:14,234 INFO 	Epoch: [4][130/309]	Batch time 4.0779 (4.1132)	Loss 0.84399 (0.84691)
2022-05-06 18:23:54,667 INFO 	Epoch: [4][140/309]	Batch time 3.9642 (4.1082)	Loss 0.91530 (0.84832)
2022-05-06 18:24:34,741 INFO 	Epoch: [4][150/309]	Batch time 3.5837 (4.1016)	Loss 0.88847 (0.84773)
2022-05-06 18:25:17,308 INFO 	Epoch: [4][160/309]	Batch time 3.8336 (4.1112)	Loss 0.76799 (0.84640)
2022-05-06 18:25:55,853 INFO 	Epoch: [4][170/309]	Batch time 3.3633 (4.0962)	Loss 0.82829 (0.84615)
2022-05-06 18:26:35,660 INFO 	Epoch: [4][180/309]	Batch time 3.9675 (4.0898)	Loss 0.86504 (0.84706)
2022-05-06 18:27:16,834 INFO 	Epoch: [4][190/309]	Batch time 4.0401 (4.0913)	Loss 0.87261 (0.84612)
2022-05-06 18:27:56,973 INFO 	Epoch: [4][200/309]	Batch time 3.7729 (4.0874)	Loss 0.93918 (0.84530)
2022-05-06 18:28:37,542 INFO 	Epoch: [4][210/309]	Batch time 4.1175 (4.0860)	Loss 0.82617 (0.84430)
2022-05-06 18:29:19,053 INFO 	Epoch: [4][220/309]	Batch time 3.8055 (4.0889)	Loss 0.84854 (0.84312)
2022-05-06 18:30:00,559 INFO 	Epoch: [4][230/309]	Batch time 4.4340 (4.0916)	Loss 0.72332 (0.84226)
2022-05-06 18:30:41,044 INFO 	Epoch: [4][240/309]	Batch time 3.9600 (4.0898)	Loss 0.84092 (0.84212)
2022-05-06 18:31:22,460 INFO 	Epoch: [4][250/309]	Batch time 3.6545 (4.0919)	Loss 0.86503 (0.84081)
2022-05-06 18:32:03,023 INFO 	Epoch: [4][260/309]	Batch time 4.3777 (4.0905)	Loss 0.82325 (0.84151)
2022-05-06 18:32:44,381 INFO 	Epoch: [4][270/309]	Batch time 4.1359 (4.0922)	Loss 0.78230 (0.84112)
2022-05-06 18:33:25,362 INFO 	Epoch: [4][280/309]	Batch time 4.8469 (4.0924)	Loss 0.75565 (0.84068)
2022-05-06 18:34:06,098 INFO 	Epoch: [4][290/309]	Batch time 3.8294 (4.0917)	Loss 0.76412 (0.83879)
2022-05-06 18:34:45,509 INFO 	Epoch: [4][300/309]	Batch time 4.1547 (4.0867)	Loss 0.86050 (0.83831)

Learning rate: 0.001
Step num: 1545

2022-05-06 18:35:22,325 INFO 	
Validation Loss 0.80268 (0.75868)

Warning! Reached max decoder steps
save mel done
2022-05-06 18:35:37,715 INFO 	Epoch: [5][0/309]	Batch time 6.7743 (6.7743)	Loss 0.75590 (0.75590)
2022-05-06 18:36:17,415 INFO 	Epoch: [5][10/309]	Batch time 3.9131 (4.2250)	Loss 0.81946 (0.83237)
2022-05-06 18:36:56,157 INFO 	Epoch: [5][20/309]	Batch time 4.0708 (4.0579)	Loss 0.73738 (0.84751)
2022-05-06 18:37:37,651 INFO 	Epoch: [5][30/309]	Batch time 4.0425 (4.0874)	Loss 0.74218 (0.83309)
2022-05-06 18:38:17,891 INFO 	Epoch: [5][40/309]	Batch time 3.9996 (4.0720)	Loss 0.85523 (0.81993)
2022-05-06 18:38:58,873 INFO 	Epoch: [5][50/309]	Batch time 4.0240 (4.0771)	Loss 0.78973 (0.81810)
2022-05-06 18:39:39,552 INFO 	Epoch: [5][60/309]	Batch time 3.8903 (4.0756)	Loss 0.90476 (0.80999)
2022-05-06 18:40:20,355 INFO 	Epoch: [5][70/309]	Batch time 3.7860 (4.0763)	Loss 0.82722 (0.80689)
2022-05-06 18:40:59,252 INFO 	Epoch: [5][80/309]	Batch time 3.4943 (4.0532)	Loss 0.83207 (0.80864)
2022-05-06 18:41:39,934 INFO 	Epoch: [5][90/309]	Batch time 3.9276 (4.0549)	Loss 0.84932 (0.80838)
2022-05-06 18:42:22,772 INFO 	Epoch: [5][100/309]	Batch time 3.7542 (4.0775)	Loss 0.91275 (0.80700)
2022-05-06 18:43:01,936 INFO 	Epoch: [5][110/309]	Batch time 3.5935 (4.0630)	Loss 0.80501 (0.80383)
2022-05-06 18:43:43,117 INFO 	Epoch: [5][120/309]	Batch time 4.1799 (4.0676)	Loss 0.85471 (0.80274)
2022-05-06 18:44:24,941 INFO 	Epoch: [5][130/309]	Batch time 4.8240 (4.0763)	Loss 0.60966 (0.79884)
2022-05-06 18:45:05,908 INFO 	Epoch: [5][140/309]	Batch time 4.5821 (4.0778)	Loss 0.67943 (0.79607)
2022-05-06 18:45:46,347 INFO 	Epoch: [5][150/309]	Batch time 3.7967 (4.0755)	Loss 0.85239 (0.79450)
2022-05-06 18:46:26,043 INFO 	Epoch: [5][160/309]	Batch time 4.5656 (4.0690)	Loss 0.71174 (0.79303)
2022-05-06 18:47:06,045 INFO 	Epoch: [5][170/309]	Batch time 4.3586 (4.0649)	Loss 0.72720 (0.79274)
2022-05-06 18:47:47,650 INFO 	Epoch: [5][180/309]	Batch time 4.2769 (4.0702)	Loss 0.68233 (0.79126)
2022-05-06 18:48:28,603 INFO 	Epoch: [5][190/309]	Batch time 4.3532 (4.0715)	Loss 0.77503 (0.79169)
2022-05-06 18:49:08,174 INFO 	Epoch: [5][200/309]	Batch time 3.7739 (4.0658)	Loss 0.79748 (0.79096)
2022-05-06 18:49:48,121 INFO 	Epoch: [5][210/309]	Batch time 4.2774 (4.0625)	Loss 0.75761 (0.79050)
2022-05-06 18:50:28,045 INFO 	Epoch: [5][220/309]	Batch time 3.6550 (4.0593)	Loss 0.81296 (0.79012)
2022-05-06 18:51:08,022 INFO 	Epoch: [5][230/309]	Batch time 3.8738 (4.0566)	Loss 0.78200 (0.78977)
2022-05-06 18:51:51,139 INFO 	Epoch: [5][240/309]	Batch time 4.0260 (4.0672)	Loss 0.80716 (0.78686)
2022-05-06 18:52:31,445 INFO 	Epoch: [5][250/309]	Batch time 3.7987 (4.0658)	Loss 0.80962 (0.78504)
2022-05-06 18:53:12,193 INFO 	Epoch: [5][260/309]	Batch time 4.0493 (4.0661)	Loss 0.71835 (0.78327)
2022-05-06 18:53:52,101 INFO 	Epoch: [5][270/309]	Batch time 4.0577 (4.0633)	Loss 0.73672 (0.78183)
2022-05-06 18:54:31,217 INFO 	Epoch: [5][280/309]	Batch time 4.0461 (4.0579)	Loss 0.70247 (0.78107)
2022-05-06 18:55:11,882 INFO 	Epoch: [5][290/309]	Batch time 3.7195 (4.0582)	Loss 0.85472 (0.78084)
2022-05-06 18:55:52,886 INFO 	Epoch: [5][300/309]	Batch time 4.5975 (4.0596)	Loss 0.65618 (0.77924)

Learning rate: 0.001
Step num: 1854

2022-05-06 18:56:30,640 INFO 	
Validation Loss 0.73804 (0.69852)

Warning! Reached max decoder steps
save mel done
2022-05-06 18:56:46,176 INFO 	Epoch: [6][0/309]	Batch time 6.8947 (6.8947)	Loss 0.78824 (0.78824)
2022-05-06 18:57:25,335 INFO 	Epoch: [6][10/309]	Batch time 3.7762 (4.1867)	Loss 0.72850 (0.74917)
2022-05-06 18:58:04,106 INFO 	Epoch: [6][20/309]	Batch time 4.1293 (4.0393)	Loss 0.72168 (0.74229)
2022-05-06 18:58:44,698 INFO 	Epoch: [6][30/309]	Batch time 4.7436 (4.0457)	Loss 0.50964 (0.72995)
2022-05-06 18:59:27,575 INFO 	Epoch: [6][40/309]	Batch time 4.1150 (4.1047)	Loss 0.68713 (0.72893)
2022-05-06 19:00:06,208 INFO 	Epoch: [6][50/309]	Batch time 3.6357 (4.0574)	Loss 0.77788 (0.73493)
2022-05-06 19:00:46,636 INFO 	Epoch: [6][60/309]	Batch time 4.3514 (4.0550)	Loss 0.63462 (0.73384)
2022-05-06 19:01:24,917 INFO 	Epoch: [6][70/309]	Batch time 3.7755 (4.0230)	Loss 0.77057 (0.73772)
2022-05-06 19:02:04,925 INFO 	Epoch: [6][80/309]	Batch time 3.9733 (4.0203)	Loss 0.72175 (0.73833)
2022-05-06 19:02:45,461 INFO 	Epoch: [6][90/309]	Batch time 3.7580 (4.0240)	Loss 0.71048 (0.73507)
2022-05-06 19:03:26,576 INFO 	Epoch: [6][100/309]	Batch time 4.4803 (4.0326)	Loss 0.69431 (0.73179)
2022-05-06 19:04:07,539 INFO 	Epoch: [6][110/309]	Batch time 4.5202 (4.0384)	Loss 0.65099 (0.73066)
2022-05-06 19:04:47,316 INFO 	Epoch: [6][120/309]	Batch time 3.6613 (4.0333)	Loss 0.83206 (0.73127)
2022-05-06 19:05:29,178 INFO 	Epoch: [6][130/309]	Batch time 4.3472 (4.0450)	Loss 0.68395 (0.72839)
2022-05-06 19:06:09,960 INFO 	Epoch: [6][140/309]	Batch time 4.5308 (4.0474)	Loss 0.58699 (0.72708)
2022-05-06 19:06:49,889 INFO 	Epoch: [6][150/309]	Batch time 3.9653 (4.0438)	Loss 0.72341 (0.72717)
2022-05-06 19:07:29,183 INFO 	Epoch: [6][160/309]	Batch time 3.9452 (4.0367)	Loss 0.64704 (0.72496)
2022-05-06 19:08:09,437 INFO 	Epoch: [6][170/309]	Batch time 3.9965 (4.0360)	Loss 0.78040 (0.72342)
2022-05-06 19:08:49,709 INFO 	Epoch: [6][180/309]	Batch time 4.1014 (4.0355)	Loss 0.71789 (0.72286)
2022-05-06 19:09:30,944 INFO 	Epoch: [6][190/309]	Batch time 3.8464 (4.0401)	Loss 0.78296 (0.72213)
2022-05-06 19:10:11,680 INFO 	Epoch: [6][200/309]	Batch time 4.7354 (4.0418)	Loss 0.62048 (0.72182)
2022-05-06 19:10:53,551 INFO 	Epoch: [6][210/309]	Batch time 4.6132 (4.0487)	Loss 0.69105 (0.71938)
2022-05-06 19:11:32,774 INFO 	Epoch: [6][220/309]	Batch time 3.6545 (4.0430)	Loss 0.83951 (0.72109)
2022-05-06 19:12:13,283 INFO 	Epoch: [6][230/309]	Batch time 4.1221 (4.0433)	Loss 0.74198 (0.72175)
2022-05-06 19:12:52,974 INFO 	Epoch: [6][240/309]	Batch time 4.3695 (4.0402)	Loss 0.73289 (0.72224)
2022-05-06 19:13:33,189 INFO 	Epoch: [6][250/309]	Batch time 4.1110 (4.0395)	Loss 0.70127 (0.72114)
2022-05-06 19:14:13,641 INFO 	Epoch: [6][260/309]	Batch time 4.1584 (4.0397)	Loss 0.65985 (0.71976)
2022-05-06 19:14:54,314 INFO 	Epoch: [6][270/309]	Batch time 4.0286 (4.0407)	Loss 0.64951 (0.72004)
2022-05-06 19:15:34,720 INFO 	Epoch: [6][280/309]	Batch time 3.9218 (4.0407)	Loss 0.73788 (0.71957)
2022-05-06 19:16:15,529 INFO 	Epoch: [6][290/309]	Batch time 4.5186 (4.0421)	Loss 0.65635 (0.71924)
2022-05-06 19:16:55,811 INFO 	Epoch: [6][300/309]	Batch time 3.9905 (4.0416)	Loss 0.67133 (0.71832)

Learning rate: 0.001
Step num: 2163

2022-05-06 19:17:30,308 INFO 	
Validation Loss 0.71097 (0.67188)

Warning! Reached max decoder steps
save mel done
2022-05-06 19:17:45,318 INFO 	Epoch: [7][0/309]	Batch time 6.5063 (6.5063)	Loss 0.83145 (0.83145)
2022-05-06 19:18:26,217 INFO 	Epoch: [7][10/309]	Batch time 4.8053 (4.3096)	Loss 0.54872 (0.69427)
2022-05-06 19:19:07,950 INFO 	Epoch: [7][20/309]	Batch time 5.0798 (4.2447)	Loss 0.58426 (0.68809)
2022-05-06 19:19:47,608 INFO 	Epoch: [7][30/309]	Batch time 3.9510 (4.1547)	Loss 0.74617 (0.70283)
2022-05-06 19:20:28,286 INFO 	Epoch: [7][40/309]	Batch time 3.7730 (4.1335)	Loss 0.72282 (0.69899)
2022-05-06 19:21:08,656 INFO 	Epoch: [7][50/309]	Batch time 4.1431 (4.1146)	Loss 0.62981 (0.69557)
2022-05-06 19:21:47,685 INFO 	Epoch: [7][60/309]	Batch time 4.0815 (4.0799)	Loss 0.65141 (0.69615)
2022-05-06 19:22:28,980 INFO 	Epoch: [7][70/309]	Batch time 4.2704 (4.0869)	Loss 0.64767 (0.68995)
2022-05-06 19:23:09,001 INFO 	Epoch: [7][80/309]	Batch time 3.8922 (4.0764)	Loss 0.71161 (0.69038)
2022-05-06 19:23:49,330 INFO 	Epoch: [7][90/309]	Batch time 3.9054 (4.0716)	Loss 0.66848 (0.68844)
2022-05-06 19:24:29,075 INFO 	Epoch: [7][100/309]	Batch time 4.0301 (4.0620)	Loss 0.69007 (0.68959)
2022-05-06 19:25:09,170 INFO 	Epoch: [7][110/309]	Batch time 4.8423 (4.0573)	Loss 0.52138 (0.69022)
2022-05-06 19:25:49,078 INFO 	Epoch: [7][120/309]	Batch time 4.5643 (4.0518)	Loss 0.56309 (0.68977)
2022-05-06 19:26:29,336 INFO 	Epoch: [7][130/309]	Batch time 4.0220 (4.0498)	Loss 0.69639 (0.68723)
2022-05-06 19:27:09,109 INFO 	Epoch: [7][140/309]	Batch time 3.3956 (4.0447)	Loss 0.79655 (0.68827)
2022-05-06 19:27:48,853 INFO 	Epoch: [7][150/309]	Batch time 4.2466 (4.0400)	Loss 0.65256 (0.68782)
2022-05-06 19:28:30,360 INFO 	Epoch: [7][160/309]	Batch time 4.0998 (4.0469)	Loss 0.71438 (0.68672)
2022-05-06 19:29:11,264 INFO 	Epoch: [7][170/309]	Batch time 3.5011 (4.0494)	Loss 0.71547 (0.68614)
2022-05-06 19:29:50,325 INFO 	Epoch: [7][180/309]	Batch time 3.8386 (4.0415)	Loss 0.79911 (0.68713)
2022-05-06 19:30:30,761 INFO 	Epoch: [7][190/309]	Batch time 4.1781 (4.0416)	Loss 0.67677 (0.68627)
2022-05-06 19:31:11,686 INFO 	Epoch: [7][200/309]	Batch time 4.1550 (4.0442)	Loss 0.59879 (0.68505)
2022-05-06 19:31:53,141 INFO 	Epoch: [7][210/309]	Batch time 4.2332 (4.0490)	Loss 0.66696 (0.68362)
2022-05-06 19:32:31,464 INFO 	Epoch: [7][220/309]	Batch time 3.9109 (4.0392)	Loss 0.66029 (0.68412)
2022-05-06 19:33:14,727 INFO 	Epoch: [7][230/309]	Batch time 5.2741 (4.0516)	Loss 0.61654 (0.68279)
2022-05-06 19:33:54,659 INFO 	Epoch: [7][240/309]	Batch time 4.1230 (4.0492)	Loss 0.70240 (0.68349)
2022-05-06 19:34:32,432 INFO 	Epoch: [7][250/309]	Batch time 4.2508 (4.0383)	Loss 0.63170 (0.68349)
2022-05-06 19:35:11,028 INFO 	Epoch: [7][260/309]	Batch time 3.9966 (4.0315)	Loss 0.68429 (0.68287)
2022-05-06 19:35:51,705 INFO 	Epoch: [7][270/309]	Batch time 4.8536 (4.0328)	Loss 0.54939 (0.68239)
2022-05-06 19:36:32,744 INFO 	Epoch: [7][280/309]	Batch time 4.4249 (4.0353)	Loss 0.64697 (0.68095)
2022-05-06 19:37:12,281 INFO 	Epoch: [7][290/309]	Batch time 3.8040 (4.0325)	Loss 0.68907 (0.68092)
2022-05-06 19:37:52,413 INFO 	Epoch: [7][300/309]	Batch time 3.9993 (4.0319)	Loss 0.71386 (0.68045)

Learning rate: 0.001
Step num: 2472

2022-05-06 19:38:29,767 INFO 	
Validation Loss 0.66747 (0.62468)

Warning! Reached max decoder steps
save mel done
2022-05-06 19:38:44,860 INFO 	Epoch: [8][0/309]	Batch time 6.5173 (6.5173)	Loss 0.70303 (0.70303)
2022-05-06 19:39:24,264 INFO 	Epoch: [8][10/309]	Batch time 4.2949 (4.1747)	Loss 0.59405 (0.67497)
2022-05-06 19:40:04,841 INFO 	Epoch: [8][20/309]	Batch time 4.1588 (4.1190)	Loss 0.62881 (0.65860)
2022-05-06 19:40:45,364 INFO 	Epoch: [8][30/309]	Batch time 3.9515 (4.0975)	Loss 0.67036 (0.65614)
2022-05-06 19:41:25,509 INFO 	Epoch: [8][40/309]	Batch time 3.8652 (4.0772)	Loss 0.71646 (0.65280)
2022-05-06 19:42:05,830 INFO 	Epoch: [8][50/309]	Batch time 3.8973 (4.0684)	Loss 0.69907 (0.65192)
2022-05-06 19:42:46,475 INFO 	Epoch: [8][60/309]	Batch time 3.9446 (4.0677)	Loss 0.63289 (0.64921)
2022-05-06 19:43:27,576 INFO 	Epoch: [8][70/309]	Batch time 3.8048 (4.0737)	Loss 0.66192 (0.64913)
2022-05-06 19:44:07,092 INFO 	Epoch: [8][80/309]	Batch time 3.9073 (4.0586)	Loss 0.63992 (0.65048)
2022-05-06 19:44:47,095 INFO 	Epoch: [8][90/309]	Batch time 3.9937 (4.0522)	Loss 0.68066 (0.64886)
2022-05-06 19:45:27,253 INFO 	Epoch: [8][100/309]	Batch time 3.7446 (4.0486)	Loss 0.73153 (0.64887)
2022-05-06 19:46:06,923 INFO 	Epoch: [8][110/309]	Batch time 4.2447 (4.0413)	Loss 0.56954 (0.64785)
2022-05-06 19:46:47,467 INFO 	Epoch: [8][120/309]	Batch time 4.1029 (4.0423)	Loss 0.63140 (0.64604)
2022-05-06 19:47:26,397 INFO 	Epoch: [8][130/309]	Batch time 3.6170 (4.0310)	Loss 0.73031 (0.64750)
2022-05-06 19:48:07,870 INFO 	Epoch: [8][140/309]	Batch time 4.0177 (4.0392)	Loss 0.59817 (0.64668)
2022-05-06 19:48:47,158 INFO 	Epoch: [8][150/309]	Batch time 3.7320 (4.0319)	Loss 0.69881 (0.64764)
2022-05-06 19:49:26,545 INFO 	Epoch: [8][160/309]	Batch time 3.8447 (4.0261)	Loss 0.68710 (0.64750)
2022-05-06 19:50:06,052 INFO 	Epoch: [8][170/309]	Batch time 3.6118 (4.0217)	Loss 0.73340 (0.64758)
2022-05-06 19:50:48,969 INFO 	Epoch: [8][180/309]	Batch time 4.1001 (4.0366)	Loss 0.62034 (0.64566)
2022-05-06 19:51:26,756 INFO 	Epoch: [8][190/309]	Batch time 3.6774 (4.0231)	Loss 0.62368 (0.64483)
2022-05-06 19:52:05,984 INFO 	Epoch: [8][200/309]	Batch time 3.6191 (4.0181)	Loss 0.65970 (0.64420)
2022-05-06 19:52:45,658 INFO 	Epoch: [8][210/309]	Batch time 4.0922 (4.0157)	Loss 0.63587 (0.64369)
2022-05-06 19:53:26,773 INFO 	Epoch: [8][220/309]	Batch time 4.0199 (4.0200)	Loss 0.75273 (0.64390)
2022-05-06 19:54:05,537 INFO 	Epoch: [8][230/309]	Batch time 3.7671 (4.0138)	Loss 0.65032 (0.64360)
2022-05-06 19:54:45,924 INFO 	Epoch: [8][240/309]	Batch time 4.0860 (4.0149)	Loss 0.54761 (0.64218)
2022-05-06 19:55:25,404 INFO 	Epoch: [8][250/309]	Batch time 3.9859 (4.0122)	Loss 0.57978 (0.64112)
2022-05-06 19:56:07,621 INFO 	Epoch: [8][260/309]	Batch time 4.4459 (4.0202)	Loss 0.50576 (0.63917)
2022-05-06 19:56:47,406 INFO 	Epoch: [8][270/309]	Batch time 3.8972 (4.0187)	Loss 0.64052 (0.63807)
2022-05-06 19:57:28,061 INFO 	Epoch: [8][280/309]	Batch time 3.6990 (4.0203)	Loss 0.65836 (0.63696)
2022-05-06 19:58:08,067 INFO 	Epoch: [8][290/309]	Batch time 4.0031 (4.0197)	Loss 0.59529 (0.63671)
2022-05-06 19:58:47,820 INFO 	Epoch: [8][300/309]	Batch time 4.3040 (4.0182)	Loss 0.55949 (0.63580)

Learning rate: 0.001
Step num: 2781

2022-05-06 19:59:25,541 INFO 	
Validation Loss 0.61913 (0.57927)

Warning! Reached max decoder steps
save mel done
2022-05-06 19:59:40,950 INFO 	Epoch: [9][0/309]	Batch time 7.0020 (7.0020)	Loss 0.62435 (0.62435)
2022-05-06 20:00:20,917 INFO 	Epoch: [9][10/309]	Batch time 4.1063 (4.2699)	Loss 0.56326 (0.59697)
2022-05-06 20:00:59,852 INFO 	Epoch: [9][20/309]	Batch time 3.9328 (4.0907)	Loss 0.60226 (0.61821)
2022-05-06 20:01:41,833 INFO 	Epoch: [9][30/309]	Batch time 4.3135 (4.1253)	Loss 0.55997 (0.61089)
2022-05-06 20:02:20,974 INFO 	Epoch: [9][40/309]	Batch time 3.5745 (4.0738)	Loss 0.65775 (0.60937)
2022-05-06 20:02:59,626 INFO 	Epoch: [9][50/309]	Batch time 3.7100 (4.0329)	Loss 0.68586 (0.61004)
2022-05-06 20:03:39,526 INFO 	Epoch: [9][60/309]	Batch time 3.7639 (4.0259)	Loss 0.68385 (0.61061)
2022-05-06 20:04:19,126 INFO 	Epoch: [9][70/309]	Batch time 3.7421 (4.0166)	Loss 0.65419 (0.61445)
2022-05-06 20:04:59,349 INFO 	Epoch: [9][80/309]	Batch time 4.4816 (4.0173)	Loss 0.56028 (0.61131)
2022-05-06 20:05:40,040 INFO 	Epoch: [9][90/309]	Batch time 4.3766 (4.0230)	Loss 0.63820 (0.61092)
2022-05-06 20:06:20,410 INFO 	Epoch: [9][100/309]	Batch time 3.9588 (4.0244)	Loss 0.56380 (0.60680)
2022-05-06 20:06:59,526 INFO 	Epoch: [9][110/309]	Batch time 3.9242 (4.0142)	Loss 0.66891 (0.60808)
2022-05-06 20:07:42,850 INFO 	Epoch: [9][120/309]	Batch time 4.3762 (4.0405)	Loss 0.56491 (0.60637)
2022-05-06 20:08:23,689 INFO 	Epoch: [9][130/309]	Batch time 3.8088 (4.0438)	Loss 0.59854 (0.60589)
2022-05-06 20:09:04,129 INFO 	Epoch: [9][140/309]	Batch time 3.9092 (4.0438)	Loss 0.60928 (0.60482)
2022-05-06 20:09:44,566 INFO 	Epoch: [9][150/309]	Batch time 4.1859 (4.0438)	Loss 0.56796 (0.60305)
2022-05-06 20:10:24,612 INFO 	Epoch: [9][160/309]	Batch time 4.4072 (4.0414)	Loss 0.54692 (0.60287)
2022-05-06 20:11:06,554 INFO 	Epoch: [9][170/309]	Batch time 4.1214 (4.0503)	Loss 0.54779 (0.60185)
2022-05-06 20:11:45,699 INFO 	Epoch: [9][180/309]	Batch time 3.7200 (4.0428)	Loss 0.60325 (0.60236)
2022-05-06 20:12:26,521 INFO 	Epoch: [9][190/309]	Batch time 4.4086 (4.0449)	Loss 0.58642 (0.60083)
2022-05-06 20:13:07,580 INFO 	Epoch: [9][200/309]	Batch time 4.4528 (4.0479)	Loss 0.51181 (0.59916)
2022-05-06 20:13:47,733 INFO 	Epoch: [9][210/309]	Batch time 4.1688 (4.0464)	Loss 0.62101 (0.59899)
2022-05-06 20:14:28,178 INFO 	Epoch: [9][220/309]	Batch time 4.3271 (4.0463)	Loss 0.55225 (0.59845)
2022-05-06 20:15:08,754 INFO 	Epoch: [9][230/309]	Batch time 4.0819 (4.0468)	Loss 0.60852 (0.59702)
2022-05-06 20:15:48,285 INFO 	Epoch: [9][240/309]	Batch time 3.8863 (4.0429)	Loss 0.63599 (0.59728)
2022-05-06 20:16:29,730 INFO 	Epoch: [9][250/309]	Batch time 4.2017 (4.0469)	Loss 0.55608 (0.59660)
2022-05-06 20:17:11,777 INFO 	Epoch: [9][260/309]	Batch time 4.3128 (4.0530)	Loss 0.53927 (0.59629)
2022-05-06 20:17:52,501 INFO 	Epoch: [9][270/309]	Batch time 4.1193 (4.0537)	Loss 0.54801 (0.59545)
2022-05-06 20:18:31,940 INFO 	Epoch: [9][280/309]	Batch time 3.6957 (4.0498)	Loss 0.64936 (0.59541)
2022-05-06 20:19:13,219 INFO 	Epoch: [9][290/309]	Batch time 4.6904 (4.0525)	Loss 0.52448 (0.59460)
2022-05-06 20:19:52,901 INFO 	Epoch: [9][300/309]	Batch time 3.9361 (4.0497)	Loss 0.55697 (0.59404)

Learning rate: 0.001
Step num: 3090

2022-05-06 20:20:28,278 INFO 	
Validation Loss 0.59268 (0.55263)

Warning! Reached max decoder steps
save mel done
2022-05-06 20:20:43,702 INFO 	Epoch: [10][0/309]	Batch time 6.7245 (6.7245)	Loss 0.65402 (0.65402)
2022-05-06 20:21:23,419 INFO 	Epoch: [10][10/309]	Batch time 4.1904 (4.2220)	Loss 0.63790 (0.61134)
2022-05-06 20:22:06,175 INFO 	Epoch: [10][20/309]	Batch time 3.8585 (4.2475)	Loss 0.66230 (0.59349)
2022-05-06 20:22:46,549 INFO 	Epoch: [10][30/309]	Batch time 3.9224 (4.1797)	Loss 0.57824 (0.59202)
2022-05-06 20:23:26,592 INFO 	Epoch: [10][40/309]	Batch time 4.1887 (4.1369)	Loss 0.58585 (0.59345)
2022-05-06 20:24:07,455 INFO 	Epoch: [10][50/309]	Batch time 4.0215 (4.1270)	Loss 0.59318 (0.58827)
2022-05-06 20:24:49,537 INFO 	Epoch: [10][60/309]	Batch time 5.0765 (4.1403)	Loss 0.50814 (0.58325)
2022-05-06 20:25:30,236 INFO 	Epoch: [10][70/309]	Batch time 4.6787 (4.1304)	Loss 0.46801 (0.57932)
2022-05-06 20:26:10,645 INFO 	Epoch: [10][80/309]	Batch time 3.7498 (4.1194)	Loss 0.56648 (0.57535)
2022-05-06 20:26:51,807 INFO 	Epoch: [10][90/309]	Batch time 5.0170 (4.1190)	Loss 0.46456 (0.57489)
2022-05-06 20:27:31,223 INFO 	Epoch: [10][100/309]	Batch time 3.8969 (4.1014)	Loss 0.56845 (0.57519)
2022-05-06 20:28:10,791 INFO 	Epoch: [10][110/309]	Batch time 4.0602 (4.0884)	Loss 0.56659 (0.57576)
2022-05-06 20:28:51,521 INFO 	Epoch: [10][120/309]	Batch time 4.5360 (4.0871)	Loss 0.51354 (0.57453)
2022-05-06 20:29:30,168 INFO 	Epoch: [10][130/309]	Batch time 4.0778 (4.0702)	Loss 0.57012 (0.57492)
2022-05-06 20:30:10,722 INFO 	Epoch: [10][140/309]	Batch time 4.0950 (4.0691)	Loss 0.56126 (0.57390)
2022-05-06 20:30:50,660 INFO 	Epoch: [10][150/309]	Batch time 4.0377 (4.0641)	Loss 0.55804 (0.57382)
2022-05-06 20:31:33,220 INFO 	Epoch: [10][160/309]	Batch time 3.8383 (4.0760)	Loss 0.60210 (0.57283)
2022-05-06 20:32:13,369 INFO 	Epoch: [10][170/309]	Batch time 4.6075 (4.0725)	Loss 0.52348 (0.57205)
2022-05-06 20:32:53,996 INFO 	Epoch: [10][180/309]	Batch time 4.0974 (4.0719)	Loss 0.54578 (0.57206)
2022-05-06 20:33:34,658 INFO 	Epoch: [10][190/309]	Batch time 3.9986 (4.0716)	Loss 0.59671 (0.57158)
2022-05-06 20:34:16,483 INFO 	Epoch: [10][200/309]	Batch time 4.3438 (4.0771)	Loss 0.53016 (0.57119)
2022-05-06 20:34:58,451 INFO 	Epoch: [10][210/309]	Batch time 4.5532 (4.0828)	Loss 0.46414 (0.57000)
2022-05-06 20:35:38,674 INFO 	Epoch: [10][220/309]	Batch time 3.9422 (4.0801)	Loss 0.56970 (0.57080)
2022-05-06 20:36:17,712 INFO 	Epoch: [10][230/309]	Batch time 4.6838 (4.0724)	Loss 0.48097 (0.57048)
2022-05-06 20:36:57,686 INFO 	Epoch: [10][240/309]	Batch time 3.6792 (4.0693)	Loss 0.58955 (0.57089)
2022-05-06 20:37:38,535 INFO 	Epoch: [10][250/309]	Batch time 3.7959 (4.0699)	Loss 0.60052 (0.57079)
2022-05-06 20:38:19,109 INFO 	Epoch: [10][260/309]	Batch time 4.0509 (4.0695)	Loss 0.60928 (0.57107)
2022-05-06 20:39:00,971 INFO 	Epoch: [10][270/309]	Batch time 4.8914 (4.0738)	Loss 0.49959 (0.57045)
2022-05-06 20:39:43,320 INFO 	Epoch: [10][280/309]	Batch time 4.3077 (4.0795)	Loss 0.51915 (0.56969)
2022-05-06 20:40:22,498 INFO 	Epoch: [10][290/309]	Batch time 4.1770 (4.0740)	Loss 0.55933 (0.57013)
2022-05-06 20:41:01,100 INFO 	Epoch: [10][300/309]	Batch time 3.6913 (4.0669)	Loss 0.58523 (0.56951)

Learning rate: 0.001
Step num: 3399

2022-05-06 20:41:37,934 INFO 	
Validation Loss 0.57155 (0.53694)

Warning! Reached max decoder steps
save mel done
2022-05-06 20:41:53,629 INFO 	Epoch: [11][0/309]	Batch time 6.9957 (6.9957)	Loss 0.63636 (0.63636)
2022-05-06 20:42:35,477 INFO 	Epoch: [11][10/309]	Batch time 4.5560 (4.4403)	Loss 0.56748 (0.57988)
2022-05-06 20:43:13,380 INFO 	Epoch: [11][20/309]	Batch time 4.1869 (4.1308)	Loss 0.57166 (0.57822)
2022-05-06 20:43:53,196 INFO 	Epoch: [11][30/309]	Batch time 3.8001 (4.0827)	Loss 0.57946 (0.57751)
2022-05-06 20:44:33,645 INFO 	Epoch: [11][40/309]	Batch time 4.0874 (4.0734)	Loss 0.56715 (0.57406)
2022-05-06 20:45:14,170 INFO 	Epoch: [11][50/309]	Batch time 3.8945 (4.0693)	Loss 0.61504 (0.57084)
2022-05-06 20:45:55,619 INFO 	Epoch: [11][60/309]	Batch time 4.2410 (4.0817)	Loss 0.58011 (0.56926)
2022-05-06 20:46:36,801 INFO 	Epoch: [11][70/309]	Batch time 3.9129 (4.0869)	Loss 0.52489 (0.56686)
2022-05-06 20:47:17,762 INFO 	Epoch: [11][80/309]	Batch time 4.3537 (4.0880)	Loss 0.48549 (0.56352)
2022-05-06 20:47:58,950 INFO 	Epoch: [11][90/309]	Batch time 3.9694 (4.0914)	Loss 0.55143 (0.55993)
2022-05-06 20:48:40,743 INFO 	Epoch: [11][100/309]	Batch time 3.8644 (4.1001)	Loss 0.56074 (0.55779)
2022-05-06 20:49:21,359 INFO 	Epoch: [11][110/309]	Batch time 3.8055 (4.0966)	Loss 0.58738 (0.55750)
2022-05-06 20:50:01,911 INFO 	Epoch: [11][120/309]	Batch time 4.1030 (4.0932)	Loss 0.54197 (0.55706)
2022-05-06 20:50:42,641 INFO 	Epoch: [11][130/309]	Batch time 3.4574 (4.0917)	Loss 0.59339 (0.55676)
2022-05-06 20:51:23,581 INFO 	Epoch: [11][140/309]	Batch time 4.5570 (4.0918)	Loss 0.52180 (0.55645)
2022-05-06 20:52:05,502 INFO 	Epoch: [11][150/309]	Batch time 3.9461 (4.0985)	Loss 0.57084 (0.55463)
2022-05-06 20:52:48,349 INFO 	Epoch: [11][160/309]	Batch time 4.0553 (4.1100)	Loss 0.55628 (0.55297)
2022-05-06 20:53:28,062 INFO 	Epoch: [11][170/309]	Batch time 3.7256 (4.1019)	Loss 0.54903 (0.55425)
2022-05-06 20:54:08,802 INFO 	Epoch: [11][180/309]	Batch time 4.3504 (4.1004)	Loss 0.48376 (0.55393)
2022-05-06 20:54:49,637 INFO 	Epoch: [11][190/309]	Batch time 3.8949 (4.0995)	Loss 0.56350 (0.55380)
2022-05-06 20:55:30,383 INFO 	Epoch: [11][200/309]	Batch time 4.3714 (4.0983)	Loss 0.48581 (0.55439)
2022-05-06 20:56:11,787 INFO 	Epoch: [11][210/309]	Batch time 4.6396 (4.1003)	Loss 0.49957 (0.55339)
2022-05-06 20:56:52,050 INFO 	Epoch: [11][220/309]	Batch time 4.2541 (4.0969)	Loss 0.55264 (0.55340)
2022-05-06 20:57:31,312 INFO 	Epoch: [11][230/309]	Batch time 3.8726 (4.0895)	Loss 0.57074 (0.55427)
2022-05-06 20:58:11,342 INFO 	Epoch: [11][240/309]	Batch time 3.9610 (4.0859)	Loss 0.56401 (0.55470)
2022-05-06 20:58:50,772 INFO 	Epoch: [11][250/309]	Batch time 3.8466 (4.0802)	Loss 0.54006 (0.55550)
2022-05-06 20:59:34,554 INFO 	Epoch: [11][260/309]	Batch time 4.2163 (4.0916)	Loss 0.54252 (0.55383)
2022-05-06 21:00:16,798 INFO 	Epoch: [11][270/309]	Batch time 3.7910 (4.0965)	Loss 0.57155 (0.55312)
2022-05-06 21:00:56,209 INFO 	Epoch: [11][280/309]	Batch time 4.1184 (4.0910)	Loss 0.54337 (0.55274)
2022-05-06 21:01:37,868 INFO 	Epoch: [11][290/309]	Batch time 4.0973 (4.0936)	Loss 0.51887 (0.55213)
2022-05-06 21:02:18,201 INFO 	Epoch: [11][300/309]	Batch time 4.6198 (4.0916)	Loss 0.46926 (0.55203)

Learning rate: 0.001
Step num: 3708

2022-05-06 21:02:55,369 INFO 	
Validation Loss 0.57854 (0.54414)


Epochs since last improvement: 1

Warning! Reached max decoder steps
save mel done
2022-05-06 21:03:07,980 INFO 	Epoch: [12][0/309]	Batch time 6.7804 (6.7804)	Loss 0.53815 (0.53815)
2022-05-06 21:03:48,208 INFO 	Epoch: [12][10/309]	Batch time 3.8581 (4.2736)	Loss 0.56747 (0.53586)
2022-05-06 21:04:30,572 INFO 	Epoch: [12][20/309]	Batch time 4.2229 (4.2558)	Loss 0.53906 (0.53220)
2022-05-06 21:05:11,477 INFO 	Epoch: [12][30/309]	Batch time 4.3727 (4.2025)	Loss 0.47445 (0.52949)
2022-05-06 21:05:53,056 INFO 	Epoch: [12][40/309]	Batch time 4.1117 (4.1916)	Loss 0.54713 (0.53052)
2022-05-06 21:06:34,544 INFO 	Epoch: [12][50/309]	Batch time 4.3695 (4.1832)	Loss 0.50684 (0.53131)
2022-05-06 21:07:16,465 INFO 	Epoch: [12][60/309]	Batch time 3.9224 (4.1847)	Loss 0.53145 (0.52772)
2022-05-06 21:07:58,383 INFO 	Epoch: [12][70/309]	Batch time 4.5844 (4.1857)	Loss 0.47838 (0.52807)
2022-05-06 21:08:37,847 INFO 	Epoch: [12][80/309]	Batch time 3.7163 (4.1561)	Loss 0.59662 (0.53030)
2022-05-06 21:09:19,075 INFO 	Epoch: [12][90/309]	Batch time 4.1696 (4.1525)	Loss 0.50683 (0.53047)
2022-05-06 21:09:59,421 INFO 	Epoch: [12][100/309]	Batch time 4.0892 (4.1408)	Loss 0.55167 (0.53099)
2022-05-06 21:10:41,466 INFO 	Epoch: [12][110/309]	Batch time 4.2746 (4.1465)	Loss 0.54780 (0.52942)
2022-05-06 21:11:21,061 INFO 	Epoch: [12][120/309]	Batch time 4.1337 (4.1311)	Loss 0.52223 (0.53174)
2022-05-06 21:12:02,332 INFO 	Epoch: [12][130/309]	Batch time 4.3060 (4.1308)	Loss 0.52214 (0.53349)
2022-05-06 21:12:43,297 INFO 	Epoch: [12][140/309]	Batch time 3.8263 (4.1284)	Loss 0.51889 (0.53224)
2022-05-06 21:13:23,821 INFO 	Epoch: [12][150/309]	Batch time 3.9065 (4.1233)	Loss 0.54478 (0.53292)
2022-05-06 21:14:03,034 INFO 	Epoch: [12][160/309]	Batch time 4.0815 (4.1108)	Loss 0.55132 (0.53285)
2022-05-06 21:14:43,715 INFO 	Epoch: [12][170/309]	Batch time 4.4460 (4.1083)	Loss 0.47545 (0.53304)
2022-05-06 21:15:23,496 INFO 	Epoch: [12][180/309]	Batch time 4.1918 (4.1011)	Loss 0.55987 (0.53450)
2022-05-06 21:16:03,334 INFO 	Epoch: [12][190/309]	Batch time 3.8227 (4.0949)	Loss 0.58268 (0.53598)
2022-05-06 21:16:41,913 INFO 	Epoch: [12][200/309]	Batch time 3.8966 (4.0832)	Loss 0.54613 (0.53643)
2022-05-06 21:17:25,890 INFO 	Epoch: [12][210/309]	Batch time 4.0716 (4.0981)	Loss 0.51598 (0.53615)
2022-05-06 21:18:05,264 INFO 	Epoch: [12][220/309]	Batch time 3.9064 (4.0908)	Loss 0.47923 (0.53649)
2022-05-06 21:18:45,418 INFO 	Epoch: [12][230/309]	Batch time 4.0848 (4.0875)	Loss 0.49820 (0.53562)
2022-05-06 21:19:26,298 INFO 	Epoch: [12][240/309]	Batch time 4.0790 (4.0875)	Loss 0.55358 (0.53517)
2022-05-06 21:20:07,730 INFO 	Epoch: [12][250/309]	Batch time 4.1624 (4.0898)	Loss 0.50257 (0.53409)
2022-05-06 21:20:46,209 INFO 	Epoch: [12][260/309]	Batch time 3.8238 (4.0805)	Loss 0.57367 (0.53491)
2022-05-06 21:21:28,241 INFO 	Epoch: [12][270/309]	Batch time 4.6970 (4.0850)	Loss 0.51008 (0.53495)
2022-05-06 21:22:09,737 INFO 	Epoch: [12][280/309]	Batch time 4.2596 (4.0873)	Loss 0.54143 (0.53460)
2022-05-06 21:22:49,556 INFO 	Epoch: [12][290/309]	Batch time 3.9651 (4.0837)	Loss 0.53254 (0.53489)
2022-05-06 21:23:31,635 INFO 	Epoch: [12][300/309]	Batch time 3.9347 (4.0878)	Loss 0.55844 (0.53443)

Learning rate: 0.001
Step num: 4017

2022-05-06 21:24:08,401 INFO 	
Validation Loss 0.53670 (0.50071)

Warning! Reached max decoder steps
save mel done
2022-05-06 21:24:24,017 INFO 	Epoch: [13][0/309]	Batch time 7.0857 (7.0857)	Loss 0.48885 (0.48885)
2022-05-06 21:25:04,921 INFO 	Epoch: [13][10/309]	Batch time 3.6135 (4.3627)	Loss 0.61215 (0.51542)
2022-05-06 21:25:47,172 INFO 	Epoch: [13][20/309]	Batch time 4.2317 (4.2972)	Loss 0.47909 (0.50981)
2022-05-06 21:26:29,700 INFO 	Epoch: [13][30/309]	Batch time 4.9006 (4.2829)	Loss 0.46008 (0.51029)
2022-05-06 21:27:10,192 INFO 	Epoch: [13][40/309]	Batch time 4.1580 (4.2259)	Loss 0.50944 (0.51070)
2022-05-06 21:27:51,363 INFO 	Epoch: [13][50/309]	Batch time 4.3999 (4.2046)	Loss 0.52555 (0.51142)
2022-05-06 21:28:31,775 INFO 	Epoch: [13][60/309]	Batch time 4.1198 (4.1778)	Loss 0.51025 (0.51113)
2022-05-06 21:29:09,344 INFO 	Epoch: [13][70/309]	Batch time 3.8350 (4.1185)	Loss 0.54748 (0.51539)
2022-05-06 21:29:51,241 INFO 	Epoch: [13][80/309]	Batch time 3.7524 (4.1273)	Loss 0.51739 (0.51368)
2022-05-06 21:30:31,854 INFO 	Epoch: [13][90/309]	Batch time 4.0110 (4.1200)	Loss 0.57121 (0.51573)
2022-05-06 21:31:14,763 INFO 	Epoch: [13][100/309]	Batch time 3.5926 (4.1369)	Loss 0.52266 (0.51301)
2022-05-06 21:31:56,157 INFO 	Epoch: [13][110/309]	Batch time 4.4715 (4.1372)	Loss 0.49883 (0.51376)
2022-05-06 21:32:36,997 INFO 	Epoch: [13][120/309]	Batch time 4.0226 (4.1328)	Loss 0.53599 (0.51466)
2022-05-06 21:33:19,793 INFO 	Epoch: [13][130/309]	Batch time 4.4504 (4.1440)	Loss 0.51221 (0.51383)
2022-05-06 21:33:59,884 INFO 	Epoch: [13][140/309]	Batch time 3.8125 (4.1344)	Loss 0.46000 (0.51329)
2022-05-06 21:34:40,620 INFO 	Epoch: [13][150/309]	Batch time 4.6048 (4.1304)	Loss 0.49498 (0.51408)
2022-05-06 21:35:25,452 INFO 	Epoch: [13][160/309]	Batch time 4.2945 (4.1523)	Loss 0.56245 (0.51304)
2022-05-06 21:36:05,374 INFO 	Epoch: [13][170/309]	Batch time 4.2132 (4.1429)	Loss 0.54791 (0.51348)
2022-05-06 21:36:47,847 INFO 	Epoch: [13][180/309]	Batch time 4.0044 (4.1487)	Loss 0.47098 (0.51286)
2022-05-06 21:37:27,720 INFO 	Epoch: [13][190/309]	Batch time 4.2088 (4.1403)	Loss 0.50435 (0.51387)
2022-05-06 21:38:07,496 INFO 	Epoch: [13][200/309]	Batch time 3.7555 (4.1322)	Loss 0.56316 (0.51406)
2022-05-06 21:38:48,047 INFO 	Epoch: [13][210/309]	Batch time 4.2069 (4.1285)	Loss 0.57452 (0.51440)
2022-05-06 21:39:28,737 INFO 	Epoch: [13][220/309]	Batch time 4.2950 (4.1258)	Loss 0.44942 (0.51456)
2022-05-06 21:40:10,229 INFO 	Epoch: [13][230/309]	Batch time 4.2057 (4.1268)	Loss 0.49407 (0.51445)
2022-05-06 21:40:51,870 INFO 	Epoch: [13][240/309]	Batch time 4.0735 (4.1284)	Loss 0.50547 (0.51380)
2022-05-06 21:41:34,091 INFO 	Epoch: [13][250/309]	Batch time 4.2083 (4.1321)	Loss 0.47595 (0.51380)
2022-05-06 21:42:14,043 INFO 	Epoch: [13][260/309]	Batch time 3.5687 (4.1269)	Loss 0.56182 (0.51371)
2022-05-06 21:42:55,161 INFO 	Epoch: [13][270/309]	Batch time 4.1142 (4.1263)	Loss 0.48446 (0.51370)
2022-05-06 21:43:35,733 INFO 	Epoch: [13][280/309]	Batch time 4.5812 (4.1238)	Loss 0.46492 (0.51391)
2022-05-06 21:44:15,910 INFO 	Epoch: [13][290/309]	Batch time 4.1789 (4.1202)	Loss 0.53090 (0.51336)
2022-05-06 21:44:56,342 INFO 	Epoch: [13][300/309]	Batch time 4.0049 (4.1176)	Loss 0.50155 (0.51267)

Learning rate: 0.001
Step num: 4326

2022-05-06 21:45:32,580 INFO 	
Validation Loss 0.51318 (0.48139)

Warning! Reached max decoder steps
save mel done
2022-05-06 21:45:48,549 INFO 	Epoch: [14][0/309]	Batch time 7.0970 (7.0970)	Loss 0.48189 (0.48189)
2022-05-06 21:46:30,111 INFO 	Epoch: [14][10/309]	Batch time 4.1436 (4.4235)	Loss 0.50739 (0.50447)
2022-05-06 21:47:12,508 INFO 	Epoch: [14][20/309]	Batch time 4.2336 (4.3360)	Loss 0.49585 (0.49862)
2022-05-06 21:47:56,804 INFO 	Epoch: [14][30/309]	Batch time 4.6951 (4.3662)	Loss 0.45660 (0.48909)
2022-05-06 21:48:37,312 INFO 	Epoch: [14][40/309]	Batch time 4.0426 (4.2893)	Loss 0.44364 (0.48858)
2022-05-06 21:49:17,615 INFO 	Epoch: [14][50/309]	Batch time 3.9205 (4.2385)	Loss 0.50767 (0.49058)
2022-05-06 21:49:57,402 INFO 	Epoch: [14][60/309]	Batch time 3.9404 (4.1959)	Loss 0.56721 (0.49347)
2022-05-06 21:50:37,198 INFO 	Epoch: [14][70/309]	Batch time 4.2403 (4.1654)	Loss 0.50996 (0.49584)
2022-05-06 21:51:19,257 INFO 	Epoch: [14][80/309]	Batch time 4.2790 (4.1704)	Loss 0.51879 (0.49538)
2022-05-06 21:52:00,214 INFO 	Epoch: [14][90/309]	Batch time 4.7024 (4.1622)	Loss 0.48378 (0.49736)
2022-05-06 21:52:45,614 INFO 	Epoch: [14][100/309]	Batch time 5.0167 (4.1996)	Loss 0.41543 (0.49684)
2022-05-06 21:53:27,300 INFO 	Epoch: [14][110/309]	Batch time 3.3359 (4.1968)	Loss 0.51057 (0.49673)
2022-05-06 21:54:07,648 INFO 	Epoch: [14][120/309]	Batch time 3.8515 (4.1834)	Loss 0.47884 (0.49731)
2022-05-06 21:54:46,619 INFO 	Epoch: [14][130/309]	Batch time 3.6156 (4.1616)	Loss 0.51603 (0.49823)
2022-05-06 21:55:26,791 INFO 	Epoch: [14][140/309]	Batch time 3.9119 (4.1513)	Loss 0.53714 (0.49909)
2022-05-06 21:56:08,115 INFO 	Epoch: [14][150/309]	Batch time 3.9857 (4.1501)	Loss 0.52777 (0.49938)
2022-05-06 21:56:48,123 INFO 	Epoch: [14][160/309]	Batch time 3.8500 (4.1408)	Loss 0.53335 (0.50093)
2022-05-06 21:57:28,156 INFO 	Epoch: [14][170/309]	Batch time 3.8983 (4.1328)	Loss 0.49374 (0.50162)
2022-05-06 21:58:08,473 INFO 	Epoch: [14][180/309]	Batch time 4.0326 (4.1272)	Loss 0.47100 (0.50165)
2022-05-06 21:58:49,274 INFO 	Epoch: [14][190/309]	Batch time 3.7218 (4.1247)	Loss 0.47486 (0.50058)
2022-05-06 21:59:31,093 INFO 	Epoch: [14][200/309]	Batch time 3.9939 (4.1276)	Loss 0.49681 (0.50035)
2022-05-06 22:00:12,372 INFO 	Epoch: [14][210/309]	Batch time 4.4225 (4.1276)	Loss 0.48154 (0.50018)
2022-05-06 22:00:54,217 INFO 	Epoch: [14][220/309]	Batch time 3.9769 (4.1302)	Loss 0.45665 (0.49959)
2022-05-06 22:01:35,093 INFO 	Epoch: [14][230/309]	Batch time 4.0634 (4.1283)	Loss 0.48862 (0.49907)
2022-05-06 22:02:15,710 INFO 	Epoch: [14][240/309]	Batch time 4.1122 (4.1256)	Loss 0.50507 (0.49919)
2022-05-06 22:02:56,538 INFO 	Epoch: [14][250/309]	Batch time 3.7618 (4.1238)	Loss 0.47891 (0.49908)
2022-05-06 22:03:38,094 INFO 	Epoch: [14][260/309]	Batch time 4.8974 (4.1251)	Loss 0.48503 (0.49791)
2022-05-06 22:04:20,491 INFO 	Epoch: [14][270/309]	Batch time 4.1972 (4.1293)	Loss 0.46225 (0.49727)
2022-05-06 22:05:02,593 INFO 	Epoch: [14][280/309]	Batch time 3.9373 (4.1322)	Loss 0.46826 (0.49642)
2022-05-06 22:05:44,774 INFO 	Epoch: [14][290/309]	Batch time 4.4299 (4.1351)	Loss 0.43503 (0.49591)
2022-05-06 22:06:25,190 INFO 	Epoch: [14][300/309]	Batch time 3.6301 (4.1320)	Loss 0.49294 (0.49554)

Learning rate: 0.001
Step num: 4635

2022-05-06 22:07:03,002 INFO 	
Validation Loss 0.51306 (0.48286)


Epochs since last improvement: 1

Warning! Reached max decoder steps
save mel done
2022-05-06 22:07:16,528 INFO 	Epoch: [15][0/309]	Batch time 7.4317 (7.4317)	Loss 0.48118 (0.48118)
2022-05-06 22:07:57,585 INFO 	Epoch: [15][10/309]	Batch time 3.8076 (4.4081)	Loss 0.50940 (0.49830)
2022-05-06 22:08:37,773 INFO 	Epoch: [15][20/309]	Batch time 3.6663 (4.2227)	Loss 0.54192 (0.49969)
2022-05-06 22:09:18,599 INFO 	Epoch: [15][30/309]	Batch time 3.9111 (4.1775)	Loss 0.47873 (0.49759)
2022-05-06 22:09:58,980 INFO 	Epoch: [15][40/309]	Batch time 4.5954 (4.1435)	Loss 0.50183 (0.49912)
2022-05-06 22:10:43,856 INFO 	Epoch: [15][50/309]	Batch time 4.2409 (4.2110)	Loss 0.45615 (0.49522)
2022-05-06 22:11:24,428 INFO 	Epoch: [15][60/309]	Batch time 4.0694 (4.1858)	Loss 0.53264 (0.49347)
2022-05-06 22:12:05,337 INFO 	Epoch: [15][70/309]	Batch time 3.7023 (4.1724)	Loss 0.50207 (0.49172)
2022-05-06 22:12:45,930 INFO 	Epoch: [15][80/309]	Batch time 3.7032 (4.1584)	Loss 0.52163 (0.48913)
2022-05-06 22:13:26,623 INFO 	Epoch: [15][90/309]	Batch time 4.0025 (4.1487)	Loss 0.49680 (0.49023)
2022-05-06 22:14:07,570 INFO 	Epoch: [15][100/309]	Batch time 4.1599 (4.1433)	Loss 0.45271 (0.48975)
2022-05-06 22:14:49,460 INFO 	Epoch: [15][110/309]	Batch time 3.8218 (4.1474)	Loss 0.47048 (0.48878)
2022-05-06 22:15:29,329 INFO 	Epoch: [15][120/309]	Batch time 4.0724 (4.1342)	Loss 0.53008 (0.48935)
2022-05-06 22:16:11,216 INFO 	Epoch: [15][130/309]	Batch time 3.7496 (4.1383)	Loss 0.47562 (0.48726)
2022-05-06 22:16:50,599 INFO 	Epoch: [15][140/309]	Batch time 3.6562 (4.1241)	Loss 0.48865 (0.48777)
2022-05-06 22:17:30,362 INFO 	Epoch: [15][150/309]	Batch time 4.2982 (4.1143)	Loss 0.41460 (0.48725)
2022-05-06 22:18:10,727 INFO 	Epoch: [15][160/309]	Batch time 4.1322 (4.1095)	Loss 0.48443 (0.48720)
2022-05-06 22:18:53,739 INFO 	Epoch: [15][170/309]	Batch time 4.0933 (4.1207)	Loss 0.49637 (0.48640)
2022-05-06 22:19:36,357 INFO 	Epoch: [15][180/309]	Batch time 3.8688 (4.1285)	Loss 0.50635 (0.48536)
2022-05-06 22:20:17,896 INFO 	Epoch: [15][190/309]	Batch time 4.0626 (4.1298)	Loss 0.51883 (0.48552)
2022-05-06 22:21:00,000 INFO 	Epoch: [15][200/309]	Batch time 4.4197 (4.1338)	Loss 0.52584 (0.48470)
2022-05-06 22:21:41,222 INFO 	Epoch: [15][210/309]	Batch time 4.2094 (4.1333)	Loss 0.51129 (0.48493)
2022-05-06 22:22:22,580 INFO 	Epoch: [15][220/309]	Batch time 3.9275 (4.1334)	Loss 0.49393 (0.48447)
2022-05-06 22:23:03,991 INFO 	Epoch: [15][230/309]	Batch time 4.2658 (4.1337)	Loss 0.44457 (0.48461)
2022-05-06 22:23:45,769 INFO 	Epoch: [15][240/309]	Batch time 4.4110 (4.1356)	Loss 0.43064 (0.48363)
2022-05-06 22:24:26,053 INFO 	Epoch: [15][250/309]	Batch time 4.1081 (4.1313)	Loss 0.47566 (0.48371)
2022-05-06 22:25:08,437 INFO 	Epoch: [15][260/309]	Batch time 3.9839 (4.1354)	Loss 0.47266 (0.48328)
2022-05-06 22:25:49,196 INFO 	Epoch: [15][270/309]	Batch time 4.6336 (4.1332)	Loss 0.40703 (0.48302)
2022-05-06 22:26:30,966 INFO 	Epoch: [15][280/309]	Batch time 3.8615 (4.1348)	Loss 0.54165 (0.48230)
2022-05-06 22:27:10,466 INFO 	Epoch: [15][290/309]	Batch time 3.9969 (4.1284)	Loss 0.49402 (0.48254)
2022-05-06 22:27:50,881 INFO 	Epoch: [15][300/309]	Batch time 3.9808 (4.1255)	Loss 0.57138 (0.48324)

Learning rate: 0.001
Step num: 4944

2022-05-06 22:28:29,774 INFO 	
Validation Loss 0.48547 (0.45554)

Warning! Reached max decoder steps
save mel done
2022-05-06 22:28:45,306 INFO 	Epoch: [16][0/309]	Batch time 6.9119 (6.9119)	Loss 0.45819 (0.45819)
2022-05-06 22:29:25,415 INFO 	Epoch: [16][10/309]	Batch time 3.6633 (4.2746)	Loss 0.49467 (0.46491)
2022-05-06 22:30:06,369 INFO 	Epoch: [16][20/309]	Batch time 4.4444 (4.1892)	Loss 0.38164 (0.46241)
2022-05-06 22:30:47,496 INFO 	Epoch: [16][30/309]	Batch time 3.8217 (4.1646)	Loss 0.45372 (0.47017)
2022-05-06 22:31:30,712 INFO 	Epoch: [16][40/309]	Batch time 4.7598 (4.2029)	Loss 0.39314 (0.46879)
2022-05-06 22:32:13,574 INFO 	Epoch: [16][50/309]	Batch time 4.5567 (4.2192)	Loss 0.39556 (0.46668)
2022-05-06 22:32:55,572 INFO 	Epoch: [16][60/309]	Batch time 3.9017 (4.2160)	Loss 0.46075 (0.46496)
2022-05-06 22:33:37,533 INFO 	Epoch: [16][70/309]	Batch time 3.9384 (4.2132)	Loss 0.45982 (0.46245)
2022-05-06 22:34:18,685 INFO 	Epoch: [16][80/309]	Batch time 4.0883 (4.2011)	Loss 0.45900 (0.46381)
2022-05-06 22:34:58,634 INFO 	Epoch: [16][90/309]	Batch time 4.0012 (4.1785)	Loss 0.53190 (0.46598)
2022-05-06 22:35:41,429 INFO 	Epoch: [16][100/309]	Batch time 4.2814 (4.1885)	Loss 0.48597 (0.46622)
2022-05-06 22:36:22,575 INFO 	Epoch: [16][110/309]	Batch time 4.7106 (4.1818)	Loss 0.40712 (0.46771)
2022-05-06 22:37:02,568 INFO 	Epoch: [16][120/309]	Batch time 3.9658 (4.1667)	Loss 0.44142 (0.46740)
2022-05-06 22:37:42,012 INFO 	Epoch: [16][130/309]	Batch time 4.2552 (4.1497)	Loss 0.46445 (0.46842)
2022-05-06 22:38:24,054 INFO 	Epoch: [16][140/309]	Batch time 4.1524 (4.1536)	Loss 0.45518 (0.46769)
2022-05-06 22:39:06,218 INFO 	Epoch: [16][150/309]	Batch time 3.8757 (4.1578)	Loss 0.53341 (0.46731)
2022-05-06 22:39:47,088 INFO 	Epoch: [16][160/309]	Batch time 3.8035 (4.1534)	Loss 0.47302 (0.46728)
2022-05-06 22:40:27,570 INFO 	Epoch: [16][170/309]	Batch time 3.8566 (4.1472)	Loss 0.52839 (0.46878)
2022-05-06 22:41:09,389 INFO 	Epoch: [16][180/309]	Batch time 3.9698 (4.1491)	Loss 0.44062 (0.46822)
2022-05-06 22:41:50,503 INFO 	Epoch: [16][190/309]	Batch time 4.0038 (4.1472)	Loss 0.48074 (0.46823)
2022-05-06 22:42:31,949 INFO 	Epoch: [16][200/309]	Batch time 4.0783 (4.1470)	Loss 0.44522 (0.46740)
2022-05-06 22:43:13,070 INFO 	Epoch: [16][210/309]	Batch time 3.9412 (4.1454)	Loss 0.51725 (0.46769)
2022-05-06 22:43:53,612 INFO 	Epoch: [16][220/309]	Batch time 4.1723 (4.1413)	Loss 0.43787 (0.46779)
2022-05-06 22:44:34,241 INFO 	Epoch: [16][230/309]	Batch time 3.8102 (4.1379)	Loss 0.48735 (0.46739)
2022-05-06 22:45:13,799 INFO 	Epoch: [16][240/309]	Batch time 3.9499 (4.1303)	Loss 0.49751 (0.46807)
2022-05-06 22:45:57,504 INFO 	Epoch: [16][250/309]	Batch time 4.7416 (4.1399)	Loss 0.43572 (0.46805)
2022-05-06 22:46:37,315 INFO 	Epoch: [16][260/309]	Batch time 3.3013 (4.1338)	Loss 0.48584 (0.46833)
2022-05-06 22:47:17,096 INFO 	Epoch: [16][270/309]	Batch time 4.4471 (4.1280)	Loss 0.45163 (0.46810)
2022-05-06 22:47:58,362 INFO 	Epoch: [16][280/309]	Batch time 3.7864 (4.1280)	Loss 0.47076 (0.46767)
2022-05-06 22:48:41,617 INFO 	Epoch: [16][290/309]	Batch time 4.2633 (4.1348)	Loss 0.47035 (0.46751)
2022-05-06 22:49:22,411 INFO 	Epoch: [16][300/309]	Batch time 3.7992 (4.1329)	Loss 0.48828 (0.46709)

Learning rate: 0.001
Step num: 5253

2022-05-06 22:49:59,706 INFO 	
Validation Loss 0.46869 (0.43910)

Warning! Reached max decoder steps
save mel done
2022-05-06 22:50:15,520 INFO 	Epoch: [17][0/309]	Batch time 7.2490 (7.2490)	Loss 0.47563 (0.47563)
2022-05-06 22:50:56,062 INFO 	Epoch: [17][10/309]	Batch time 4.1228 (4.3447)	Loss 0.43357 (0.46279)
2022-05-06 22:51:37,337 INFO 	Epoch: [17][20/309]	Batch time 3.8942 (4.2412)	Loss 0.54612 (0.46220)
2022-05-06 22:52:17,458 INFO 	Epoch: [17][30/309]	Batch time 3.7614 (4.1673)	Loss 0.45758 (0.46332)
2022-05-06 22:52:59,182 INFO 	Epoch: [17][40/309]	Batch time 4.0123 (4.1686)	Loss 0.50018 (0.46454)
2022-05-06 22:53:40,923 INFO 	Epoch: [17][50/309]	Batch time 4.1426 (4.1697)	Loss 0.43039 (0.46063)
2022-05-06 22:54:20,627 INFO 	Epoch: [17][60/309]	Batch time 3.8923 (4.1370)	Loss 0.44452 (0.46170)
2022-05-06 22:55:01,662 INFO 	Epoch: [17][70/309]	Batch time 4.2390 (4.1323)	Loss 0.44768 (0.46030)
2022-05-06 22:55:42,553 INFO 	Epoch: [17][80/309]	Batch time 3.9590 (4.1269)	Loss 0.46298 (0.45929)
2022-05-06 22:56:22,654 INFO 	Epoch: [17][90/309]	Batch time 3.8033 (4.1141)	Loss 0.45778 (0.46032)
2022-05-06 22:57:05,768 INFO 	Epoch: [17][100/309]	Batch time 4.1614 (4.1336)	Loss 0.38744 (0.45706)
2022-05-06 22:57:44,508 INFO 	Epoch: [17][110/309]	Batch time 3.7300 (4.1102)	Loss 0.50442 (0.45855)
2022-05-06 22:58:26,252 INFO 	Epoch: [17][120/309]	Batch time 3.9725 (4.1155)	Loss 0.53974 (0.45922)
2022-05-06 22:59:06,745 INFO 	Epoch: [17][130/309]	Batch time 3.7536 (4.1105)	Loss 0.47446 (0.45965)
2022-05-06 22:59:47,342 INFO 	Epoch: [17][140/309]	Batch time 3.7539 (4.1069)	Loss 0.50742 (0.45989)
2022-05-06 23:00:29,007 INFO 	Epoch: [17][150/309]	Batch time 4.0776 (4.1108)	Loss 0.49069 (0.45996)
2022-05-06 23:01:10,315 INFO 	Epoch: [17][160/309]	Batch time 4.0627 (4.1121)	Loss 0.44090 (0.45951)
2022-05-06 23:01:50,847 INFO 	Epoch: [17][170/309]	Batch time 3.8297 (4.1086)	Loss 0.46583 (0.45895)
2022-05-06 23:02:32,219 INFO 	Epoch: [17][180/309]	Batch time 4.0867 (4.1102)	Loss 0.44448 (0.45855)
2022-05-06 23:03:13,929 INFO 	Epoch: [17][190/309]	Batch time 4.5032 (4.1134)	Loss 0.43898 (0.45845)
2022-05-06 23:03:57,930 INFO 	Epoch: [17][200/309]	Batch time 4.3990 (4.1277)	Loss 0.44436 (0.45793)
2022-05-06 23:04:37,562 INFO 	Epoch: [17][210/309]	Batch time 4.1679 (4.1199)	Loss 0.43028 (0.45779)
2022-05-06 23:05:18,107 INFO 	Epoch: [17][220/309]	Batch time 3.9168 (4.1169)	Loss 0.41731 (0.45745)
2022-05-06 23:06:00,451 INFO 	Epoch: [17][230/309]	Batch time 4.4425 (4.1220)	Loss 0.38469 (0.45661)
2022-05-06 23:06:40,986 INFO 	Epoch: [17][240/309]	Batch time 3.8053 (4.1191)	Loss 0.41043 (0.45654)
2022-05-06 23:07:20,096 INFO 	Epoch: [17][250/309]	Batch time 3.7089 (4.1109)	Loss 0.51116 (0.45734)
2022-05-06 23:08:02,277 INFO 	Epoch: [17][260/309]	Batch time 4.0296 (4.1150)	Loss 0.44833 (0.45659)
2022-05-06 23:08:43,405 INFO 	Epoch: [17][270/309]	Batch time 4.1370 (4.1149)	Loss 0.51318 (0.45636)
2022-05-06 23:09:23,888 INFO 	Epoch: [17][280/309]	Batch time 4.1827 (4.1125)	Loss 0.39695 (0.45608)
2022-05-06 23:10:03,997 INFO 	Epoch: [17][290/309]	Batch time 3.7351 (4.1090)	Loss 0.51600 (0.45618)
2022-05-06 23:10:45,159 INFO 	Epoch: [17][300/309]	Batch time 4.1978 (4.1093)	Loss 0.42744 (0.45590)

Learning rate: 0.001
Step num: 5562

2022-05-06 23:11:22,741 INFO 	
Validation Loss 0.45364 (0.42764)

Warning! Reached max decoder steps
save mel done
2022-05-06 23:11:38,458 INFO 	Epoch: [18][0/309]	Batch time 7.1607 (7.1607)	Loss 0.41618 (0.41618)
2022-05-06 23:12:19,006 INFO 	Epoch: [18][10/309]	Batch time 4.6395 (4.3372)	Loss 0.40261 (0.44260)
2022-05-06 23:12:59,722 INFO 	Epoch: [18][20/309]	Batch time 4.2075 (4.2107)	Loss 0.45193 (0.45383)
2022-05-06 23:13:42,350 INFO 	Epoch: [18][30/309]	Batch time 3.8700 (4.2275)	Loss 0.46252 (0.44884)
2022-05-06 23:14:24,863 INFO 	Epoch: [18][40/309]	Batch time 4.3491 (4.2333)	Loss 0.46385 (0.44378)
2022-05-06 23:15:04,967 INFO 	Epoch: [18][50/309]	Batch time 3.8801 (4.1896)	Loss 0.46469 (0.44535)
2022-05-06 23:15:45,575 INFO 	Epoch: [18][60/309]	Batch time 4.6579 (4.1685)	Loss 0.39517 (0.44603)
2022-05-06 23:16:25,949 INFO 	Epoch: [18][70/309]	Batch time 3.8573 (4.1500)	Loss 0.46833 (0.44906)
2022-05-06 23:17:06,022 INFO 	Epoch: [18][80/309]	Batch time 3.9980 (4.1324)	Loss 0.45562 (0.44971)
2022-05-06 23:17:47,376 INFO 	Epoch: [18][90/309]	Batch time 4.1193 (4.1327)	Loss 0.49042 (0.44918)
2022-05-06 23:18:29,840 INFO 	Epoch: [18][100/309]	Batch time 3.8423 (4.1440)	Loss 0.49201 (0.44805)
2022-05-06 23:19:09,970 INFO 	Epoch: [18][110/309]	Batch time 4.4400 (4.1322)	Loss 0.42331 (0.44807)
2022-05-06 23:19:53,169 INFO 	Epoch: [18][120/309]	Batch time 4.6145 (4.1477)	Loss 0.38559 (0.44639)
2022-05-06 23:20:34,282 INFO 	Epoch: [18][130/309]	Batch time 4.2137 (4.1449)	Loss 0.45061 (0.44627)
2022-05-06 23:21:17,861 INFO 	Epoch: [18][140/309]	Batch time 4.0761 (4.1600)	Loss 0.46760 (0.44591)
2022-05-06 23:21:58,607 INFO 	Epoch: [18][150/309]	Batch time 3.5116 (4.1544)	Loss 0.46012 (0.44519)
2022-05-06 23:22:39,072 INFO 	Epoch: [18][160/309]	Batch time 4.0436 (4.1477)	Loss 0.41466 (0.44457)
2022-05-06 23:23:20,329 INFO 	Epoch: [18][170/309]	Batch time 4.3353 (4.1464)	Loss 0.40637 (0.44544)
2022-05-06 23:24:00,752 INFO 	Epoch: [18][180/309]	Batch time 3.7789 (4.1406)	Loss 0.48034 (0.44589)
2022-05-06 23:24:42,123 INFO 	Epoch: [18][190/309]	Batch time 3.6199 (4.1405)	Loss 0.46334 (0.44605)
2022-05-06 23:25:21,995 INFO 	Epoch: [18][200/309]	Batch time 4.0055 (4.1328)	Loss 0.47410 (0.44686)
2022-05-06 23:26:03,862 INFO 	Epoch: [18][210/309]	Batch time 4.3958 (4.1354)	Loss 0.37151 (0.44607)
2022-05-06 23:26:44,033 INFO 	Epoch: [18][220/309]	Batch time 3.9802 (4.1300)	Loss 0.47156 (0.44667)
2022-05-06 23:27:26,280 INFO 	Epoch: [18][230/309]	Batch time 3.8708 (4.1341)	Loss 0.46988 (0.44592)
2022-05-06 23:28:05,876 INFO 	Epoch: [18][240/309]	Batch time 3.9019 (4.1269)	Loss 0.43568 (0.44668)
2022-05-06 23:28:47,207 INFO 	Epoch: [18][250/309]	Batch time 4.5439 (4.1271)	Loss 0.35193 (0.44695)
2022-05-06 23:29:27,408 INFO 	Epoch: [18][260/309]	Batch time 3.9629 (4.1230)	Loss 0.46928 (0.44747)
2022-05-06 23:30:09,650 INFO 	Epoch: [18][270/309]	Batch time 4.3564 (4.1268)	Loss 0.40908 (0.44699)
2022-05-06 23:30:50,689 INFO 	Epoch: [18][280/309]	Batch time 4.2291 (4.1259)	Loss 0.41493 (0.44694)
2022-05-06 23:31:32,467 INFO 	Epoch: [18][290/309]	Batch time 4.1322 (4.1277)	Loss 0.43644 (0.44641)
2022-05-06 23:32:15,628 INFO 	Epoch: [18][300/309]	Batch time 4.1519 (4.1340)	Loss 0.42735 (0.44571)

Learning rate: 0.001
Step num: 5871

2022-05-06 23:32:54,657 INFO 	
Validation Loss 0.44711 (0.41739)

Warning! Reached max decoder steps
save mel done
2022-05-06 23:33:10,440 INFO 	Epoch: [19][0/309]	Batch time 7.1711 (7.1711)	Loss 0.46844 (0.46844)
2022-05-06 23:33:52,303 INFO 	Epoch: [19][10/309]	Batch time 4.6779 (4.4577)	Loss 0.40511 (0.43301)
2022-05-06 23:34:33,599 INFO 	Epoch: [19][20/309]	Batch time 3.9836 (4.3014)	Loss 0.42188 (0.43620)
2022-05-06 23:35:14,861 INFO 	Epoch: [19][30/309]	Batch time 4.3577 (4.2449)	Loss 0.40224 (0.43623)
2022-05-06 23:35:55,002 INFO 	Epoch: [19][40/309]	Batch time 3.7580 (4.1886)	Loss 0.51891 (0.44394)
2022-05-06 23:36:36,709 INFO 	Epoch: [19][50/309]	Batch time 4.3232 (4.1851)	Loss 0.44609 (0.44481)
2022-05-06 23:37:18,804 INFO 	Epoch: [19][60/309]	Batch time 4.0016 (4.1891)	Loss 0.49917 (0.44388)
2022-05-06 23:37:58,752 INFO 	Epoch: [19][70/309]	Batch time 3.2659 (4.1617)	Loss 0.53587 (0.44501)
2022-05-06 23:38:42,863 INFO 	Epoch: [19][80/309]	Batch time 4.7185 (4.1925)	Loss 0.44995 (0.44372)
2022-05-06 23:39:24,633 INFO 	Epoch: [19][90/309]	Batch time 3.9049 (4.1908)	Loss 0.47009 (0.44427)
2022-05-06 23:40:04,720 INFO 	Epoch: [19][100/309]	Batch time 4.0821 (4.1728)	Loss 0.42513 (0.44333)
2022-05-06 23:40:47,163 INFO 	Epoch: [19][110/309]	Batch time 4.9851 (4.1792)	Loss 0.36668 (0.44111)
2022-05-06 23:41:27,833 INFO 	Epoch: [19][120/309]	Batch time 3.8239 (4.1700)	Loss 0.46284 (0.44118)
2022-05-06 23:42:09,065 INFO 	Epoch: [19][130/309]	Batch time 3.7409 (4.1664)	Loss 0.47615 (0.44124)
2022-05-06 23:42:49,984 INFO 	Epoch: [19][140/309]	Batch time 4.3889 (4.1611)	Loss 0.42445 (0.44131)
2022-05-06 23:43:31,060 INFO 	Epoch: [19][150/309]	Batch time 3.7662 (4.1576)	Loss 0.46611 (0.44055)
2022-05-06 23:44:11,813 INFO 	Epoch: [19][160/309]	Batch time 3.7399 (4.1524)	Loss 0.50243 (0.44020)
2022-05-06 23:44:53,232 INFO 	Epoch: [19][170/309]	Batch time 4.0893 (4.1518)	Loss 0.45398 (0.44090)
2022-05-06 23:45:34,899 INFO 	Epoch: [19][180/309]	Batch time 4.0523 (4.1526)	Loss 0.44807 (0.44076)
2022-05-06 23:46:15,984 INFO 	Epoch: [19][190/309]	Batch time 3.9337 (4.1503)	Loss 0.46982 (0.44067)
2022-05-06 23:46:57,719 INFO 	Epoch: [19][200/309]	Batch time 4.1745 (4.1515)	Loss 0.38232 (0.43963)
2022-05-06 23:47:37,506 INFO 	Epoch: [19][210/309]	Batch time 3.9034 (4.1433)	Loss 0.42801 (0.43920)
2022-05-06 23:48:19,779 INFO 	Epoch: [19][220/309]	Batch time 3.9845 (4.1471)	Loss 0.64107 (0.44394)
2022-05-06 23:49:00,116 INFO 	Epoch: [19][230/309]	Batch time 4.2565 (4.1422)	Loss 0.52065 (0.45037)
2022-05-06 23:49:43,267 INFO 	Epoch: [19][240/309]	Batch time 4.4427 (4.1494)	Loss 0.47959 (0.45191)
2022-05-06 23:50:25,537 INFO 	Epoch: [19][250/309]	Batch time 5.0192 (4.1525)	Loss 0.40054 (0.45274)
2022-05-06 23:51:07,109 INFO 	Epoch: [19][260/309]	Batch time 4.3934 (4.1526)	Loss 0.45771 (0.45323)
2022-05-06 23:51:48,593 INFO 	Epoch: [19][270/309]	Batch time 4.0812 (4.1525)	Loss 0.50431 (0.45416)
2022-05-06 23:52:29,555 INFO 	Epoch: [19][280/309]	Batch time 4.1840 (4.1505)	Loss 0.44965 (0.45438)
2022-05-06 23:53:11,168 INFO 	Epoch: [19][290/309]	Batch time 4.2518 (4.1509)	Loss 0.46885 (0.45422)
2022-05-06 23:53:51,444 INFO 	Epoch: [19][300/309]	Batch time 4.0119 (4.1468)	Loss 0.45499 (0.45520)

Learning rate: 0.001
Step num: 6180

2022-05-06 23:54:31,152 INFO 	
Validation Loss 0.46044 (0.43420)


Epochs since last improvement: 1

Warning! Reached max decoder steps
save mel done
2022-05-06 23:54:44,369 INFO 	Epoch: [20][0/309]	Batch time 7.1193 (7.1193)	Loss 0.47331 (0.47331)
2022-05-06 23:55:25,841 INFO 	Epoch: [20][10/309]	Batch time 3.8757 (4.4174)	Loss 0.43412 (0.44148)
2022-05-06 23:56:07,732 INFO 	Epoch: [20][20/309]	Batch time 4.1680 (4.3087)	Loss 0.46517 (0.43980)
2022-05-06 23:56:53,030 INFO 	Epoch: [20][30/309]	Batch time 4.7304 (4.3800)	Loss 0.40869 (0.44584)
2022-05-06 23:57:33,943 INFO 	Epoch: [20][40/309]	Batch time 4.1843 (4.3096)	Loss 0.44957 (0.44351)
2022-05-06 23:58:15,758 INFO 	Epoch: [20][50/309]	Batch time 3.9004 (4.2845)	Loss 0.45443 (0.44439)
2022-05-06 23:58:56,914 INFO 	Epoch: [20][60/309]	Batch time 4.6831 (4.2568)	Loss 0.41025 (0.44545)
2022-05-06 23:59:37,743 INFO 	Epoch: [20][70/309]	Batch time 4.0351 (4.2323)	Loss 0.44442 (0.44636)
2022-05-07 00:00:17,671 INFO 	Epoch: [20][80/309]	Batch time 3.8149 (4.2027)	Loss 0.47356 (0.44646)
2022-05-07 00:01:00,353 INFO 	Epoch: [20][90/309]	Batch time 4.4217 (4.2099)	Loss 0.43415 (0.44455)
2022-05-07 00:01:40,514 INFO 	Epoch: [20][100/309]	Batch time 4.0838 (4.1907)	Loss 0.47674 (0.44529)
2022-05-07 00:02:23,528 INFO 	Epoch: [20][110/309]	Batch time 4.2859 (4.2007)	Loss 0.41861 (0.44388)
2022-05-07 00:03:03,647 INFO 	Epoch: [20][120/309]	Batch time 3.8934 (4.1851)	Loss 0.48550 (0.44383)
2022-05-07 00:03:45,811 INFO 	Epoch: [20][130/309]	Batch time 4.2737 (4.1875)	Loss 0.39412 (0.44299)
2022-05-07 00:04:27,350 INFO 	Epoch: [20][140/309]	Batch time 4.7328 (4.1851)	Loss 0.35560 (0.44140)
2022-05-07 00:05:09,789 INFO 	Epoch: [20][150/309]	Batch time 4.0690 (4.1890)	Loss 0.46247 (0.44145)
2022-05-07 00:05:51,169 INFO 	Epoch: [20][160/309]	Batch time 4.1390 (4.1858)	Loss 0.45150 (0.44128)
2022-05-07 00:06:33,348 INFO 	Epoch: [20][170/309]	Batch time 4.1128 (4.1877)	Loss 0.41948 (0.44128)
2022-05-07 00:07:16,060 INFO 	Epoch: [20][180/309]	Batch time 3.7838 (4.1923)	Loss 0.45332 (0.44030)
2022-05-07 00:07:57,520 INFO 	Epoch: [20][190/309]	Batch time 4.0789 (4.1899)	Loss 0.44626 (0.44030)
2022-05-07 00:08:39,894 INFO 	Epoch: [20][200/309]	Batch time 4.0610 (4.1923)	Loss 0.42674 (0.44025)
2022-05-07 00:09:22,106 INFO 	Epoch: [20][210/309]	Batch time 4.1673 (4.1936)	Loss 0.38321 (0.44061)
2022-05-07 00:10:04,474 INFO 	Epoch: [20][220/309]	Batch time 4.3530 (4.1956)	Loss 0.43801 (0.43950)
2022-05-07 00:10:45,795 INFO 	Epoch: [20][230/309]	Batch time 3.9083 (4.1928)	Loss 0.49219 (0.43912)
2022-05-07 00:11:26,816 INFO 	Epoch: [20][240/309]	Batch time 3.9705 (4.1891)	Loss 0.44626 (0.43921)
2022-05-07 00:12:08,206 INFO 	Epoch: [20][250/309]	Batch time 4.0836 (4.1871)	Loss 0.43304 (0.43932)
2022-05-07 00:12:49,034 INFO 	Epoch: [20][260/309]	Batch time 4.0896 (4.1831)	Loss 0.44669 (0.43977)
2022-05-07 00:13:30,563 INFO 	Epoch: [20][270/309]	Batch time 3.8112 (4.1820)	Loss 0.43228 (0.43933)
2022-05-07 00:14:12,985 INFO 	Epoch: [20][280/309]	Batch time 4.5582 (4.1841)	Loss 0.42752 (0.43916)
2022-05-07 00:14:56,267 INFO 	Epoch: [20][290/309]	Batch time 4.2654 (4.1891)	Loss 0.46504 (0.43912)
2022-05-07 00:15:36,407 INFO 	Epoch: [20][300/309]	Batch time 4.6110 (4.1832)	Loss 0.39329 (0.43907)

Learning rate: 0.001
Step num: 6489

2022-05-07 00:16:15,662 INFO 	
Validation Loss 0.44701 (0.41861)


Epochs since last improvement: 2

Warning! Reached max decoder steps
/media/cv516/8e2bd071-88a1-41bb-a3dc-b7eadc0c192e/cv516/ceiling_workspace/renjie/GST-Tacotron2/utils.py:167: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  fig, axes = plt.subplots(1, len(data), figsize=figsize)
save mel done
2022-05-07 00:16:29,397 INFO 	Epoch: [21][0/309]	Batch time 7.6191 (7.6191)	Loss 0.36305 (0.36305)
2022-05-07 00:17:11,116 INFO 	Epoch: [21][10/309]	Batch time 3.8912 (4.4853)	Loss 0.44418 (0.43746)
2022-05-07 00:17:53,405 INFO 	Epoch: [21][20/309]	Batch time 3.9108 (4.3632)	Loss 0.45360 (0.43166)
2022-05-07 00:18:33,352 INFO 	Epoch: [21][30/309]	Batch time 4.1496 (4.2443)	Loss 0.41493 (0.43300)
2022-05-07 00:19:15,130 INFO 	Epoch: [21][40/309]	Batch time 4.5455 (4.2281)	Loss 0.39031 (0.43470)
2022-05-07 00:19:56,488 INFO 	Epoch: [21][50/309]	Batch time 4.0208 (4.2100)	Loss 0.49235 (0.43291)
2022-05-07 00:20:39,096 INFO 	Epoch: [21][60/309]	Batch time 4.2279 (4.2183)	Loss 0.43304 (0.43175)
2022-05-07 00:21:20,321 INFO 	Epoch: [21][70/309]	Batch time 4.3634 (4.2048)	Loss 0.40397 (0.43122)
2022-05-07 00:22:04,834 INFO 	Epoch: [21][80/309]	Batch time 4.4128 (4.2353)	Loss 0.39332 (0.42991)
2022-05-07 00:22:47,744 INFO 	Epoch: [21][90/309]	Batch time 4.8325 (4.2414)	Loss 0.36715 (0.42734)
2022-05-07 00:23:29,416 INFO 	Epoch: [21][100/309]	Batch time 4.3604 (4.2340)	Loss 0.40205 (0.42773)
2022-05-07 00:24:12,191 INFO 	Epoch: [21][110/309]	Batch time 4.7791 (4.2380)	Loss 0.36907 (0.42611)
2022-05-07 00:24:54,740 INFO 	Epoch: [21][120/309]	Batch time 4.2633 (4.2394)	Loss 0.45623 (0.42714)
2022-05-07 00:25:38,369 INFO 	Epoch: [21][130/309]	Batch time 5.1361 (4.2488)	Loss 0.35540 (0.42690)
2022-05-07 00:26:20,533 INFO 	Epoch: [21][140/309]	Batch time 4.4455 (4.2465)	Loss 0.39925 (0.42663)
2022-05-07 00:27:00,905 INFO 	Epoch: [21][150/309]	Batch time 4.0919 (4.2326)	Loss 0.43566 (0.42730)
2022-05-07 00:27:41,784 INFO 	Epoch: [21][160/309]	Batch time 4.0886 (4.2236)	Loss 0.43809 (0.42766)
2022-05-07 00:28:21,454 INFO 	Epoch: [21][170/309]	Batch time 3.9755 (4.2086)	Loss 0.40984 (0.42798)
2022-05-07 00:29:04,285 INFO 	Epoch: [21][180/309]	Batch time 4.1888 (4.2127)	Loss 0.32612 (0.42720)
2022-05-07 00:29:45,909 INFO 	Epoch: [21][190/309]	Batch time 4.2714 (4.2101)	Loss 0.43067 (0.42781)
2022-05-07 00:30:27,754 INFO 	Epoch: [21][200/309]	Batch time 4.4783 (4.2088)	Loss 0.39373 (0.42812)
2022-05-07 00:31:09,615 INFO 	Epoch: [21][210/309]	Batch time 4.1687 (4.2078)	Loss 0.42272 (0.42788)
2022-05-07 00:31:50,044 INFO 	Epoch: [21][220/309]	Batch time 4.2248 (4.2003)	Loss 0.42564 (0.42793)
2022-05-07 00:32:35,593 INFO 	Epoch: [21][230/309]	Batch time 4.8523 (4.2157)	Loss 0.38493 (0.42782)
2022-05-07 00:33:14,382 INFO 	Epoch: [21][240/309]	Batch time 4.3292 (4.2017)	Loss 0.39542 (0.42838)
2022-05-07 00:33:56,295 INFO 	Epoch: [21][250/309]	Batch time 4.3483 (4.2013)	Loss 0.45644 (0.42887)
2022-05-07 00:34:38,594 INFO 	Epoch: [21][260/309]	Batch time 4.2620 (4.2024)	Loss 0.43908 (0.42782)
2022-05-07 00:35:20,156 INFO 	Epoch: [21][270/309]	Batch time 4.5908 (4.2007)	Loss 0.37076 (0.42825)
2022-05-07 00:36:00,421 INFO 	Epoch: [21][280/309]	Batch time 3.9234 (4.1945)	Loss 0.42420 (0.42861)
2022-05-07 00:36:39,921 INFO 	Epoch: [21][290/309]	Batch time 3.8776 (4.1861)	Loss 0.49268 (0.42940)
2022-05-07 00:37:22,083 INFO 	Epoch: [21][300/309]	Batch time 3.6429 (4.1871)	Loss 0.42694 (0.42877)

Learning rate: 0.001
Step num: 6798

2022-05-07 00:38:00,154 INFO 	
Validation Loss 0.43234 (0.40630)

Warning! Reached max decoder steps
save mel done
2022-05-07 00:38:15,881 INFO 	Epoch: [22][0/309]	Batch time 7.0574 (7.0574)	Loss 0.41387 (0.41387)
2022-05-07 00:38:56,082 INFO 	Epoch: [22][10/309]	Batch time 4.0643 (4.2962)	Loss 0.41148 (0.43644)
2022-05-07 00:39:36,672 INFO 	Epoch: [22][20/309]	Batch time 4.0170 (4.1833)	Loss 0.41574 (0.42932)
2022-05-07 00:40:17,817 INFO 	Epoch: [22][30/309]	Batch time 4.0377 (4.1611)	Loss 0.43344 (0.43350)
2022-05-07 00:40:58,875 INFO 	Epoch: [22][40/309]	Batch time 3.9891 (4.1476)	Loss 0.42527 (0.42907)
2022-05-07 00:41:40,183 INFO 	Epoch: [22][50/309]	Batch time 4.0899 (4.1443)	Loss 0.42010 (0.42874)
2022-05-07 00:42:21,690 INFO 	Epoch: [22][60/309]	Batch time 4.4392 (4.1453)	Loss 0.40091 (0.42818)
2022-05-07 00:43:02,525 INFO 	Epoch: [22][70/309]	Batch time 4.1221 (4.1366)	Loss 0.41789 (0.42766)
2022-05-07 00:43:43,773 INFO 	Epoch: [22][80/309]	Batch time 3.8758 (4.1352)	Loss 0.45456 (0.42710)
2022-05-07 00:44:24,840 INFO 	Epoch: [22][90/309]	Batch time 4.1901 (4.1320)	Loss 0.41756 (0.42624)
2022-05-07 00:45:06,738 INFO 	Epoch: [22][100/309]	Batch time 4.2808 (4.1378)	Loss 0.38793 (0.42502)
2022-05-07 00:45:48,119 INFO 	Epoch: [22][110/309]	Batch time 3.8384 (4.1378)	Loss 0.45059 (0.42479)
2022-05-07 00:46:30,734 INFO 	Epoch: [22][120/309]	Batch time 3.8611 (4.1480)	Loss 0.45151 (0.42428)
2022-05-07 00:47:13,297 INFO 	Epoch: [22][130/309]	Batch time 4.2316 (4.1563)	Loss 0.40076 (0.42328)
2022-05-07 00:47:55,062 INFO 	Epoch: [22][140/309]	Batch time 4.2316 (4.1577)	Loss 0.39674 (0.42296)
2022-05-07 00:48:37,089 INFO 	Epoch: [22][150/309]	Batch time 4.3718 (4.1607)	Loss 0.38894 (0.42196)
2022-05-07 00:49:20,832 INFO 	Epoch: [22][160/309]	Batch time 3.8884 (4.1740)	Loss 0.45292 (0.42137)
2022-05-07 00:50:03,846 INFO 	Epoch: [22][170/309]	Batch time 3.9722 (4.1814)	Loss 0.42219 (0.42028)
2022-05-07 00:50:49,214 INFO 	Epoch: [22][180/309]	Batch time 3.9186 (4.2011)	Loss 0.45387 (0.41999)
2022-05-07 00:51:29,265 INFO 	Epoch: [22][190/309]	Batch time 3.9962 (4.1908)	Loss 0.44541 (0.41973)
2022-05-07 00:52:09,982 INFO 	Epoch: [22][200/309]	Batch time 4.0708 (4.1849)	Loss 0.37295 (0.41979)
2022-05-07 00:52:52,621 INFO 	Epoch: [22][210/309]	Batch time 4.2508 (4.1886)	Loss 0.42306 (0.42009)
2022-05-07 00:53:35,633 INFO 	Epoch: [22][220/309]	Batch time 4.5227 (4.1937)	Loss 0.41544 (0.42012)
2022-05-07 00:54:16,255 INFO 	Epoch: [22][230/309]	Batch time 4.1207 (4.1880)	Loss 0.44854 (0.42031)
2022-05-07 00:54:58,607 INFO 	Epoch: [22][240/309]	Batch time 4.8244 (4.1900)	Loss 0.37642 (0.42031)
2022-05-07 00:55:39,641 INFO 	Epoch: [22][250/309]	Batch time 4.2165 (4.1865)	Loss 0.43260 (0.42069)
2022-05-07 00:56:21,396 INFO 	Epoch: [22][260/309]	Batch time 3.8954 (4.1861)	Loss 0.47139 (0.42107)
2022-05-07 00:57:02,065 INFO 	Epoch: [22][270/309]	Batch time 4.1515 (4.1817)	Loss 0.39930 (0.42142)
2022-05-07 00:57:44,172 INFO 	Epoch: [22][280/309]	Batch time 4.2530 (4.1827)	Loss 0.37201 (0.42103)
2022-05-07 00:58:24,844 INFO 	Epoch: [22][290/309]	Batch time 3.9394 (4.1788)	Loss 0.45123 (0.42178)
2022-05-07 00:59:08,449 INFO 	Epoch: [22][300/309]	Batch time 4.1701 (4.1848)	Loss 0.42600 (0.42139)

Learning rate: 0.001
Step num: 7107

2022-05-07 00:59:47,492 INFO 	
Validation Loss 0.42726 (0.40120)

Warning! Reached max decoder steps
save mel done
2022-05-07 01:00:03,150 INFO 	Epoch: [23][0/309]	Batch time 6.9625 (6.9625)	Loss 0.39625 (0.39625)
2022-05-07 01:00:44,101 INFO 	Epoch: [23][10/309]	Batch time 4.3088 (4.3558)	Loss 0.38295 (0.41753)
2022-05-07 01:01:25,547 INFO 	Epoch: [23][20/309]	Batch time 4.6496 (4.2552)	Loss 0.34282 (0.41293)
2022-05-07 01:02:06,497 INFO 	Epoch: [23][30/309]	Batch time 4.1432 (4.2036)	Loss 0.41587 (0.41811)
2022-05-07 01:02:48,859 INFO 	Epoch: [23][40/309]	Batch time 4.1799 (4.2115)	Loss 0.41336 (0.41763)
2022-05-07 01:03:30,208 INFO 	Epoch: [23][50/309]	Batch time 4.1878 (4.1965)	Loss 0.44107 (0.41862)
2022-05-07 01:04:13,147 INFO 	Epoch: [23][60/309]	Batch time 4.3877 (4.2125)	Loss 0.42608 (0.41857)
2022-05-07 01:04:55,554 INFO 	Epoch: [23][70/309]	Batch time 4.5543 (4.2164)	Loss 0.41421 (0.41766)
2022-05-07 01:05:37,521 INFO 	Epoch: [23][80/309]	Batch time 4.8239 (4.2140)	Loss 0.34410 (0.41639)
2022-05-07 01:06:19,538 INFO 	Epoch: [23][90/309]	Batch time 4.9690 (4.2126)	Loss 0.33094 (0.41493)
2022-05-07 01:07:01,710 INFO 	Epoch: [23][100/309]	Batch time 4.2095 (4.2131)	Loss 0.42173 (0.41389)
2022-05-07 01:07:42,831 INFO 	Epoch: [23][110/309]	Batch time 4.1739 (4.2040)	Loss 0.42932 (0.41476)
2022-05-07 01:08:29,334 INFO 	Epoch: [23][120/309]	Batch time 4.3204 (4.2409)	Loss 0.40502 (0.41350)
2022-05-07 01:09:09,606 INFO 	Epoch: [23][130/309]	Batch time 4.1383 (4.2246)	Loss 0.43670 (0.41333)
2022-05-07 01:09:52,659 INFO 	Epoch: [23][140/309]	Batch time 4.2277 (4.2303)	Loss 0.40632 (0.41265)
2022-05-07 01:10:33,985 INFO 	Epoch: [23][150/309]	Batch time 4.1330 (4.2238)	Loss 0.41719 (0.41369)
2022-05-07 01:11:15,615 INFO 	Epoch: [23][160/309]	Batch time 4.1484 (4.2200)	Loss 0.42317 (0.41396)
2022-05-07 01:11:57,202 INFO 	Epoch: [23][170/309]	Batch time 4.4359 (4.2165)	Loss 0.36351 (0.41375)
2022-05-07 01:12:37,819 INFO 	Epoch: [23][180/309]	Batch time 4.2349 (4.2079)	Loss 0.44222 (0.41393)
2022-05-07 01:13:18,958 INFO 	Epoch: [23][190/309]	Batch time 4.2963 (4.2030)	Loss 0.44393 (0.41394)
2022-05-07 01:14:00,731 INFO 	Epoch: [23][200/309]	Batch time 3.8403 (4.2017)	Loss 0.41591 (0.41418)
2022-05-07 01:14:43,451 INFO 	Epoch: [23][210/309]	Batch time 5.1182 (4.2050)	Loss 0.33626 (0.41441)
2022-05-07 01:15:26,894 INFO 	Epoch: [23][220/309]	Batch time 4.2093 (4.2113)	Loss 0.44759 (0.41424)
2022-05-07 01:16:09,483 INFO 	Epoch: [23][230/309]	Batch time 3.6746 (4.2134)	Loss 0.45171 (0.41390)
2022-05-07 01:16:50,900 INFO 	Epoch: [23][240/309]	Batch time 4.1515 (4.2104)	Loss 0.41148 (0.41349)
2022-05-07 01:17:32,650 INFO 	Epoch: [23][250/309]	Batch time 4.2122 (4.2090)	Loss 0.43277 (0.41384)
2022-05-07 01:18:14,644 INFO 	Epoch: [23][260/309]	Batch time 4.2701 (4.2086)	Loss 0.42324 (0.41376)
2022-05-07 01:18:55,632 INFO 	Epoch: [23][270/309]	Batch time 4.1086 (4.2046)	Loss 0.47170 (0.41437)
2022-05-07 01:19:36,504 INFO 	Epoch: [23][280/309]	Batch time 4.1147 (4.2004)	Loss 0.39985 (0.41392)
2022-05-07 01:20:18,380 INFO 	Epoch: [23][290/309]	Batch time 4.1845 (4.2000)	Loss 0.40245 (0.41417)
2022-05-07 01:20:58,551 INFO 	Epoch: [23][300/309]	Batch time 3.9840 (4.1939)	Loss 0.44975 (0.41458)

Learning rate: 0.001
Step num: 7416

2022-05-07 01:21:36,563 INFO 	
Validation Loss 0.42362 (0.39904)

Warning! Reached max decoder steps
save mel done
2022-05-07 01:21:52,313 INFO 	Epoch: [24][0/309]	Batch time 7.1606 (7.1606)	Loss 0.42583 (0.42583)
2022-05-07 01:22:33,136 INFO 	Epoch: [24][10/309]	Batch time 4.7032 (4.3621)	Loss 0.34396 (0.40933)
2022-05-07 01:23:15,960 INFO 	Epoch: [24][20/309]	Batch time 4.2148 (4.3241)	Loss 0.41607 (0.40371)
2022-05-07 01:23:58,379 INFO 	Epoch: [24][30/309]	Batch time 4.2012 (4.2976)	Loss 0.41740 (0.40472)
2022-05-07 01:24:40,884 INFO 	Epoch: [24][40/309]	Batch time 3.8691 (4.2861)	Loss 0.44484 (0.40504)
2022-05-07 01:25:22,214 INFO 	Epoch: [24][50/309]	Batch time 3.9940 (4.2561)	Loss 0.45959 (0.40659)
2022-05-07 01:26:06,536 INFO 	Epoch: [24][60/309]	Batch time 5.0449 (4.2850)	Loss 0.40745 (0.40794)
2022-05-07 01:26:48,093 INFO 	Epoch: [24][70/309]	Batch time 4.1279 (4.2668)	Loss 0.45872 (0.40974)
2022-05-07 01:27:28,710 INFO 	Epoch: [24][80/309]	Batch time 4.7102 (4.2414)	Loss 0.35256 (0.41048)
2022-05-07 01:28:10,276 INFO 	Epoch: [24][90/309]	Batch time 4.0628 (4.2321)	Loss 0.39694 (0.41199)
2022-05-07 01:28:52,061 INFO 	Epoch: [24][100/309]	Batch time 4.0129 (4.2268)	Loss 0.40008 (0.41133)
2022-05-07 01:29:33,224 INFO 	Epoch: [24][110/309]	Batch time 3.8856 (4.2169)	Loss 0.46928 (0.41122)
2022-05-07 01:30:14,422 INFO 	Epoch: [24][120/309]	Batch time 3.8103 (4.2088)	Loss 0.43239 (0.41167)
2022-05-07 01:30:55,836 INFO 	Epoch: [24][130/309]	Batch time 4.4882 (4.2037)	Loss 0.38217 (0.41196)
2022-05-07 01:31:37,333 INFO 	Epoch: [24][140/309]	Batch time 4.0745 (4.1999)	Loss 0.38631 (0.41205)
2022-05-07 01:32:18,639 INFO 	Epoch: [24][150/309]	Batch time 4.2785 (4.1953)	Loss 0.42478 (0.41176)
2022-05-07 01:33:00,175 INFO 	Epoch: [24][160/309]	Batch time 3.5614 (4.1927)	Loss 0.40600 (0.41253)
2022-05-07 01:33:40,927 INFO 	Epoch: [24][170/309]	Batch time 4.1062 (4.1858)	Loss 0.35739 (0.41256)
2022-05-07 01:34:20,600 INFO 	Epoch: [24][180/309]	Batch time 3.4474 (4.1737)	Loss 0.45011 (0.41389)
2022-05-07 01:35:02,211 INFO 	Epoch: [24][190/309]	Batch time 3.9281 (4.1731)	Loss 0.43488 (0.41364)
2022-05-07 01:35:43,620 INFO 	Epoch: [24][200/309]	Batch time 3.9920 (4.1715)	Loss 0.42026 (0.41283)
2022-05-07 01:36:25,304 INFO 	Epoch: [24][210/309]	Batch time 4.3787 (4.1713)	Loss 0.38071 (0.41306)
2022-05-07 01:37:07,482 INFO 	Epoch: [24][220/309]	Batch time 4.2267 (4.1734)	Loss 0.41060 (0.41283)
2022-05-07 01:37:48,340 INFO 	Epoch: [24][230/309]	Batch time 3.6238 (4.1696)	Loss 0.44371 (0.41276)
2022-05-07 01:38:31,245 INFO 	Epoch: [24][240/309]	Batch time 4.0827 (4.1747)	Loss 0.38587 (0.41202)
2022-05-07 01:39:12,444 INFO 	Epoch: [24][250/309]	Batch time 4.3338 (4.1725)	Loss 0.40699 (0.41182)
2022-05-07 01:39:56,550 INFO 	Epoch: [24][260/309]	Batch time 4.2634 (4.1816)	Loss 0.42542 (0.41125)
2022-05-07 01:40:37,248 INFO 	Epoch: [24][270/309]	Batch time 4.0102 (4.1775)	Loss 0.42638 (0.41168)
2022-05-07 01:41:20,827 INFO 	Epoch: [24][280/309]	Batch time 4.0223 (4.1839)	Loss 0.43136 (0.41131)
2022-05-07 01:42:04,782 INFO 	Epoch: [24][290/309]	Batch time 4.0769 (4.1912)	Loss 0.40037 (0.41055)
2022-05-07 01:42:46,191 INFO 	Epoch: [24][300/309]	Batch time 3.9017 (4.1895)	Loss 0.43645 (0.41094)

Learning rate: 0.001
Step num: 7725

2022-05-07 01:43:24,445 INFO 	
Validation Loss 0.41875 (0.39420)

Warning! Reached max decoder steps
save mel done
2022-05-07 01:43:40,739 INFO 	Epoch: [25][0/309]	Batch time 7.6618 (7.6618)	Loss 0.32724 (0.32724)
2022-05-07 01:44:25,093 INFO 	Epoch: [25][10/309]	Batch time 3.9028 (4.7288)	Loss 0.43337 (0.40715)
2022-05-07 01:45:03,528 INFO 	Epoch: [25][20/309]	Batch time 3.7248 (4.3072)	Loss 0.41014 (0.41298)
2022-05-07 01:45:42,912 INFO 	Epoch: [25][30/309]	Batch time 4.0402 (4.1882)	Loss 0.39077 (0.41046)
2022-05-07 01:46:22,261 INFO 	Epoch: [25][40/309]	Batch time 3.9542 (4.1264)	Loss 0.39447 (0.40936)
2022-05-07 01:47:01,675 INFO 	Epoch: [25][50/309]	Batch time 4.0814 (4.0902)	Loss 0.44707 (0.40742)
2022-05-07 01:47:38,980 INFO 	Epoch: [25][60/309]	Batch time 3.3371 (4.0312)	Loss 0.41183 (0.40891)
2022-05-07 01:48:11,127 INFO 	Epoch: [25][70/309]	Batch time 3.1648 (3.9162)	Loss 0.41585 (0.40869)
2022-05-07 01:48:44,455 INFO 	Epoch: [25][80/309]	Batch time 3.4210 (3.8442)	Loss 0.42473 (0.40903)
2022-05-07 01:49:18,335 INFO 	Epoch: [25][90/309]	Batch time 3.4695 (3.7940)	Loss 0.35560 (0.40892)
2022-05-07 01:49:51,598 INFO 	Epoch: [25][100/309]	Batch time 3.2350 (3.7477)	Loss 0.39534 (0.40891)
2022-05-07 01:50:22,243 INFO 	Epoch: [25][110/309]	Batch time 3.1443 (3.6862)	Loss 0.40462 (0.40965)
2022-05-07 01:50:53,560 INFO 	Epoch: [25][120/309]	Batch time 2.9193 (3.6404)	Loss 0.43553 (0.41005)
2022-05-07 01:51:24,549 INFO 	Epoch: [25][130/309]	Batch time 3.2905 (3.5990)	Loss 0.39173 (0.41032)
2022-05-07 01:51:57,630 INFO 	Epoch: [25][140/309]	Batch time 3.5468 (3.5784)	Loss 0.36534 (0.40974)
2022-05-07 01:52:27,985 INFO 	Epoch: [25][150/309]	Batch time 2.7563 (3.5424)	Loss 0.47544 (0.41072)
2022-05-07 01:52:59,383 INFO 	Epoch: [25][160/309]	Batch time 3.2120 (3.5174)	Loss 0.40062 (0.41026)
2022-05-07 01:53:30,590 INFO 	Epoch: [25][170/309]	Batch time 3.0242 (3.4942)	Loss 0.46206 (0.41026)
2022-05-07 01:54:00,640 INFO 	Epoch: [25][180/309]	Batch time 3.0788 (3.4672)	Loss 0.42992 (0.41057)
2022-05-07 01:54:32,089 INFO 	Epoch: [25][190/309]	Batch time 3.3998 (3.4503)	Loss 0.32723 (0.40954)
2022-05-07 01:55:02,121 INFO 	Epoch: [25][200/309]	Batch time 2.9683 (3.4281)	Loss 0.39783 (0.40918)
2022-05-07 01:55:33,622 INFO 	Epoch: [25][210/309]	Batch time 3.1577 (3.4149)	Loss 0.40182 (0.40870)
2022-05-07 01:56:00,868 INFO 	Epoch: [25][220/309]	Batch time 2.9136 (3.3837)	Loss 0.37585 (0.40880)
2022-05-07 01:56:28,685 INFO 	Epoch: [25][230/309]	Batch time 2.7968 (3.3576)	Loss 0.38613 (0.40855)
2022-05-07 01:56:57,934 INFO 	Epoch: [25][240/309]	Batch time 2.8935 (3.3397)	Loss 0.43200 (0.40820)
2022-05-07 01:57:26,799 INFO 	Epoch: [25][250/309]	Batch time 3.1039 (3.3216)	Loss 0.38570 (0.40788)
2022-05-07 01:57:54,987 INFO 	Epoch: [25][260/309]	Batch time 2.9494 (3.3023)	Loss 0.39129 (0.40745)
2022-05-07 01:58:24,367 INFO 	Epoch: [25][270/309]	Batch time 2.9327 (3.2889)	Loss 0.37654 (0.40814)
2022-05-07 01:58:53,830 INFO 	Epoch: [25][280/309]	Batch time 2.7053 (3.2767)	Loss 0.47951 (0.40748)
2022-05-07 01:59:23,845 INFO 	Epoch: [25][290/309]	Batch time 2.9217 (3.2672)	Loss 0.42322 (0.40726)
2022-05-07 01:59:52,762 INFO 	Epoch: [25][300/309]	Batch time 2.3409 (3.2548)	Loss 0.42931 (0.40731)

Learning rate: 0.001
Step num: 8034

2022-05-07 02:00:18,648 INFO 	
Validation Loss 0.41244 (0.38736)

Warning! Reached max decoder steps
save mel done
2022-05-07 02:00:30,794 INFO 	Epoch: [26][0/309]	Batch time 4.8359 (4.8359)	Loss 0.38279 (0.38279)
2022-05-07 02:00:57,508 INFO 	Epoch: [26][10/309]	Batch time 2.8277 (2.8682)	Loss 0.40059 (0.40771)
2022-05-07 02:01:24,653 INFO 	Epoch: [26][20/309]	Batch time 3.0043 (2.7950)	Loss 0.40376 (0.41283)
2022-05-07 02:01:52,256 INFO 	Epoch: [26][30/309]	Batch time 2.8157 (2.7838)	Loss 0.40456 (0.40993)
2022-05-07 02:02:20,597 INFO 	Epoch: [26][40/309]	Batch time 2.8832 (2.7961)	Loss 0.40467 (0.40628)
2022-05-07 02:02:49,112 INFO 	Epoch: [26][50/309]	Batch time 2.8212 (2.8069)	Loss 0.39595 (0.40438)
2022-05-07 02:03:17,198 INFO 	Epoch: [26][60/309]	Batch time 3.1135 (2.8072)	Loss 0.34896 (0.40180)
2022-05-07 02:03:44,029 INFO 	Epoch: [26][70/309]	Batch time 2.7551 (2.7897)	Loss 0.41946 (0.40436)
2022-05-07 02:04:12,128 INFO 	Epoch: [26][80/309]	Batch time 2.6964 (2.7922)	Loss 0.36938 (0.40385)
2022-05-07 02:04:40,196 INFO 	Epoch: [26][90/309]	Batch time 2.7769 (2.7938)	Loss 0.37903 (0.40319)
2022-05-07 02:05:08,857 INFO 	Epoch: [26][100/309]	Batch time 2.6276 (2.8010)	Loss 0.41432 (0.40201)
2022-05-07 02:05:37,127 INFO 	Epoch: [26][110/309]	Batch time 2.7905 (2.8033)	Loss 0.45597 (0.40162)
2022-05-07 02:06:04,244 INFO 	Epoch: [26][120/309]	Batch time 2.4454 (2.7957)	Loss 0.45895 (0.40308)
2022-05-07 02:06:31,622 INFO 	Epoch: [26][130/309]	Batch time 3.1927 (2.7913)	Loss 0.38457 (0.40433)
2022-05-07 02:06:59,764 INFO 	Epoch: [26][140/309]	Batch time 2.7342 (2.7929)	Loss 0.40513 (0.40415)
2022-05-07 02:07:27,558 INFO 	Epoch: [26][150/309]	Batch time 2.8354 (2.7920)	Loss 0.38106 (0.40434)
2022-05-07 02:07:55,071 INFO 	Epoch: [26][160/309]	Batch time 3.4663 (2.7895)	Loss 0.32731 (0.40408)
2022-05-07 02:08:23,329 INFO 	Epoch: [26][170/309]	Batch time 2.7778 (2.7916)	Loss 0.36658 (0.40379)
2022-05-07 02:08:52,136 INFO 	Epoch: [26][180/309]	Batch time 2.6618 (2.7966)	Loss 0.45076 (0.40369)
2022-05-07 02:09:19,461 INFO 	Epoch: [26][190/309]	Batch time 2.5521 (2.7932)	Loss 0.38495 (0.40334)
2022-05-07 02:09:47,068 INFO 	Epoch: [26][200/309]	Batch time 3.2070 (2.7916)	Loss 0.32068 (0.40268)
2022-05-07 02:10:15,088 INFO 	Epoch: [26][210/309]	Batch time 2.7466 (2.7921)	Loss 0.39207 (0.40273)
2022-05-07 02:10:42,816 INFO 	Epoch: [26][220/309]	Batch time 2.7011 (2.7912)	Loss 0.39999 (0.40273)
2022-05-07 02:11:10,303 INFO 	Epoch: [26][230/309]	Batch time 2.6589 (2.7894)	Loss 0.41839 (0.40295)
2022-05-07 02:11:38,735 INFO 	Epoch: [26][240/309]	Batch time 2.6860 (2.7916)	Loss 0.40214 (0.40212)
2022-05-07 02:12:06,362 INFO 	Epoch: [26][250/309]	Batch time 2.6610 (2.7905)	Loss 0.40870 (0.40183)
2022-05-07 02:12:33,200 INFO 	Epoch: [26][260/309]	Batch time 2.7817 (2.7864)	Loss 0.39032 (0.40214)
2022-05-07 02:13:00,882 INFO 	Epoch: [26][270/309]	Batch time 2.6070 (2.7857)	Loss 0.41335 (0.40232)
2022-05-07 02:13:28,120 INFO 	Epoch: [26][280/309]	Batch time 2.7588 (2.7835)	Loss 0.39284 (0.40243)
2022-05-07 02:13:54,745 INFO 	Epoch: [26][290/309]	Batch time 2.7612 (2.7793)	Loss 0.48793 (0.40307)
2022-05-07 02:14:21,678 INFO 	Epoch: [26][300/309]	Batch time 2.4260 (2.7765)	Loss 0.44048 (0.40326)

Learning rate: 0.001
Step num: 8343

2022-05-07 02:14:46,966 INFO 	
Validation Loss 0.41036 (0.38441)

save mel done
2022-05-07 02:14:58,552 INFO 	Epoch: [27][0/309]	Batch time 4.3311 (4.3311)	Loss 0.45200 (0.45200)
2022-05-07 02:15:26,341 INFO 	Epoch: [27][10/309]	Batch time 2.5884 (2.9200)	Loss 0.46285 (0.40040)
2022-05-07 02:15:54,883 INFO 	Epoch: [27][20/309]	Batch time 2.7164 (2.8887)	Loss 0.39423 (0.39390)
2022-05-07 02:16:23,047 INFO 	Epoch: [27][30/309]	Batch time 2.8389 (2.8654)	Loss 0.40049 (0.40105)
2022-05-07 02:16:51,728 INFO 	Epoch: [27][40/309]	Batch time 2.7408 (2.8660)	Loss 0.39795 (0.40134)
2022-05-07 02:17:19,898 INFO 	Epoch: [27][50/309]	Batch time 3.0148 (2.8564)	Loss 0.35316 (0.40014)
2022-05-07 02:17:48,585 INFO 	Epoch: [27][60/309]	Batch time 2.9724 (2.8584)	Loss 0.34309 (0.39746)
2022-05-07 02:18:16,265 INFO 	Epoch: [27][70/309]	Batch time 2.7348 (2.8457)	Loss 0.44748 (0.39943)
2022-05-07 02:18:43,368 INFO 	Epoch: [27][80/309]	Batch time 2.7811 (2.8290)	Loss 0.42586 (0.40242)
2022-05-07 02:19:11,503 INFO 	Epoch: [27][90/309]	Batch time 3.0171 (2.8273)	Loss 0.39221 (0.40284)
2022-05-07 02:19:40,042 INFO 	Epoch: [27][100/309]	Batch time 2.7807 (2.8299)	Loss 0.39996 (0.40160)
2022-05-07 02:20:07,899 INFO 	Epoch: [27][110/309]	Batch time 2.9763 (2.8259)	Loss 0.35525 (0.40079)
2022-05-07 02:20:35,042 INFO 	Epoch: [27][120/309]	Batch time 2.7305 (2.8167)	Loss 0.42037 (0.40160)
2022-05-07 02:21:02,998 INFO 	Epoch: [27][130/309]	Batch time 2.4734 (2.8151)	Loss 0.39133 (0.40109)
2022-05-07 02:21:29,790 INFO 	Epoch: [27][140/309]	Batch time 2.7588 (2.8055)	Loss 0.36048 (0.40177)
2022-05-07 02:21:57,412 INFO 	Epoch: [27][150/309]	Batch time 2.5856 (2.8026)	Loss 0.39173 (0.40192)
2022-05-07 02:22:25,437 INFO 	Epoch: [27][160/309]	Batch time 2.7152 (2.8026)	Loss 0.40487 (0.40105)
2022-05-07 02:22:52,895 INFO 	Epoch: [27][170/309]	Batch time 2.7369 (2.7993)	Loss 0.44203 (0.40060)
2022-05-07 02:23:20,023 INFO 	Epoch: [27][180/309]	Batch time 2.7489 (2.7945)	Loss 0.40430 (0.39997)
2022-05-07 02:23:47,528 INFO 	Epoch: [27][190/309]	Batch time 3.0347 (2.7922)	Loss 0.35679 (0.40000)
2022-05-07 02:24:14,179 INFO 	Epoch: [27][200/309]	Batch time 2.7364 (2.7859)	Loss 0.39424 (0.40019)
2022-05-07 02:24:40,592 INFO 	Epoch: [27][210/309]	Batch time 2.5516 (2.7790)	Loss 0.39969 (0.39979)
2022-05-07 02:25:07,739 INFO 	Epoch: [27][220/309]	Batch time 3.1874 (2.7761)	Loss 0.33044 (0.39947)
2022-05-07 02:25:34,325 INFO 	Epoch: [27][230/309]	Batch time 2.4804 (2.7710)	Loss 0.37070 (0.39934)
2022-05-07 02:26:01,745 INFO 	Epoch: [27][240/309]	Batch time 2.4991 (2.7698)	Loss 0.45216 (0.39903)
2022-05-07 02:26:29,128 INFO 	Epoch: [27][250/309]	Batch time 2.6762 (2.7686)	Loss 0.42489 (0.39905)
2022-05-07 02:26:57,811 INFO 	Epoch: [27][260/309]	Batch time 3.1412 (2.7724)	Loss 0.37064 (0.39868)
2022-05-07 02:27:27,450 INFO 	Epoch: [27][270/309]	Batch time 3.2780 (2.7794)	Loss 0.33110 (0.39816)
2022-05-07 02:27:54,434 INFO 	Epoch: [27][280/309]	Batch time 2.7906 (2.7766)	Loss 0.40449 (0.39848)
2022-05-07 02:28:23,172 INFO 	Epoch: [27][290/309]	Batch time 2.6111 (2.7799)	Loss 0.36512 (0.39805)
2022-05-07 02:28:50,803 INFO 	Epoch: [27][300/309]	Batch time 2.5297 (2.7793)	Loss 0.41990 (0.39839)

Learning rate: 0.001
Step num: 8652

2022-05-07 02:29:15,990 INFO 	
Validation Loss 0.41123 (0.38564)


Epochs since last improvement: 1

Warning! Reached max decoder steps
save mel done
2022-05-07 02:29:25,559 INFO 	Epoch: [28][0/309]	Batch time 4.7523 (4.7523)	Loss 0.41103 (0.41103)
2022-05-07 02:29:53,052 INFO 	Epoch: [28][10/309]	Batch time 2.5355 (2.9314)	Loss 0.39533 (0.39689)
2022-05-07 02:30:21,294 INFO 	Epoch: [28][20/309]	Batch time 2.8133 (2.8804)	Loss 0.40465 (0.39111)
2022-05-07 02:30:50,019 INFO 	Epoch: [28][30/309]	Batch time 2.7227 (2.8778)	Loss 0.36577 (0.39446)
2022-05-07 02:31:18,653 INFO 	Epoch: [28][40/309]	Batch time 2.6462 (2.8743)	Loss 0.43427 (0.39459)
2022-05-07 02:31:46,459 INFO 	Epoch: [28][50/309]	Batch time 2.5897 (2.8559)	Loss 0.41981 (0.39191)
2022-05-07 02:32:13,144 INFO 	Epoch: [28][60/309]	Batch time 3.0489 (2.8252)	Loss 0.35755 (0.39224)
2022-05-07 02:32:41,314 INFO 	Epoch: [28][70/309]	Batch time 2.7951 (2.8240)	Loss 0.40057 (0.39246)
2022-05-07 02:33:09,281 INFO 	Epoch: [28][80/309]	Batch time 3.0632 (2.8207)	Loss 0.38194 (0.39386)
2022-05-07 02:33:37,573 INFO 	Epoch: [28][90/309]	Batch time 2.5212 (2.8216)	Loss 0.42614 (0.39481)
2022-05-07 02:34:04,218 INFO 	Epoch: [28][100/309]	Batch time 2.6807 (2.8060)	Loss 0.40255 (0.39645)
2022-05-07 02:34:32,190 INFO 	Epoch: [28][110/309]	Batch time 2.7206 (2.8053)	Loss 0.38442 (0.39611)
2022-05-07 02:34:59,936 INFO 	Epoch: [28][120/309]	Batch time 2.9469 (2.8027)	Loss 0.36778 (0.39609)
2022-05-07 02:35:26,578 INFO 	Epoch: [28][130/309]	Batch time 2.7174 (2.7921)	Loss 0.44874 (0.39823)
2022-05-07 02:35:55,068 INFO 	Epoch: [28][140/309]	Batch time 2.7296 (2.7962)	Loss 0.45043 (0.39801)
2022-05-07 02:36:22,041 INFO 	Epoch: [28][150/309]	Batch time 2.6129 (2.7896)	Loss 0.39572 (0.39899)
2022-05-07 02:36:50,608 INFO 	Epoch: [28][160/309]	Batch time 2.9908 (2.7938)	Loss 0.34379 (0.39774)
2022-05-07 02:37:18,428 INFO 	Epoch: [28][170/309]	Batch time 2.5498 (2.7931)	Loss 0.38966 (0.39731)
2022-05-07 02:37:47,109 INFO 	Epoch: [28][180/309]	Batch time 2.7004 (2.7972)	Loss 0.43605 (0.39736)
2022-05-07 02:38:13,096 INFO 	Epoch: [28][190/309]	Batch time 2.7566 (2.7869)	Loss 0.37668 (0.39761)
2022-05-07 02:38:40,216 INFO 	Epoch: [28][200/309]	Batch time 2.7672 (2.7831)	Loss 0.38626 (0.39763)
2022-05-07 02:39:08,153 INFO 	Epoch: [28][210/309]	Batch time 3.1831 (2.7836)	Loss 0.33042 (0.39769)
2022-05-07 02:39:35,734 INFO 	Epoch: [28][220/309]	Batch time 2.9380 (2.7825)	Loss 0.37399 (0.39770)
2022-05-07 02:40:03,030 INFO 	Epoch: [28][230/309]	Batch time 2.7975 (2.7802)	Loss 0.41133 (0.39800)
2022-05-07 02:40:30,208 INFO 	Epoch: [28][240/309]	Batch time 2.4541 (2.7776)	Loss 0.45219 (0.39833)
2022-05-07 02:40:57,733 INFO 	Epoch: [28][250/309]	Batch time 3.1573 (2.7766)	Loss 0.35229 (0.39810)
2022-05-07 02:41:26,810 INFO 	Epoch: [28][260/309]	Batch time 2.8665 (2.7816)	Loss 0.42722 (0.39819)
2022-05-07 02:41:56,455 INFO 	Epoch: [28][270/309]	Batch time 3.1798 (2.7884)	Loss 0.32839 (0.39760)
2022-05-07 02:42:24,218 INFO 	Epoch: [28][280/309]	Batch time 3.0986 (2.7879)	Loss 0.34249 (0.39762)
2022-05-07 02:42:51,229 INFO 	Epoch: [28][290/309]	Batch time 2.6510 (2.7850)	Loss 0.41190 (0.39790)
2022-05-07 02:43:19,957 INFO 	Epoch: [28][300/309]	Batch time 2.5987 (2.7879)	Loss 0.38804 (0.39716)

Learning rate: 0.001
Step num: 8961

2022-05-07 02:43:44,109 INFO 	
Validation Loss 0.40166 (0.37702)

Warning! Reached max decoder steps
save mel done
2022-05-07 02:43:55,952 INFO 	Epoch: [29][0/309]	Batch time 4.4670 (4.4670)	Loss 0.46306 (0.46306)
2022-05-07 02:44:23,016 INFO 	Epoch: [29][10/309]	Batch time 2.6559 (2.8665)	Loss 0.36680 (0.39610)
2022-05-07 02:44:50,624 INFO 	Epoch: [29][20/309]	Batch time 2.6276 (2.8162)	Loss 0.41133 (0.39109)
2022-05-07 02:45:18,646 INFO 	Epoch: [29][30/309]	Batch time 2.5652 (2.8117)	Loss 0.44503 (0.39090)
2022-05-07 02:45:47,017 INFO 	Epoch: [29][40/309]	Batch time 2.7514 (2.8179)	Loss 0.36532 (0.39022)
2022-05-07 02:46:15,229 INFO 	Epoch: [29][50/309]	Batch time 2.8799 (2.8185)	Loss 0.40155 (0.39146)
2022-05-07 02:46:43,403 INFO 	Epoch: [29][60/309]	Batch time 2.7623 (2.8183)	Loss 0.35527 (0.39003)
2022-05-07 02:47:11,388 INFO 	Epoch: [29][70/309]	Batch time 2.4716 (2.8156)	Loss 0.38362 (0.38950)
2022-05-07 02:47:38,292 INFO 	Epoch: [29][80/309]	Batch time 2.5896 (2.8001)	Loss 0.41361 (0.39069)
2022-05-07 02:48:07,368 INFO 	Epoch: [29][90/309]	Batch time 2.6875 (2.8119)	Loss 0.40671 (0.38941)
2022-05-07 02:48:34,305 INFO 	Epoch: [29][100/309]	Batch time 2.7255 (2.8002)	Loss 0.40490 (0.38970)
2022-05-07 02:49:02,832 INFO 	Epoch: [29][110/309]	Batch time 2.8218 (2.8049)	Loss 0.40113 (0.38929)
2022-05-07 02:49:29,266 INFO 	Epoch: [29][120/309]	Batch time 2.3756 (2.7916)	Loss 0.43811 (0.38944)
2022-05-07 02:49:55,787 INFO 	Epoch: [29][130/309]	Batch time 2.5536 (2.7809)	Loss 0.38278 (0.38977)
2022-05-07 02:50:22,240 INFO 	Epoch: [29][140/309]	Batch time 2.5153 (2.7713)	Loss 0.36953 (0.38972)
2022-05-07 02:50:48,729 INFO 	Epoch: [29][150/309]	Batch time 2.8471 (2.7632)	Loss 0.36426 (0.39040)
2022-05-07 02:51:16,379 INFO 	Epoch: [29][160/309]	Batch time 2.5871 (2.7633)	Loss 0.39048 (0.39063)
2022-05-07 02:51:44,600 INFO 	Epoch: [29][170/309]	Batch time 2.4148 (2.7668)	Loss 0.42457 (0.39088)
2022-05-07 02:52:12,775 INFO 	Epoch: [29][180/309]	Batch time 2.8907 (2.7696)	Loss 0.38563 (0.39108)
2022-05-07 02:52:40,032 INFO 	Epoch: [29][190/309]	Batch time 2.6412 (2.7673)	Loss 0.40717 (0.39224)
2022-05-07 02:53:07,328 INFO 	Epoch: [29][200/309]	Batch time 2.3248 (2.7654)	Loss 0.43235 (0.39273)
2022-05-07 02:53:34,393 INFO 	Epoch: [29][210/309]	Batch time 2.6201 (2.7626)	Loss 0.41687 (0.39213)
2022-05-07 02:54:00,556 INFO 	Epoch: [29][220/309]	Batch time 2.4987 (2.7560)	Loss 0.42298 (0.39263)
2022-05-07 02:54:28,163 INFO 	Epoch: [29][230/309]	Batch time 2.6209 (2.7562)	Loss 0.40406 (0.39205)
2022-05-07 02:54:55,113 INFO 	Epoch: [29][240/309]	Batch time 2.4529 (2.7536)	Loss 0.41220 (0.39161)
2022-05-07 02:55:21,877 INFO 	Epoch: [29][250/309]	Batch time 2.9260 (2.7506)	Loss 0.34815 (0.39188)
2022-05-07 02:55:48,540 INFO 	Epoch: [29][260/309]	Batch time 2.6922 (2.7473)	Loss 0.40236 (0.39233)
2022-05-07 02:56:15,402 INFO 	Epoch: [29][270/309]	Batch time 2.8523 (2.7451)	Loss 0.39978 (0.39283)
2022-05-07 02:56:43,272 INFO 	Epoch: [29][280/309]	Batch time 2.6425 (2.7466)	Loss 0.41421 (0.39273)
2022-05-07 02:57:11,117 INFO 	Epoch: [29][290/309]	Batch time 3.0380 (2.7479)	Loss 0.35175 (0.39273)
2022-05-07 02:57:38,167 INFO 	Epoch: [29][300/309]	Batch time 2.8320 (2.7465)	Loss 0.39348 (0.39277)

Learning rate: 0.001
Step num: 9270

2022-05-07 02:58:02,926 INFO 	
Validation Loss 0.40495 (0.37866)


Epochs since last improvement: 1

Warning! Reached max decoder steps
save mel done
2022-05-07 02:58:12,353 INFO 	Epoch: [30][0/309]	Batch time 4.4022 (4.4022)	Loss 0.45039 (0.45039)
2022-05-07 02:58:40,072 INFO 	Epoch: [30][10/309]	Batch time 2.8033 (2.9201)	Loss 0.40548 (0.39430)
2022-05-07 02:59:07,428 INFO 	Epoch: [30][20/309]	Batch time 2.3822 (2.8323)	Loss 0.40656 (0.39778)
2022-05-07 02:59:35,033 INFO 	Epoch: [30][30/309]	Batch time 2.3534 (2.8091)	Loss 0.42156 (0.39258)
2022-05-07 03:00:00,734 INFO 	Epoch: [30][40/309]	Batch time 2.7385 (2.7508)	Loss 0.43371 (0.39496)
2022-05-07 03:00:28,710 INFO 	Epoch: [30][50/309]	Batch time 2.7247 (2.7600)	Loss 0.40742 (0.39269)
2022-05-07 03:00:56,712 INFO 	Epoch: [30][60/309]	Batch time 2.7487 (2.7666)	Loss 0.40879 (0.39524)
2022-05-07 03:01:24,348 INFO 	Epoch: [30][70/309]	Batch time 3.0119 (2.7662)	Loss 0.37663 (0.39541)
2022-05-07 03:01:52,350 INFO 	Epoch: [30][80/309]	Batch time 2.8034 (2.7704)	Loss 0.41213 (0.39430)
2022-05-07 03:02:20,587 INFO 	Epoch: [30][90/309]	Batch time 3.2786 (2.7762)	Loss 0.34129 (0.39410)
2022-05-07 03:02:49,655 INFO 	Epoch: [30][100/309]	Batch time 2.7985 (2.7892)	Loss 0.40598 (0.39310)
2022-05-07 03:03:18,118 INFO 	Epoch: [30][110/309]	Batch time 2.6785 (2.7943)	Loss 0.44059 (0.39229)
2022-05-07 03:03:46,768 INFO 	Epoch: [30][120/309]	Batch time 3.1277 (2.8002)	Loss 0.32360 (0.39135)
2022-05-07 03:04:14,797 INFO 	Epoch: [30][130/309]	Batch time 2.6930 (2.8004)	Loss 0.35056 (0.39213)
2022-05-07 03:04:41,399 INFO 	Epoch: [30][140/309]	Batch time 2.5373 (2.7904)	Loss 0.42212 (0.39242)
2022-05-07 03:05:08,540 INFO 	Epoch: [30][150/309]	Batch time 2.4060 (2.7854)	Loss 0.38219 (0.39174)
2022-05-07 03:05:35,684 INFO 	Epoch: [30][160/309]	Batch time 3.1997 (2.7810)	Loss 0.32394 (0.39112)
2022-05-07 03:06:02,579 INFO 	Epoch: [30][170/309]	Batch time 2.5584 (2.7756)	Loss 0.43461 (0.39114)
2022-05-07 03:06:31,432 INFO 	Epoch: [30][180/309]	Batch time 2.9190 (2.7817)	Loss 0.39925 (0.39040)
2022-05-07 03:06:58,865 INFO 	Epoch: [30][190/309]	Batch time 2.6773 (2.7797)	Loss 0.43691 (0.39075)
2022-05-07 03:07:27,010 INFO 	Epoch: [30][200/309]	Batch time 3.1552 (2.7814)	Loss 0.39059 (0.39135)
2022-05-07 03:07:54,515 INFO 	Epoch: [30][210/309]	Batch time 2.5983 (2.7799)	Loss 0.39300 (0.39087)
2022-05-07 03:08:22,139 INFO 	Epoch: [30][220/309]	Batch time 2.7176 (2.7791)	Loss 0.42826 (0.39022)
2022-05-07 03:08:50,085 INFO 	Epoch: [30][230/309]	Batch time 2.7697 (2.7798)	Loss 0.36981 (0.39047)
2022-05-07 03:09:17,338 INFO 	Epoch: [30][240/309]	Batch time 2.5081 (2.7775)	Loss 0.37181 (0.39035)
2022-05-07 03:09:45,524 INFO 	Epoch: [30][250/309]	Batch time 3.0214 (2.7792)	Loss 0.32882 (0.38980)
2022-05-07 03:10:11,259 INFO 	Epoch: [30][260/309]	Batch time 2.4751 (2.7713)	Loss 0.39240 (0.39001)
2022-05-07 03:10:38,739 INFO 	Epoch: [30][270/309]	Batch time 2.4004 (2.7704)	Loss 0.42272 (0.38966)
2022-05-07 03:11:05,784 INFO 	Epoch: [30][280/309]	Batch time 2.7290 (2.7681)	Loss 0.36178 (0.38921)
2022-05-07 03:11:32,229 INFO 	Epoch: [30][290/309]	Batch time 2.4540 (2.7638)	Loss 0.41641 (0.38927)
2022-05-07 03:11:59,319 INFO 	Epoch: [30][300/309]	Batch time 2.8920 (2.7620)	Loss 0.37121 (0.38908)

Learning rate: 0.001
Step num: 9579

2022-05-07 03:12:24,346 INFO 	
Validation Loss 0.39878 (0.37373)

Warning! Reached max decoder steps
save mel done
2022-05-07 03:12:35,993 INFO 	Epoch: [31][0/309]	Batch time 4.3545 (4.3545)	Loss 0.41797 (0.41797)
2022-05-07 03:13:03,626 INFO 	Epoch: [31][10/309]	Batch time 2.8548 (2.9080)	Loss 0.42752 (0.37446)
2022-05-07 03:13:30,802 INFO 	Epoch: [31][20/309]	Batch time 2.7556 (2.8173)	Loss 0.35819 (0.38499)
2022-05-07 03:13:59,697 INFO 	Epoch: [31][30/309]	Batch time 2.8940 (2.8406)	Loss 0.39948 (0.38430)
2022-05-07 03:14:27,234 INFO 	Epoch: [31][40/309]	Batch time 2.7218 (2.8194)	Loss 0.37939 (0.38480)
2022-05-07 03:14:55,880 INFO 	Epoch: [31][50/309]	Batch time 2.8955 (2.8283)	Loss 0.35883 (0.38476)
2022-05-07 03:15:22,150 INFO 	Epoch: [31][60/309]	Batch time 2.6207 (2.7953)	Loss 0.40642 (0.38536)
2022-05-07 03:15:49,354 INFO 	Epoch: [31][70/309]	Batch time 2.4885 (2.7847)	Loss 0.44172 (0.38556)
2022-05-07 03:16:16,857 INFO 	Epoch: [31][80/309]	Batch time 2.7202 (2.7805)	Loss 0.36036 (0.38447)
2022-05-07 03:16:44,196 INFO 	Epoch: [31][90/309]	Batch time 2.9852 (2.7754)	Loss 0.33331 (0.38521)
2022-05-07 03:17:11,533 INFO 	Epoch: [31][100/309]	Batch time 2.6158 (2.7712)	Loss 0.38659 (0.38482)
2022-05-07 03:17:39,021 INFO 	Epoch: [31][110/309]	Batch time 2.5888 (2.7692)	Loss 0.36238 (0.38373)
2022-05-07 03:18:04,969 INFO 	Epoch: [31][120/309]	Batch time 2.4919 (2.7548)	Loss 0.44221 (0.38539)
2022-05-07 03:18:31,558 INFO 	Epoch: [31][130/309]	Batch time 2.7367 (2.7475)	Loss 0.39604 (0.38637)
2022-05-07 03:18:57,245 INFO 	Epoch: [31][140/309]	Batch time 2.6432 (2.7348)	Loss 0.39293 (0.38768)
2022-05-07 03:19:24,442 INFO 	Epoch: [31][150/309]	Batch time 2.5653 (2.7338)	Loss 0.41720 (0.38834)
2022-05-07 03:19:51,551 INFO 	Epoch: [31][160/309]	Batch time 3.1778 (2.7324)	Loss 0.34114 (0.38869)
2022-05-07 03:20:18,114 INFO 	Epoch: [31][170/309]	Batch time 2.3658 (2.7279)	Loss 0.44278 (0.38917)
2022-05-07 03:20:47,165 INFO 	Epoch: [31][180/309]	Batch time 3.4259 (2.7377)	Loss 0.32660 (0.38828)
2022-05-07 03:21:15,063 INFO 	Epoch: [31][190/309]	Batch time 2.7562 (2.7404)	Loss 0.34734 (0.38795)
2022-05-07 03:21:43,788 INFO 	Epoch: [31][200/309]	Batch time 2.9249 (2.7470)	Loss 0.37434 (0.38815)
2022-05-07 03:22:12,483 INFO 	Epoch: [31][210/309]	Batch time 3.0673 (2.7528)	Loss 0.32633 (0.38748)
2022-05-07 03:22:39,103 INFO 	Epoch: [31][220/309]	Batch time 2.5688 (2.7487)	Loss 0.43114 (0.38812)
2022-05-07 03:23:07,708 INFO 	Epoch: [31][230/309]	Batch time 3.0914 (2.7535)	Loss 0.33549 (0.38700)
2022-05-07 03:23:35,136 INFO 	Epoch: [31][240/309]	Batch time 2.9484 (2.7531)	Loss 0.31105 (0.38678)
2022-05-07 03:24:01,903 INFO 	Epoch: [31][250/309]	Batch time 2.7150 (2.7501)	Loss 0.39320 (0.38711)
2022-05-07 03:24:29,173 INFO 	Epoch: [31][260/309]	Batch time 2.9077 (2.7492)	Loss 0.36293 (0.38773)
2022-05-07 03:24:57,315 INFO 	Epoch: [31][270/309]	Batch time 3.3487 (2.7516)	Loss 0.33046 (0.38750)
2022-05-07 03:25:26,181 INFO 	Epoch: [31][280/309]	Batch time 2.6797 (2.7564)	Loss 0.42597 (0.38714)
2022-05-07 03:25:53,804 INFO 	Epoch: [31][290/309]	Batch time 2.6496 (2.7566)	Loss 0.40842 (0.38699)
2022-05-07 03:26:21,854 INFO 	Epoch: [31][300/309]	Batch time 2.6628 (2.7582)	Loss 0.39169 (0.38645)

Learning rate: 0.001
Step num: 9888

2022-05-07 03:26:46,938 INFO 	
Validation Loss 0.39303 (0.37125)

Warning! Reached max decoder steps
save mel done
2022-05-07 03:26:59,199 INFO 	Epoch: [32][0/309]	Batch time 4.7917 (4.7917)	Loss 0.36369 (0.36369)
2022-05-07 03:27:27,246 INFO 	Epoch: [32][10/309]	Batch time 2.7176 (2.9854)	Loss 0.37186 (0.36677)
2022-05-07 03:27:53,562 INFO 	Epoch: [32][20/309]	Batch time 2.7123 (2.8169)	Loss 0.38369 (0.37307)
2022-05-07 03:28:20,882 INFO 	Epoch: [32][30/309]	Batch time 3.1096 (2.7895)	Loss 0.33260 (0.37878)
2022-05-07 03:28:47,449 INFO 	Epoch: [32][40/309]	Batch time 2.6882 (2.7571)	Loss 0.39077 (0.38273)
2022-05-07 03:29:14,461 INFO 	Epoch: [32][50/309]	Batch time 3.0804 (2.7462)	Loss 0.34716 (0.38493)
2022-05-07 03:29:40,214 INFO 	Epoch: [32][60/309]	Batch time 2.4420 (2.7182)	Loss 0.42706 (0.38918)
2022-05-07 03:30:07,146 INFO 	Epoch: [32][70/309]	Batch time 2.5935 (2.7146)	Loss 0.38863 (0.38988)
2022-05-07 03:30:34,581 INFO 	Epoch: [32][80/309]	Batch time 2.7666 (2.7182)	Loss 0.35113 (0.38836)
2022-05-07 03:31:01,488 INFO 	Epoch: [32][90/309]	Batch time 2.6925 (2.7152)	Loss 0.42889 (0.38969)
2022-05-07 03:31:28,371 INFO 	Epoch: [32][100/309]	Batch time 2.6599 (2.7125)	Loss 0.42342 (0.39022)
2022-05-07 03:31:55,338 INFO 	Epoch: [32][110/309]	Batch time 2.5567 (2.7111)	Loss 0.39860 (0.39024)
2022-05-07 03:32:22,735 INFO 	Epoch: [32][120/309]	Batch time 2.6051 (2.7135)	Loss 0.41865 (0.39081)
2022-05-07 03:32:50,560 INFO 	Epoch: [32][130/309]	Batch time 3.0187 (2.7187)	Loss 0.38384 (0.38992)
2022-05-07 03:33:20,208 INFO 	Epoch: [32][140/309]	Batch time 2.9679 (2.7362)	Loss 0.37142 (0.38844)
2022-05-07 03:33:47,366 INFO 	Epoch: [32][150/309]	Batch time 2.5998 (2.7348)	Loss 0.38636 (0.38820)
2022-05-07 03:34:15,091 INFO 	Epoch: [32][160/309]	Batch time 3.0580 (2.7372)	Loss 0.35090 (0.38799)
2022-05-07 03:34:42,739 INFO 	Epoch: [32][170/309]	Batch time 2.9217 (2.7388)	Loss 0.35558 (0.38702)
2022-05-07 03:35:10,809 INFO 	Epoch: [32][180/309]	Batch time 2.6309 (2.7426)	Loss 0.42376 (0.38722)
2022-05-07 03:35:39,109 INFO 	Epoch: [32][190/309]	Batch time 2.5556 (2.7471)	Loss 0.41716 (0.38709)
2022-05-07 03:36:07,647 INFO 	Epoch: [32][200/309]	Batch time 2.7731 (2.7524)	Loss 0.41977 (0.38715)
2022-05-07 03:36:35,693 INFO 	Epoch: [32][210/309]	Batch time 2.4931 (2.7549)	Loss 0.39467 (0.38702)
2022-05-07 03:37:03,686 INFO 	Epoch: [32][220/309]	Batch time 2.6437 (2.7569)	Loss 0.37197 (0.38712)
2022-05-07 03:37:31,311 INFO 	Epoch: [32][230/309]	Batch time 2.4296 (2.7572)	Loss 0.40695 (0.38666)
2022-05-07 03:37:59,344 INFO 	Epoch: [32][240/309]	Batch time 2.7157 (2.7591)	Loss 0.38758 (0.38657)
2022-05-07 03:38:27,639 INFO 	Epoch: [32][250/309]	Batch time 2.7328 (2.7619)	Loss 0.37222 (0.38582)
2022-05-07 03:38:54,747 INFO 	Epoch: [32][260/309]	Batch time 3.1798 (2.7599)	Loss 0.34926 (0.38612)
2022-05-07 03:39:23,030 INFO 	Epoch: [32][270/309]	Batch time 2.7443 (2.7624)	Loss 0.34859 (0.38555)
2022-05-07 03:39:51,544 INFO 	Epoch: [32][280/309]	Batch time 3.0861 (2.7656)	Loss 0.34863 (0.38555)
2022-05-07 03:40:19,156 INFO 	Epoch: [32][290/309]	Batch time 2.5322 (2.7655)	Loss 0.42084 (0.38557)
2022-05-07 03:40:46,651 INFO 	Epoch: [32][300/309]	Batch time 2.5361 (2.7649)	Loss 0.39021 (0.38555)

Learning rate: 0.001
Step num: 10197

2022-05-07 03:41:12,344 INFO 	
Validation Loss 0.39410 (0.37136)


Epochs since last improvement: 1

Warning! Reached max decoder steps
save mel done
2022-05-07 03:41:21,912 INFO 	Epoch: [33][0/309]	Batch time 4.5372 (4.5372)	Loss 0.36567 (0.36567)
2022-05-07 03:41:49,122 INFO 	Epoch: [33][10/309]	Batch time 2.4839 (2.8861)	Loss 0.40993 (0.37895)
2022-05-07 03:42:16,366 INFO 	Epoch: [33][20/309]	Batch time 2.7613 (2.8091)	Loss 0.39223 (0.38217)
2022-05-07 03:42:44,204 INFO 	Epoch: [33][30/309]	Batch time 2.6007 (2.8009)	Loss 0.39998 (0.38062)
2022-05-07 03:43:11,558 INFO 	Epoch: [33][40/309]	Batch time 2.7217 (2.7850)	Loss 0.39058 (0.38062)
2022-05-07 03:43:39,770 INFO 	Epoch: [33][50/309]	Batch time 2.4932 (2.7921)	Loss 0.39390 (0.38071)
2022-05-07 03:44:05,865 INFO 	Epoch: [33][60/309]	Batch time 2.7747 (2.7621)	Loss 0.37308 (0.38472)
2022-05-07 03:44:32,717 INFO 	Epoch: [33][70/309]	Batch time 2.4562 (2.7513)	Loss 0.41958 (0.38258)
2022-05-07 03:45:00,453 INFO 	Epoch: [33][80/309]	Batch time 2.8051 (2.7540)	Loss 0.38482 (0.38351)
2022-05-07 03:45:28,495 INFO 	Epoch: [33][90/309]	Batch time 2.7482 (2.7596)	Loss 0.37299 (0.38147)
2022-05-07 03:45:55,148 INFO 	Epoch: [33][100/309]	Batch time 2.5706 (2.7502)	Loss 0.41117 (0.38107)
2022-05-07 03:46:21,673 INFO 	Epoch: [33][110/309]	Batch time 2.7764 (2.7414)	Loss 0.41540 (0.38237)
2022-05-07 03:46:50,060 INFO 	Epoch: [33][120/309]	Batch time 2.7713 (2.7495)	Loss 0.37439 (0.38192)
2022-05-07 03:47:17,398 INFO 	Epoch: [33][130/309]	Batch time 2.6872 (2.7483)	Loss 0.37489 (0.38250)
2022-05-07 03:47:45,491 INFO 	Epoch: [33][140/309]	Batch time 2.5744 (2.7526)	Loss 0.42269 (0.38199)
2022-05-07 03:48:12,728 INFO 	Epoch: [33][150/309]	Batch time 2.8888 (2.7507)	Loss 0.39523 (0.38216)
2022-05-07 03:48:39,815 INFO 	Epoch: [33][160/309]	Batch time 2.9609 (2.7481)	Loss 0.39189 (0.38191)
2022-05-07 03:49:08,071 INFO 	Epoch: [33][170/309]	Batch time 2.5762 (2.7526)	Loss 0.40893 (0.38228)
2022-05-07 03:49:36,692 INFO 	Epoch: [33][180/309]	Batch time 2.4804 (2.7587)	Loss 0.42310 (0.38179)
2022-05-07 03:50:03,133 INFO 	Epoch: [33][190/309]	Batch time 2.7597 (2.7527)	Loss 0.32448 (0.38250)
2022-05-07 03:50:30,369 INFO 	Epoch: [33][200/309]	Batch time 2.7272 (2.7512)	Loss 0.39585 (0.38281)
2022-05-07 03:50:57,964 INFO 	Epoch: [33][210/309]	Batch time 3.1137 (2.7516)	Loss 0.32860 (0.38233)
2022-05-07 03:51:25,142 INFO 	Epoch: [33][220/309]	Batch time 2.7803 (2.7501)	Loss 0.38304 (0.38235)
2022-05-07 03:51:52,257 INFO 	Epoch: [33][230/309]	Batch time 2.4400 (2.7484)	Loss 0.44708 (0.38244)
2022-05-07 03:52:19,094 INFO 	Epoch: [33][240/309]	Batch time 2.7047 (2.7457)	Loss 0.41225 (0.38269)
2022-05-07 03:52:46,418 INFO 	Epoch: [33][250/309]	Batch time 2.6810 (2.7452)	Loss 0.39863 (0.38248)
2022-05-07 03:53:13,181 INFO 	Epoch: [33][260/309]	Batch time 2.5976 (2.7426)	Loss 0.39720 (0.38253)
2022-05-07 03:53:39,933 INFO 	Epoch: [33][270/309]	Batch time 2.5755 (2.7401)	Loss 0.36030 (0.38268)
2022-05-07 03:54:06,382 INFO 	Epoch: [33][280/309]	Batch time 2.5213 (2.7367)	Loss 0.41200 (0.38357)
2022-05-07 03:54:34,038 INFO 	Epoch: [33][290/309]	Batch time 2.6386 (2.7377)	Loss 0.39243 (0.38351)
2022-05-07 03:55:02,264 INFO 	Epoch: [33][300/309]	Batch time 2.6070 (2.7405)	Loss 0.41370 (0.38297)

Learning rate: 0.001
Step num: 10506

2022-05-07 03:55:27,493 INFO 	
Validation Loss 0.39529 (0.37084)

Warning! Reached max decoder steps
save mel done
2022-05-07 03:55:39,437 INFO 	Epoch: [34][0/309]	Batch time 4.7217 (4.7217)	Loss 0.37499 (0.37499)
2022-05-07 03:56:08,235 INFO 	Epoch: [34][10/309]	Batch time 2.8107 (3.0473)	Loss 0.39387 (0.38450)
2022-05-07 03:56:35,747 INFO 	Epoch: [34][20/309]	Batch time 2.9236 (2.9063)	Loss 0.36501 (0.38719)
2022-05-07 03:57:03,421 INFO 	Epoch: [34][30/309]	Batch time 2.5080 (2.8615)	Loss 0.36371 (0.38618)
2022-05-07 03:57:30,986 INFO 	Epoch: [34][40/309]	Batch time 2.5437 (2.8359)	Loss 0.37776 (0.38255)
2022-05-07 03:57:58,530 INFO 	Epoch: [34][50/309]	Batch time 2.7392 (2.8199)	Loss 0.38853 (0.38212)
2022-05-07 03:58:25,882 INFO 	Epoch: [34][60/309]	Batch time 2.5067 (2.8060)	Loss 0.41787 (0.38160)
2022-05-07 03:58:52,190 INFO 	Epoch: [34][70/309]	Batch time 2.5845 (2.7813)	Loss 0.37183 (0.38181)
2022-05-07 03:59:18,900 INFO 	Epoch: [34][80/309]	Batch time 2.4145 (2.7677)	Loss 0.41346 (0.38211)
2022-05-07 03:59:46,502 INFO 	Epoch: [34][90/309]	Batch time 2.6088 (2.7669)	Loss 0.42844 (0.38025)
2022-05-07 04:00:13,751 INFO 	Epoch: [34][100/309]	Batch time 2.6710 (2.7627)	Loss 0.39685 (0.37928)
2022-05-07 04:00:39,497 INFO 	Epoch: [34][110/309]	Batch time 2.4561 (2.7458)	Loss 0.40553 (0.38000)
2022-05-07 04:01:06,976 INFO 	Epoch: [34][120/309]	Batch time 2.7642 (2.7460)	Loss 0.36835 (0.37980)
2022-05-07 04:01:35,807 INFO 	Epoch: [34][130/309]	Batch time 3.3061 (2.7564)	Loss 0.30283 (0.37813)
2022-05-07 04:02:03,726 INFO 	Epoch: [34][140/309]	Batch time 2.7629 (2.7589)	Loss 0.37609 (0.37778)
2022-05-07 04:02:29,686 INFO 	Epoch: [34][150/309]	Batch time 2.5603 (2.7482)	Loss 0.38275 (0.37890)
2022-05-07 04:02:56,811 INFO 	Epoch: [34][160/309]	Batch time 2.7306 (2.7459)	Loss 0.41811 (0.37988)
2022-05-07 04:03:25,269 INFO 	Epoch: [34][170/309]	Batch time 3.0603 (2.7518)	Loss 0.36244 (0.37956)
2022-05-07 04:03:53,429 INFO 	Epoch: [34][180/309]	Batch time 2.4851 (2.7553)	Loss 0.38468 (0.37934)
2022-05-07 04:04:21,749 INFO 	Epoch: [34][190/309]	Batch time 3.1090 (2.7593)	Loss 0.33795 (0.37937)
2022-05-07 04:04:49,110 INFO 	Epoch: [34][200/309]	Batch time 2.4683 (2.7582)	Loss 0.45600 (0.38013)
2022-05-07 04:05:17,506 INFO 	Epoch: [34][210/309]	Batch time 2.7349 (2.7620)	Loss 0.42306 (0.38000)
2022-05-07 04:05:45,795 INFO 	Epoch: [34][220/309]	Batch time 2.9969 (2.7651)	Loss 0.35754 (0.38007)
2022-05-07 04:06:13,569 INFO 	Epoch: [34][230/309]	Batch time 2.4567 (2.7656)	Loss 0.41068 (0.37944)
2022-05-07 04:06:41,016 INFO 	Epoch: [34][240/309]	Batch time 2.6399 (2.7647)	Loss 0.36962 (0.37906)
2022-05-07 04:07:07,689 INFO 	Epoch: [34][250/309]	Batch time 2.6753 (2.7609)	Loss 0.38799 (0.37901)
2022-05-07 04:07:33,246 INFO 	Epoch: [34][260/309]	Batch time 2.4900 (2.7530)	Loss 0.40773 (0.37963)
2022-05-07 04:08:00,190 INFO 	Epoch: [34][270/309]	Batch time 2.4044 (2.7508)	Loss 0.41757 (0.37975)
2022-05-07 04:08:28,325 INFO 	Epoch: [34][280/309]	Batch time 2.7174 (2.7531)	Loss 0.35468 (0.37888)
2022-05-07 04:08:54,277 INFO 	Epoch: [34][290/309]	Batch time 2.5398 (2.7476)	Loss 0.38155 (0.37940)
2022-05-07 04:09:22,405 INFO 	Epoch: [34][300/309]	Batch time 2.5767 (2.7498)	Loss 0.40492 (0.37978)

Learning rate: 0.001
Step num: 10815

2022-05-07 04:09:47,067 INFO 	
Validation Loss 0.39323 (0.36979)

Warning! Reached max decoder steps
save mel done
2022-05-07 04:09:59,443 INFO 	Epoch: [35][0/309]	Batch time 4.6626 (4.6626)	Loss 0.40336 (0.40336)
2022-05-07 04:10:26,822 INFO 	Epoch: [35][10/309]	Batch time 2.6978 (2.9129)	Loss 0.38631 (0.38350)
2022-05-07 04:10:53,120 INFO 	Epoch: [35][20/309]	Batch time 2.5439 (2.7781)	Loss 0.44536 (0.38775)
2022-05-07 04:11:18,510 INFO 	Epoch: [35][30/309]	Batch time 2.5526 (2.7010)	Loss 0.39785 (0.39316)
2022-05-07 04:11:46,438 INFO 	Epoch: [35][40/309]	Batch time 2.4986 (2.7234)	Loss 0.38844 (0.38644)
2022-05-07 04:12:13,426 INFO 	Epoch: [35][50/309]	Batch time 2.7098 (2.7185)	Loss 0.37376 (0.38428)
2022-05-07 04:12:40,681 INFO 	Epoch: [35][60/309]	Batch time 2.6925 (2.7197)	Loss 0.36085 (0.38278)
2022-05-07 04:13:08,009 INFO 	Epoch: [35][70/309]	Batch time 2.7387 (2.7215)	Loss 0.32774 (0.38232)
2022-05-07 04:13:37,448 INFO 	Epoch: [35][80/309]	Batch time 2.8572 (2.7490)	Loss 0.36648 (0.37947)
2022-05-07 04:14:05,907 INFO 	Epoch: [35][90/309]	Batch time 3.1099 (2.7596)	Loss 0.31899 (0.37762)
2022-05-07 04:14:33,335 INFO 	Epoch: [35][100/309]	Batch time 2.6803 (2.7580)	Loss 0.41277 (0.37730)
2022-05-07 04:15:00,062 INFO 	Epoch: [35][110/309]	Batch time 2.5373 (2.7503)	Loss 0.39626 (0.37744)
2022-05-07 04:15:26,555 INFO 	Epoch: [35][120/309]	Batch time 2.8317 (2.7419)	Loss 0.42682 (0.37866)
2022-05-07 04:15:53,629 INFO 	Epoch: [35][130/309]	Batch time 2.6024 (2.7393)	Loss 0.37973 (0.37853)
2022-05-07 04:16:19,125 INFO 	Epoch: [35][140/309]	Batch time 2.8133 (2.7258)	Loss 0.40460 (0.37906)
2022-05-07 04:16:46,744 INFO 	Epoch: [35][150/309]	Batch time 2.5974 (2.7282)	Loss 0.42135 (0.37961)
2022-05-07 04:17:15,310 INFO 	Epoch: [35][160/309]	Batch time 3.0402 (2.7362)	Loss 0.35373 (0.37935)
2022-05-07 04:17:42,300 INFO 	Epoch: [35][170/309]	Batch time 2.5322 (2.7340)	Loss 0.40006 (0.37921)
2022-05-07 04:18:09,396 INFO 	Epoch: [35][180/309]	Batch time 2.5708 (2.7327)	Loss 0.39047 (0.37944)
2022-05-07 04:18:37,886 INFO 	Epoch: [35][190/309]	Batch time 2.6652 (2.7388)	Loss 0.40568 (0.37931)
2022-05-07 04:19:05,425 INFO 	Epoch: [35][200/309]	Batch time 2.8033 (2.7395)	Loss 0.33340 (0.37863)
2022-05-07 04:19:31,527 INFO 	Epoch: [35][210/309]	Batch time 2.2848 (2.7334)	Loss 0.42988 (0.37959)
2022-05-07 04:19:59,617 INFO 	Epoch: [35][220/309]	Batch time 3.1463 (2.7368)	Loss 0.30143 (0.37860)
2022-05-07 04:20:26,833 INFO 	Epoch: [35][230/309]	Batch time 2.5706 (2.7362)	Loss 0.41424 (0.37867)
2022-05-07 04:20:55,226 INFO 	Epoch: [35][240/309]	Batch time 3.0601 (2.7404)	Loss 0.32986 (0.37825)
2022-05-07 04:21:22,553 INFO 	Epoch: [35][250/309]	Batch time 2.9991 (2.7401)	Loss 0.36766 (0.37837)
2022-05-07 04:21:52,022 INFO 	Epoch: [35][260/309]	Batch time 3.0850 (2.7481)	Loss 0.38721 (0.37795)
2022-05-07 04:22:19,526 INFO 	Epoch: [35][270/309]	Batch time 2.6916 (2.7481)	Loss 0.36892 (0.37763)
2022-05-07 04:22:46,935 INFO 	Epoch: [35][280/309]	Batch time 2.6487 (2.7479)	Loss 0.42080 (0.37757)
2022-05-07 04:23:14,965 INFO 	Epoch: [35][290/309]	Batch time 2.9985 (2.7498)	Loss 0.35394 (0.37708)
2022-05-07 04:23:41,409 INFO 	Epoch: [35][300/309]	Batch time 2.6912 (2.7463)	Loss 0.37233 (0.37763)

Learning rate: 0.001
Step num: 11124

2022-05-07 04:24:05,557 INFO 	
Validation Loss 0.38788 (0.36376)

Warning! Reached max decoder steps
save mel done
2022-05-07 04:24:17,660 INFO 	Epoch: [36][0/309]	Batch time 4.7922 (4.7922)	Loss 0.33878 (0.33878)
2022-05-07 04:24:44,507 INFO 	Epoch: [36][10/309]	Batch time 2.2784 (2.8763)	Loss 0.44736 (0.36400)
2022-05-07 04:25:10,689 INFO 	Epoch: [36][20/309]	Batch time 2.4113 (2.7534)	Loss 0.39253 (0.37532)
2022-05-07 04:25:38,288 INFO 	Epoch: [36][30/309]	Batch time 2.6112 (2.7555)	Loss 0.37335 (0.37530)
2022-05-07 04:26:05,967 INFO 	Epoch: [36][40/309]	Batch time 2.6252 (2.7585)	Loss 0.35031 (0.37482)
2022-05-07 04:26:33,502 INFO 	Epoch: [36][50/309]	Batch time 2.6450 (2.7575)	Loss 0.37228 (0.37444)
2022-05-07 04:27:01,778 INFO 	Epoch: [36][60/309]	Batch time 2.8815 (2.7690)	Loss 0.35571 (0.37400)
2022-05-07 04:27:29,410 INFO 	Epoch: [36][70/309]	Batch time 2.6545 (2.7682)	Loss 0.41012 (0.37509)
2022-05-07 04:27:57,173 INFO 	Epoch: [36][80/309]	Batch time 2.7733 (2.7692)	Loss 0.41720 (0.37564)
2022-05-07 04:28:24,959 INFO 	Epoch: [36][90/309]	Batch time 2.7289 (2.7702)	Loss 0.36506 (0.37524)
2022-05-07 04:28:54,107 INFO 	Epoch: [36][100/309]	Batch time 3.0302 (2.7845)	Loss 0.34578 (0.37337)
2022-05-07 04:29:20,373 INFO 	Epoch: [36][110/309]	Batch time 2.9604 (2.7703)	Loss 0.33207 (0.37325)
2022-05-07 04:29:46,301 INFO 	Epoch: [36][120/309]	Batch time 2.6816 (2.7557)	Loss 0.35038 (0.37364)
2022-05-07 04:30:12,757 INFO 	Epoch: [36][130/309]	Batch time 2.4955 (2.7472)	Loss 0.41146 (0.37447)
2022-05-07 04:30:40,393 INFO 	Epoch: [36][140/309]	Batch time 2.4004 (2.7484)	Loss 0.40559 (0.37450)
2022-05-07 04:31:08,525 INFO 	Epoch: [36][150/309]	Batch time 2.5794 (2.7527)	Loss 0.39205 (0.37421)
2022-05-07 04:31:35,589 INFO 	Epoch: [36][160/309]	Batch time 3.0178 (2.7498)	Loss 0.35468 (0.37558)
2022-05-07 04:32:03,155 INFO 	Epoch: [36][170/309]	Batch time 2.6500 (2.7502)	Loss 0.40108 (0.37548)
2022-05-07 04:32:30,053 INFO 	Epoch: [36][180/309]	Batch time 2.6822 (2.7469)	Loss 0.35854 (0.37558)
2022-05-07 04:32:57,681 INFO 	Epoch: [36][190/309]	Batch time 2.7118 (2.7477)	Loss 0.34121 (0.37509)
2022-05-07 04:33:24,979 INFO 	Epoch: [36][200/309]	Batch time 2.7415 (2.7468)	Loss 0.41794 (0.37520)
2022-05-07 04:33:51,959 INFO 	Epoch: [36][210/309]	Batch time 2.4365 (2.7445)	Loss 0.42884 (0.37566)
2022-05-07 04:34:19,923 INFO 	Epoch: [36][220/309]	Batch time 2.7976 (2.7469)	Loss 0.36165 (0.37577)
2022-05-07 04:34:48,047 INFO 	Epoch: [36][230/309]	Batch time 2.4364 (2.7497)	Loss 0.40769 (0.37571)
2022-05-07 04:35:15,035 INFO 	Epoch: [36][240/309]	Batch time 2.6603 (2.7476)	Loss 0.35740 (0.37583)
2022-05-07 04:35:41,735 INFO 	Epoch: [36][250/309]	Batch time 2.4597 (2.7445)	Loss 0.39133 (0.37582)
2022-05-07 04:36:10,545 INFO 	Epoch: [36][260/309]	Batch time 3.4080 (2.7497)	Loss 0.28945 (0.37546)
2022-05-07 04:36:38,751 INFO 	Epoch: [36][270/309]	Batch time 2.5200 (2.7523)	Loss 0.41678 (0.37513)
2022-05-07 04:37:07,099 INFO 	Epoch: [36][280/309]	Batch time 3.2668 (2.7553)	Loss 0.33005 (0.37523)
2022-05-07 04:37:35,272 INFO 	Epoch: [36][290/309]	Batch time 3.0623 (2.7574)	Loss 0.36958 (0.37525)
2022-05-07 04:38:03,410 INFO 	Epoch: [36][300/309]	Batch time 2.6951 (2.7593)	Loss 0.38210 (0.37497)

Learning rate: 0.001
Step num: 11433

2022-05-07 04:38:28,610 INFO 	
Validation Loss 0.39228 (0.36642)


Epochs since last improvement: 1

Warning! Reached max decoder steps
save mel done
2022-05-07 04:38:38,289 INFO 	Epoch: [37][0/309]	Batch time 4.7236 (4.7236)	Loss 0.36297 (0.36297)
2022-05-07 04:39:06,344 INFO 	Epoch: [37][10/309]	Batch time 3.2067 (2.9799)	Loss 0.34101 (0.37914)
2022-05-07 04:39:33,622 INFO 	Epoch: [37][20/309]	Batch time 2.9079 (2.8598)	Loss 0.34618 (0.37058)
2022-05-07 04:40:00,839 INFO 	Epoch: [37][30/309]	Batch time 2.5546 (2.8153)	Loss 0.38194 (0.37305)
2022-05-07 04:40:28,761 INFO 	Epoch: [37][40/309]	Batch time 3.1000 (2.8096)	Loss 0.34642 (0.36906)
2022-05-07 04:40:56,214 INFO 	Epoch: [37][50/309]	Batch time 3.0916 (2.7970)	Loss 0.30231 (0.36857)
2022-05-07 04:41:22,606 INFO 	Epoch: [37][60/309]	Batch time 2.7875 (2.7712)	Loss 0.30985 (0.36783)
2022-05-07 04:41:48,451 INFO 	Epoch: [37][70/309]	Batch time 2.5527 (2.7449)	Loss 0.32784 (0.37023)
2022-05-07 04:42:15,073 INFO 	Epoch: [37][80/309]	Batch time 2.9220 (2.7347)	Loss 0.37266 (0.37143)
2022-05-07 04:42:42,004 INFO 	Epoch: [37][90/309]	Batch time 3.1777 (2.7301)	Loss 0.31472 (0.37174)
2022-05-07 04:43:09,342 INFO 	Epoch: [37][100/309]	Batch time 2.7842 (2.7305)	Loss 0.34344 (0.37130)
2022-05-07 04:43:37,661 INFO 	Epoch: [37][110/309]	Batch time 2.7803 (2.7396)	Loss 0.39225 (0.37181)
2022-05-07 04:44:05,857 INFO 	Epoch: [37][120/309]	Batch time 2.7692 (2.7462)	Loss 0.36575 (0.37188)
2022-05-07 04:44:32,924 INFO 	Epoch: [37][130/309]	Batch time 2.5671 (2.7432)	Loss 0.37786 (0.37250)
2022-05-07 04:45:00,554 INFO 	Epoch: [37][140/309]	Batch time 2.7489 (2.7446)	Loss 0.40127 (0.37232)
2022-05-07 04:45:27,277 INFO 	Epoch: [37][150/309]	Batch time 2.5235 (2.7398)	Loss 0.41529 (0.37303)
2022-05-07 04:45:54,846 INFO 	Epoch: [37][160/309]	Batch time 3.0216 (2.7409)	Loss 0.36405 (0.37310)
2022-05-07 04:46:21,916 INFO 	Epoch: [37][170/309]	Batch time 2.4502 (2.7389)	Loss 0.39462 (0.37320)
2022-05-07 04:46:49,134 INFO 	Epoch: [37][180/309]	Batch time 2.8117 (2.7379)	Loss 0.39659 (0.37365)
2022-05-07 04:47:16,845 INFO 	Epoch: [37][190/309]	Batch time 2.7462 (2.7397)	Loss 0.42108 (0.37487)
2022-05-07 04:47:46,430 INFO 	Epoch: [37][200/309]	Batch time 2.5658 (2.7506)	Loss 0.40336 (0.37391)
2022-05-07 04:48:14,563 INFO 	Epoch: [37][210/309]	Batch time 3.2193 (2.7535)	Loss 0.33253 (0.37336)
2022-05-07 04:48:43,362 INFO 	Epoch: [37][220/309]	Batch time 2.7955 (2.7593)	Loss 0.38636 (0.37309)
2022-05-07 04:49:11,136 INFO 	Epoch: [37][230/309]	Batch time 3.0273 (2.7600)	Loss 0.32746 (0.37249)
2022-05-07 04:49:38,050 INFO 	Epoch: [37][240/309]	Batch time 3.0595 (2.7572)	Loss 0.34755 (0.37237)
2022-05-07 04:50:03,956 INFO 	Epoch: [37][250/309]	Batch time 2.5983 (2.7506)	Loss 0.37706 (0.37256)
2022-05-07 04:50:31,378 INFO 	Epoch: [37][260/309]	Batch time 3.2039 (2.7502)	Loss 0.33100 (0.37290)
2022-05-07 04:51:00,849 INFO 	Epoch: [37][270/309]	Batch time 2.8797 (2.7575)	Loss 0.35155 (0.37244)
2022-05-07 04:51:30,433 INFO 	Epoch: [37][280/309]	Batch time 2.8499 (2.7646)	Loss 0.35421 (0.37178)
2022-05-07 04:51:58,713 INFO 	Epoch: [37][290/309]	Batch time 2.7520 (2.7668)	Loss 0.36864 (0.37213)
2022-05-07 04:52:26,310 INFO 	Epoch: [37][300/309]	Batch time 2.6929 (2.7666)	Loss 0.37264 (0.37211)

Learning rate: 0.001
Step num: 11742

2022-05-07 04:52:51,231 INFO 	
Validation Loss 0.38614 (0.36172)

Warning! Reached max decoder steps
save mel done
2022-05-07 04:53:02,757 INFO 	Epoch: [38][0/309]	Batch time 4.3454 (4.3454)	Loss 0.36844 (0.36844)
2022-05-07 04:53:28,747 INFO 	Epoch: [38][10/309]	Batch time 2.5404 (2.7578)	Loss 0.41274 (0.37732)
2022-05-07 04:53:55,806 INFO 	Epoch: [38][20/309]	Batch time 2.5150 (2.7331)	Loss 0.39581 (0.37596)
2022-05-07 04:54:23,493 INFO 	Epoch: [38][30/309]	Batch time 2.5781 (2.7446)	Loss 0.39140 (0.37134)
2022-05-07 04:54:51,273 INFO 	Epoch: [38][40/309]	Batch time 2.5145 (2.7527)	Loss 0.40602 (0.37324)
2022-05-07 04:55:18,961 INFO 	Epoch: [38][50/309]	Batch time 2.8131 (2.7559)	Loss 0.34570 (0.37147)
2022-05-07 04:55:46,671 INFO 	Epoch: [38][60/309]	Batch time 2.7746 (2.7584)	Loss 0.34340 (0.36999)
2022-05-07 04:56:15,735 INFO 	Epoch: [38][70/309]	Batch time 3.1490 (2.7792)	Loss 0.33187 (0.37049)
2022-05-07 04:56:44,377 INFO 	Epoch: [38][80/309]	Batch time 2.8584 (2.7897)	Loss 0.39165 (0.37079)
2022-05-07 04:57:12,927 INFO 	Epoch: [38][90/309]	Batch time 2.9216 (2.7969)	Loss 0.34826 (0.36991)
2022-05-07 04:57:41,665 INFO 	Epoch: [38][100/309]	Batch time 3.0315 (2.8045)	Loss 0.36043 (0.36987)
2022-05-07 04:58:09,686 INFO 	Epoch: [38][110/309]	Batch time 2.7251 (2.8043)	Loss 0.38819 (0.37020)
2022-05-07 04:58:38,395 INFO 	Epoch: [38][120/309]	Batch time 2.8690 (2.8098)	Loss 0.36754 (0.37046)
2022-05-07 04:59:08,147 INFO 	Epoch: [38][130/309]	Batch time 2.9861 (2.8224)	Loss 0.37593 (0.36901)
2022-05-07 04:59:35,452 INFO 	Epoch: [38][140/309]	Batch time 2.7687 (2.8159)	Loss 0.36340 (0.36984)
2022-05-07 05:00:02,523 INFO 	Epoch: [38][150/309]	Batch time 2.5135 (2.8087)	Loss 0.37725 (0.37047)
2022-05-07 05:00:29,707 INFO 	Epoch: [38][160/309]	Batch time 2.4864 (2.8031)	Loss 0.40164 (0.37136)
2022-05-07 05:00:56,313 INFO 	Epoch: [38][170/309]	Batch time 2.9325 (2.7947)	Loss 0.35353 (0.37140)
2022-05-07 05:01:25,640 INFO 	Epoch: [38][180/309]	Batch time 3.1685 (2.8024)	Loss 0.32939 (0.37026)
2022-05-07 05:01:54,888 INFO 	Epoch: [38][190/309]	Batch time 2.9702 (2.8088)	Loss 0.34251 (0.36950)
2022-05-07 05:02:23,261 INFO 	Epoch: [38][200/309]	Batch time 2.6537 (2.8102)	Loss 0.37839 (0.36930)
2022-05-07 05:02:51,237 INFO 	Epoch: [38][210/309]	Batch time 3.3581 (2.8096)	Loss 0.33101 (0.36940)
2022-05-07 05:03:19,763 INFO 	Epoch: [38][220/309]	Batch time 2.8405 (2.8115)	Loss 0.37717 (0.36937)
2022-05-07 05:03:48,857 INFO 	Epoch: [38][230/309]	Batch time 3.2148 (2.8158)	Loss 0.33551 (0.36927)
2022-05-07 05:04:15,654 INFO 	Epoch: [38][240/309]	Batch time 2.6098 (2.8101)	Loss 0.44323 (0.37025)
2022-05-07 05:04:41,727 INFO 	Epoch: [38][250/309]	Batch time 2.7286 (2.8021)	Loss 0.37629 (0.37090)
2022-05-07 05:05:08,019 INFO 	Epoch: [38][260/309]	Batch time 2.4532 (2.7954)	Loss 0.39704 (0.37155)
2022-05-07 05:05:34,864 INFO 	Epoch: [38][270/309]	Batch time 2.5062 (2.7913)	Loss 0.37960 (0.37171)
2022-05-07 05:06:02,514 INFO 	Epoch: [38][280/309]	Batch time 2.6867 (2.7904)	Loss 0.42399 (0.37154)
2022-05-07 05:06:30,938 INFO 	Epoch: [38][290/309]	Batch time 2.4758 (2.7922)	Loss 0.35589 (0.37120)
2022-05-07 05:06:58,233 INFO 	Epoch: [38][300/309]	Batch time 2.8635 (2.7901)	Loss 0.34124 (0.37128)

Learning rate: 0.001
Step num: 12051

2022-05-07 05:07:23,884 INFO 	
Validation Loss 0.38318 (0.35951)

Warning! Reached max decoder steps
save mel done
2022-05-07 05:07:35,520 INFO 	Epoch: [39][0/309]	Batch time 4.4286 (4.4286)	Loss 0.38577 (0.38577)
2022-05-07 05:08:02,797 INFO 	Epoch: [39][10/309]	Batch time 2.8875 (2.8824)	Loss 0.33728 (0.36059)
2022-05-07 05:08:30,197 INFO 	Epoch: [39][20/309]	Batch time 3.1568 (2.8146)	Loss 0.31450 (0.36577)
2022-05-07 05:08:57,834 INFO 	Epoch: [39][30/309]	Batch time 2.9273 (2.7982)	Loss 0.38398 (0.36792)
2022-05-07 05:09:24,747 INFO 	Epoch: [39][40/309]	Batch time 2.5790 (2.7721)	Loss 0.38149 (0.36759)
2022-05-07 05:09:52,209 INFO 	Epoch: [39][50/309]	Batch time 3.1523 (2.7670)	Loss 0.33712 (0.37002)
2022-05-07 05:10:19,017 INFO 	Epoch: [39][60/309]	Batch time 3.2504 (2.7529)	Loss 0.29582 (0.37079)
2022-05-07 05:10:45,246 INFO 	Epoch: [39][70/309]	Batch time 2.7475 (2.7346)	Loss 0.35303 (0.37261)
2022-05-07 05:11:11,824 INFO 	Epoch: [39][80/309]	Batch time 2.5717 (2.7251)	Loss 0.39458 (0.37257)
2022-05-07 05:11:39,587 INFO 	Epoch: [39][90/309]	Batch time 2.6536 (2.7307)	Loss 0.39044 (0.37173)
2022-05-07 05:12:06,773 INFO 	Epoch: [39][100/309]	Batch time 2.7813 (2.7295)	Loss 0.34291 (0.37208)
2022-05-07 05:12:34,360 INFO 	Epoch: [39][110/309]	Batch time 3.0511 (2.7322)	Loss 0.37746 (0.37223)
2022-05-07 05:13:02,141 INFO 	Epoch: [39][120/309]	Batch time 3.5018 (2.7360)	Loss 0.31631 (0.37266)
2022-05-07 05:13:30,675 INFO 	Epoch: [39][130/309]	Batch time 2.9028 (2.7449)	Loss 0.36776 (0.37332)
2022-05-07 05:13:57,345 INFO 	Epoch: [39][140/309]	Batch time 2.7621 (2.7394)	Loss 0.37347 (0.37389)
2022-05-07 05:14:24,235 INFO 	Epoch: [39][150/309]	Batch time 2.5125 (2.7361)	Loss 0.37076 (0.37317)
2022-05-07 05:14:51,777 INFO 	Epoch: [39][160/309]	Batch time 2.7743 (2.7372)	Loss 0.33562 (0.37284)
2022-05-07 05:15:18,930 INFO 	Epoch: [39][170/309]	Batch time 2.5179 (2.7359)	Loss 0.39516 (0.37259)
2022-05-07 05:15:46,315 INFO 	Epoch: [39][180/309]	Batch time 2.8678 (2.7360)	Loss 0.36160 (0.37214)
2022-05-07 05:16:14,053 INFO 	Epoch: [39][190/309]	Batch time 2.8978 (2.7380)	Loss 0.34079 (0.37120)
2022-05-07 05:16:42,436 INFO 	Epoch: [39][200/309]	Batch time 2.7173 (2.7430)	Loss 0.37392 (0.37077)
2022-05-07 05:17:10,869 INFO 	Epoch: [39][210/309]	Batch time 3.2256 (2.7478)	Loss 0.31534 (0.37071)
2022-05-07 05:17:39,014 INFO 	Epoch: [39][220/309]	Batch time 2.4712 (2.7508)	Loss 0.37634 (0.36972)
2022-05-07 05:18:07,689 INFO 	Epoch: [39][230/309]	Batch time 3.3393 (2.7558)	Loss 0.33135 (0.36994)
2022-05-07 05:18:35,902 INFO 	Epoch: [39][240/309]	Batch time 2.9498 (2.7586)	Loss 0.32925 (0.36973)
2022-05-07 05:19:05,219 INFO 	Epoch: [39][250/309]	Batch time 2.6643 (2.7655)	Loss 0.40059 (0.36942)
2022-05-07 05:19:33,324 INFO 	Epoch: [39][260/309]	Batch time 2.6615 (2.7672)	Loss 0.39781 (0.36995)
2022-05-07 05:20:00,834 INFO 	Epoch: [39][270/309]	Batch time 2.9138 (2.7666)	Loss 0.39094 (0.37088)
2022-05-07 05:20:29,245 INFO 	Epoch: [39][280/309]	Batch time 2.9813 (2.7692)	Loss 0.33245 (0.37077)
2022-05-07 05:20:55,845 INFO 	Epoch: [39][290/309]	Batch time 2.5210 (2.7655)	Loss 0.43207 (0.37111)
2022-05-07 05:21:23,038 INFO 	Epoch: [39][300/309]	Batch time 2.7418 (2.7639)	Loss 0.34467 (0.37115)

Learning rate: 0.001
Step num: 12360

2022-05-07 05:21:48,851 INFO 	
Validation Loss 0.38514 (0.36054)


Epochs since last improvement: 1

Warning! Reached max decoder steps
save mel done
2022-05-07 05:21:58,284 INFO 	Epoch: [40][0/309]	Batch time 4.6339 (4.6339)	Loss 0.35698 (0.35698)
2022-05-07 05:22:25,951 INFO 	Epoch: [40][10/309]	Batch time 2.7246 (2.9365)	Loss 0.37857 (0.37759)
2022-05-07 05:22:53,460 INFO 	Epoch: [40][20/309]	Batch time 2.4951 (2.8481)	Loss 0.37137 (0.37776)
2022-05-07 05:23:21,153 INFO 	Epoch: [40][30/309]	Batch time 2.8222 (2.8227)	Loss 0.38232 (0.37927)
2022-05-07 05:23:49,842 INFO 	Epoch: [40][40/309]	Batch time 2.7451 (2.8339)	Loss 0.35285 (0.37468)
2022-05-07 05:24:18,922 INFO 	Epoch: [40][50/309]	Batch time 3.0977 (2.8485)	Loss 0.32986 (0.37333)
2022-05-07 05:24:46,609 INFO 	Epoch: [40][60/309]	Batch time 2.5617 (2.8354)	Loss 0.37555 (0.37364)
2022-05-07 05:25:14,293 INFO 	Epoch: [40][70/309]	Batch time 3.1990 (2.8260)	Loss 0.32960 (0.37271)
2022-05-07 05:25:41,053 INFO 	Epoch: [40][80/309]	Batch time 2.6943 (2.8074)	Loss 0.40030 (0.37324)
2022-05-07 05:26:09,925 INFO 	Epoch: [40][90/309]	Batch time 2.9792 (2.8162)	Loss 0.38465 (0.37124)
2022-05-07 05:26:37,812 INFO 	Epoch: [40][100/309]	Batch time 2.8878 (2.8135)	Loss 0.36831 (0.37152)
2022-05-07 05:27:06,482 INFO 	Epoch: [40][110/309]	Batch time 2.8851 (2.8183)	Loss 0.37701 (0.37203)
2022-05-07 05:27:35,112 INFO 	Epoch: [40][120/309]	Batch time 2.8065 (2.8220)	Loss 0.34940 (0.37157)
2022-05-07 05:28:03,435 INFO 	Epoch: [40][130/309]	Batch time 2.9291 (2.8228)	Loss 0.31608 (0.37074)
2022-05-07 05:28:32,272 INFO 	Epoch: [40][140/309]	Batch time 2.6691 (2.8271)	Loss 0.37296 (0.37022)
2022-05-07 05:28:59,962 INFO 	Epoch: [40][150/309]	Batch time 2.4473 (2.8233)	Loss 0.38503 (0.36987)
2022-05-07 05:29:28,246 INFO 	Epoch: [40][160/309]	Batch time 2.8400 (2.8236)	Loss 0.34170 (0.36925)
2022-05-07 05:29:56,784 INFO 	Epoch: [40][170/309]	Batch time 3.1171 (2.8253)	Loss 0.37113 (0.36927)
2022-05-07 05:30:23,630 INFO 	Epoch: [40][180/309]	Batch time 2.6005 (2.8176)	Loss 0.38956 (0.36904)
2022-05-07 05:30:53,124 INFO 	Epoch: [40][190/309]	Batch time 2.7473 (2.8245)	Loss 0.36116 (0.36828)
2022-05-07 05:31:22,509 INFO 	Epoch: [40][200/309]	Batch time 2.9754 (2.8301)	Loss 0.37503 (0.36774)
2022-05-07 05:31:50,760 INFO 	Epoch: [40][210/309]	Batch time 2.9904 (2.8299)	Loss 0.32575 (0.36750)
2022-05-07 05:32:18,645 INFO 	Epoch: [40][220/309]	Batch time 2.8322 (2.8280)	Loss 0.37608 (0.36834)
2022-05-07 05:32:44,528 INFO 	Epoch: [40][230/309]	Batch time 2.7867 (2.8177)	Loss 0.38681 (0.36943)
2022-05-07 05:33:12,879 INFO 	Epoch: [40][240/309]	Batch time 2.8362 (2.8184)	Loss 0.40776 (0.36898)
2022-05-07 05:33:40,450 INFO 	Epoch: [40][250/309]	Batch time 2.7802 (2.8159)	Loss 0.37410 (0.36894)
2022-05-07 05:34:08,271 INFO 	Epoch: [40][260/309]	Batch time 3.2094 (2.8146)	Loss 0.34081 (0.36875)
2022-05-07 05:34:36,200 INFO 	Epoch: [40][270/309]	Batch time 2.5441 (2.8138)	Loss 0.40569 (0.36912)
2022-05-07 05:35:03,302 INFO 	Epoch: [40][280/309]	Batch time 2.7489 (2.8101)	Loss 0.38975 (0.36891)
2022-05-07 05:35:32,155 INFO 	Epoch: [40][290/309]	Batch time 3.2156 (2.8127)	Loss 0.32937 (0.36879)
2022-05-07 05:35:59,704 INFO 	Epoch: [40][300/309]	Batch time 2.7009 (2.8108)	Loss 0.35949 (0.36840)

Learning rate: 0.0009832420039192554
Step num: 12669

2022-05-07 05:36:25,483 INFO 	
Validation Loss 0.37793 (0.35368)

Warning! Reached max decoder steps
save mel done
2022-05-07 05:36:36,981 INFO 	Epoch: [41][0/309]	Batch time 4.2703 (4.2703)	Loss 0.37327 (0.37327)
2022-05-07 05:37:03,748 INFO 	Epoch: [41][10/309]	Batch time 2.8036 (2.8216)	Loss 0.33121 (0.37207)
2022-05-07 05:37:31,104 INFO 	Epoch: [41][20/309]	Batch time 2.5819 (2.7806)	Loss 0.35364 (0.36594)
2022-05-07 05:37:59,356 INFO 	Epoch: [41][30/309]	Batch time 2.8666 (2.7950)	Loss 0.34188 (0.36681)
2022-05-07 05:38:26,162 INFO 	Epoch: [41][40/309]	Batch time 2.9696 (2.7671)	Loss 0.33620 (0.37042)
2022-05-07 05:38:54,906 INFO 	Epoch: [41][50/309]	Batch time 2.8295 (2.7881)	Loss 0.38720 (0.36938)
2022-05-07 05:39:22,993 INFO 	Epoch: [41][60/309]	Batch time 3.1076 (2.7915)	Loss 0.36722 (0.37085)
2022-05-07 05:39:52,130 INFO 	Epoch: [41][70/309]	Batch time 2.6238 (2.8087)	Loss 0.39561 (0.36979)
2022-05-07 05:40:21,489 INFO 	Epoch: [41][80/309]	Batch time 2.8528 (2.8244)	Loss 0.35618 (0.36836)
2022-05-07 05:40:48,082 INFO 	Epoch: [41][90/309]	Batch time 2.6148 (2.8063)	Loss 0.34558 (0.36842)
2022-05-07 05:41:15,666 INFO 	Epoch: [41][100/309]	Batch time 2.5127 (2.8015)	Loss 0.37613 (0.36758)
2022-05-07 05:41:43,688 INFO 	Epoch: [41][110/309]	Batch time 2.6626 (2.8016)	Loss 0.38867 (0.36650)
2022-05-07 05:42:11,982 INFO 	Epoch: [41][120/309]	Batch time 2.5767 (2.8039)	Loss 0.37288 (0.36538)
2022-05-07 05:42:38,996 INFO 	Epoch: [41][130/309]	Batch time 2.3278 (2.7961)	Loss 0.39233 (0.36564)
2022-05-07 05:43:06,473 INFO 	Epoch: [41][140/309]	Batch time 2.5034 (2.7926)	Loss 0.38318 (0.36551)
2022-05-07 05:43:34,230 INFO 	Epoch: [41][150/309]	Batch time 2.6448 (2.7915)	Loss 0.34235 (0.36537)
2022-05-07 05:44:02,033 INFO 	Epoch: [41][160/309]	Batch time 2.8006 (2.7908)	Loss 0.35582 (0.36528)
2022-05-07 05:44:29,843 INFO 	Epoch: [41][170/309]	Batch time 2.5913 (2.7902)	Loss 0.34697 (0.36486)
2022-05-07 05:44:58,253 INFO 	Epoch: [41][180/309]	Batch time 2.9965 (2.7930)	Loss 0.33641 (0.36491)
2022-05-07 05:45:26,101 INFO 	Epoch: [41][190/309]	Batch time 2.6180 (2.7926)	Loss 0.32286 (0.36483)
2022-05-07 05:45:54,228 INFO 	Epoch: [41][200/309]	Batch time 2.7381 (2.7936)	Loss 0.37598 (0.36485)
2022-05-07 05:46:20,778 INFO 	Epoch: [41][210/309]	Batch time 2.6687 (2.7870)	Loss 0.35282 (0.36478)
2022-05-07 05:46:47,731 INFO 	Epoch: [41][220/309]	Batch time 2.6318 (2.7829)	Loss 0.36969 (0.36528)
2022-05-07 05:47:13,938 INFO 	Epoch: [41][230/309]	Batch time 2.5447 (2.7759)	Loss 0.38924 (0.36554)
2022-05-07 05:47:41,155 INFO 	Epoch: [41][240/309]	Batch time 2.6269 (2.7736)	Loss 0.36474 (0.36554)
2022-05-07 05:48:09,379 INFO 	Epoch: [41][250/309]	Batch time 2.7042 (2.7756)	Loss 0.37949 (0.36526)
2022-05-07 05:48:38,013 INFO 	Epoch: [41][260/309]	Batch time 2.7812 (2.7789)	Loss 0.37539 (0.36510)
2022-05-07 05:49:05,287 INFO 	Epoch: [41][270/309]	Batch time 2.7215 (2.7770)	Loss 0.36787 (0.36503)
2022-05-07 05:49:33,217 INFO 	Epoch: [41][280/309]	Batch time 2.9055 (2.7776)	Loss 0.33677 (0.36511)
2022-05-07 05:50:02,069 INFO 	Epoch: [41][290/309]	Batch time 2.7015 (2.7813)	Loss 0.37620 (0.36546)
2022-05-07 05:50:31,323 INFO 	Epoch: [41][300/309]	Batch time 3.0364 (2.7861)	Loss 0.35078 (0.36546)

Learning rate: 0.0009533244328988669
Step num: 12978

2022-05-07 05:50:57,044 INFO 	
Validation Loss 0.38148 (0.35866)


Epochs since last improvement: 1

Warning! Reached max decoder steps
save mel done
2022-05-07 05:51:06,419 INFO 	Epoch: [42][0/309]	Batch time 4.5967 (4.5967)	Loss 0.41300 (0.41300)
2022-05-07 05:51:34,297 INFO 	Epoch: [42][10/309]	Batch time 2.7354 (2.9522)	Loss 0.36258 (0.36459)
2022-05-07 05:52:02,436 INFO 	Epoch: [42][20/309]	Batch time 2.6806 (2.8864)	Loss 0.39554 (0.37015)
2022-05-07 05:52:30,985 INFO 	Epoch: [42][30/309]	Batch time 2.6667 (2.8762)	Loss 0.38720 (0.36903)
2022-05-07 05:52:58,813 INFO 	Epoch: [42][40/309]	Batch time 2.6928 (2.8534)	Loss 0.34709 (0.36616)
2022-05-07 05:53:26,385 INFO 	Epoch: [42][50/309]	Batch time 2.5814 (2.8346)	Loss 0.35307 (0.36411)
2022-05-07 05:53:53,845 INFO 	Epoch: [42][60/309]	Batch time 2.5638 (2.8200)	Loss 0.39653 (0.36202)
2022-05-07 05:54:21,512 INFO 	Epoch: [42][70/309]	Batch time 2.7947 (2.8125)	Loss 0.37779 (0.36107)
2022-05-07 05:54:48,925 INFO 	Epoch: [42][80/309]	Batch time 2.3841 (2.8037)	Loss 0.35909 (0.36025)
2022-05-07 05:55:17,601 INFO 	Epoch: [42][90/309]	Batch time 2.6259 (2.8108)	Loss 0.35828 (0.35973)
2022-05-07 05:55:44,881 INFO 	Epoch: [42][100/309]	Batch time 2.7550 (2.8026)	Loss 0.35939 (0.36123)
2022-05-07 05:56:13,599 INFO 	Epoch: [42][110/309]	Batch time 2.9406 (2.8088)	Loss 0.36709 (0.36132)
2022-05-07 05:56:40,900 INFO 	Epoch: [42][120/309]	Batch time 2.5247 (2.8023)	Loss 0.41206 (0.36247)
2022-05-07 05:57:09,067 INFO 	Epoch: [42][130/309]	Batch time 2.7837 (2.8034)	Loss 0.34899 (0.36300)
2022-05-07 05:57:37,647 INFO 	Epoch: [42][140/309]	Batch time 2.8846 (2.8073)	Loss 0.37670 (0.36386)
2022-05-07 05:58:06,341 INFO 	Epoch: [42][150/309]	Batch time 2.8919 (2.8114)	Loss 0.33565 (0.36403)
2022-05-07 05:58:34,045 INFO 	Epoch: [42][160/309]	Batch time 2.7170 (2.8088)	Loss 0.35755 (0.36485)
2022-05-07 05:59:02,531 INFO 	Epoch: [42][170/309]	Batch time 2.9314 (2.8112)	Loss 0.36369 (0.36476)
2022-05-07 05:59:31,482 INFO 	Epoch: [42][180/309]	Batch time 3.1224 (2.8158)	Loss 0.36196 (0.36440)
2022-05-07 05:59:59,142 INFO 	Epoch: [42][190/309]	Batch time 2.7082 (2.8132)	Loss 0.40163 (0.36638)
2022-05-07 06:00:27,988 INFO 	Epoch: [42][200/309]	Batch time 2.9345 (2.8167)	Loss 0.36189 (0.36658)
2022-05-07 06:00:56,424 INFO 	Epoch: [42][210/309]	Batch time 2.9140 (2.8180)	Loss 0.38503 (0.36685)
2022-05-07 06:01:24,924 INFO 	Epoch: [42][220/309]	Batch time 2.4425 (2.8195)	Loss 0.40276 (0.36716)
2022-05-07 06:01:53,234 INFO 	Epoch: [42][230/309]	Batch time 2.9098 (2.8200)	Loss 0.36389 (0.36678)
2022-05-07 06:02:20,779 INFO 	Epoch: [42][240/309]	Batch time 2.6863 (2.8172)	Loss 0.33961 (0.36604)
2022-05-07 06:02:48,665 INFO 	Epoch: [42][250/309]	Batch time 3.0369 (2.8161)	Loss 0.31481 (0.36604)
2022-05-07 06:03:16,160 INFO 	Epoch: [42][260/309]	Batch time 3.0202 (2.8136)	Loss 0.36678 (0.36602)
2022-05-07 06:03:43,627 INFO 	Epoch: [42][270/309]	Batch time 2.7747 (2.8111)	Loss 0.33312 (0.36549)
2022-05-07 06:04:10,952 INFO 	Epoch: [42][280/309]	Batch time 2.9549 (2.8083)	Loss 0.34203 (0.36566)
2022-05-07 06:04:37,649 INFO 	Epoch: [42][290/309]	Batch time 2.8562 (2.8035)	Loss 0.35801 (0.36625)
2022-05-07 06:05:06,900 INFO 	Epoch: [42][300/309]	Batch time 2.9055 (2.8076)	Loss 0.36940 (0.36592)

Learning rate: 0.0009243171780083754
Step num: 13287

2022-05-07 06:05:32,249 INFO 	
Validation Loss 0.37500 (0.35219)

Warning! Reached max decoder steps
save mel done
2022-05-07 06:05:44,033 INFO 	Epoch: [43][0/309]	Batch time 4.6082 (4.6082)	Loss 0.35054 (0.35054)
2022-05-07 06:06:12,363 INFO 	Epoch: [43][10/309]	Batch time 3.1417 (2.9944)	Loss 0.32697 (0.35842)
2022-05-07 06:06:39,546 INFO 	Epoch: [43][20/309]	Batch time 2.8953 (2.8629)	Loss 0.29630 (0.35927)
2022-05-07 06:07:07,761 INFO 	Epoch: [43][30/309]	Batch time 2.7511 (2.8496)	Loss 0.34405 (0.35739)
2022-05-07 06:07:35,761 INFO 	Epoch: [43][40/309]	Batch time 2.4568 (2.8375)	Loss 0.38015 (0.35549)
2022-05-07 06:08:02,262 INFO 	Epoch: [43][50/309]	Batch time 2.9064 (2.8007)	Loss 0.32250 (0.35753)
2022-05-07 06:08:29,781 INFO 	Epoch: [43][60/309]	Batch time 2.7146 (2.7927)	Loss 0.37774 (0.35776)
2022-05-07 06:08:58,103 INFO 	Epoch: [43][70/309]	Batch time 2.8209 (2.7983)	Loss 0.33674 (0.35582)
2022-05-07 06:09:25,054 INFO 	Epoch: [43][80/309]	Batch time 2.8842 (2.7855)	Loss 0.34548 (0.35714)
2022-05-07 06:09:52,416 INFO 	Epoch: [43][90/309]	Batch time 3.1401 (2.7801)	Loss 0.31368 (0.35806)
2022-05-07 06:10:21,406 INFO 	Epoch: [43][100/309]	Batch time 2.9383 (2.7919)	Loss 0.36622 (0.35738)
2022-05-07 06:10:49,709 INFO 	Epoch: [43][110/309]	Batch time 2.9197 (2.7953)	Loss 0.36503 (0.35819)
2022-05-07 06:11:18,120 INFO 	Epoch: [43][120/309]	Batch time 3.3321 (2.7991)	Loss 0.27250 (0.35743)
2022-05-07 06:11:44,898 INFO 	Epoch: [43][130/309]	Batch time 2.6998 (2.7899)	Loss 0.38310 (0.35865)
2022-05-07 06:12:11,657 INFO 	Epoch: [43][140/309]	Batch time 2.7438 (2.7818)	Loss 0.40385 (0.35862)
2022-05-07 06:12:38,755 INFO 	Epoch: [43][150/309]	Batch time 2.9138 (2.7770)	Loss 0.33220 (0.35804)
2022-05-07 06:13:05,828 INFO 	Epoch: [43][160/309]	Batch time 2.7515 (2.7727)	Loss 0.34706 (0.35838)
2022-05-07 06:13:33,308 INFO 	Epoch: [43][170/309]	Batch time 2.8215 (2.7712)	Loss 0.36272 (0.35844)
2022-05-07 06:13:59,615 INFO 	Epoch: [43][180/309]	Batch time 2.8712 (2.7635)	Loss 0.35342 (0.35962)
2022-05-07 06:14:25,690 INFO 	Epoch: [43][190/309]	Batch time 2.6196 (2.7553)	Loss 0.37669 (0.36005)
2022-05-07 06:14:52,968 INFO 	Epoch: [43][200/309]	Batch time 2.5181 (2.7539)	Loss 0.39190 (0.36040)
2022-05-07 06:15:20,823 INFO 	Epoch: [43][210/309]	Batch time 2.9519 (2.7554)	Loss 0.37385 (0.36083)
2022-05-07 06:15:47,918 INFO 	Epoch: [43][220/309]	Batch time 2.6403 (2.7534)	Loss 0.36472 (0.36137)
2022-05-07 06:16:16,055 INFO 	Epoch: [43][230/309]	Batch time 3.0217 (2.7560)	Loss 0.34305 (0.36123)
2022-05-07 06:16:43,748 INFO 	Epoch: [43][240/309]	Batch time 2.5406 (2.7565)	Loss 0.36668 (0.36117)
2022-05-07 06:17:11,753 INFO 	Epoch: [43][250/309]	Batch time 3.0070 (2.7583)	Loss 0.38103 (0.36130)
2022-05-07 06:17:38,850 INFO 	Epoch: [43][260/309]	Batch time 2.4899 (2.7564)	Loss 0.36365 (0.36159)
2022-05-07 06:18:06,000 INFO 	Epoch: [43][270/309]	Batch time 2.6737 (2.7549)	Loss 0.40718 (0.36198)
2022-05-07 06:18:34,754 INFO 	Epoch: [43][280/309]	Batch time 2.9834 (2.7592)	Loss 0.38730 (0.36146)
2022-05-07 06:19:03,275 INFO 	Epoch: [43][290/309]	Batch time 2.3787 (2.7624)	Loss 0.37334 (0.36098)
2022-05-07 06:19:29,731 INFO 	Epoch: [43][300/309]	Batch time 2.5973 (2.7585)	Loss 0.37336 (0.36123)

Learning rate: 0.0008961925406269343
Step num: 13596

2022-05-07 06:19:54,959 INFO 	
Validation Loss 0.37232 (0.34865)

Warning! Reached max decoder steps
save mel done
2022-05-07 06:20:06,769 INFO 	Epoch: [44][0/309]	Batch time 4.6266 (4.6266)	Loss 0.32182 (0.32182)
2022-05-07 06:20:35,914 INFO 	Epoch: [44][10/309]	Batch time 2.9330 (3.0701)	Loss 0.32660 (0.34580)
2022-05-07 06:21:02,804 INFO 	Epoch: [44][20/309]	Batch time 2.5632 (2.8887)	Loss 0.34188 (0.35513)
2022-05-07 06:21:30,527 INFO 	Epoch: [44][30/309]	Batch time 2.7522 (2.8511)	Loss 0.36728 (0.35530)
2022-05-07 06:22:00,025 INFO 	Epoch: [44][40/309]	Batch time 2.8535 (2.8752)	Loss 0.31768 (0.35097)
2022-05-07 06:22:28,431 INFO 	Epoch: [44][50/309]	Batch time 2.8797 (2.8684)	Loss 0.35335 (0.35066)
2022-05-07 06:22:54,866 INFO 	Epoch: [44][60/309]	Batch time 2.5791 (2.8315)	Loss 0.34607 (0.35310)
2022-05-07 06:23:21,974 INFO 	Epoch: [44][70/309]	Batch time 3.0760 (2.8145)	Loss 0.32456 (0.35504)
2022-05-07 06:23:49,466 INFO 	Epoch: [44][80/309]	Batch time 2.6706 (2.8065)	Loss 0.38554 (0.35630)
2022-05-07 06:24:17,673 INFO 	Epoch: [44][90/309]	Batch time 2.2680 (2.8080)	Loss 0.42609 (0.35534)
2022-05-07 06:24:44,823 INFO 	Epoch: [44][100/309]	Batch time 2.5688 (2.7988)	Loss 0.37630 (0.35486)
2022-05-07 06:25:13,107 INFO 	Epoch: [44][110/309]	Batch time 2.8285 (2.8015)	Loss 0.39271 (0.35661)
2022-05-07 06:25:41,710 INFO 	Epoch: [44][120/309]	Batch time 2.8475 (2.8063)	Loss 0.37042 (0.35740)
2022-05-07 06:26:09,298 INFO 	Epoch: [44][130/309]	Batch time 2.8376 (2.8027)	Loss 0.35495 (0.35868)
2022-05-07 06:26:38,215 INFO 	Epoch: [44][140/309]	Batch time 2.8527 (2.8090)	Loss 0.37609 (0.35874)
2022-05-07 06:27:05,960 INFO 	Epoch: [44][150/309]	Batch time 2.8823 (2.8067)	Loss 0.34215 (0.35930)
2022-05-07 06:27:34,038 INFO 	Epoch: [44][160/309]	Batch time 3.1011 (2.8068)	Loss 0.31107 (0.35966)
2022-05-07 06:28:01,325 INFO 	Epoch: [44][170/309]	Batch time 2.7249 (2.8022)	Loss 0.39329 (0.36033)
2022-05-07 06:28:28,992 INFO 	Epoch: [44][180/309]	Batch time 2.7091 (2.8003)	Loss 0.33142 (0.36000)
2022-05-07 06:28:56,848 INFO 	Epoch: [44][190/309]	Batch time 2.9994 (2.7995)	Loss 0.33594 (0.36037)
2022-05-07 06:29:24,951 INFO 	Epoch: [44][200/309]	Batch time 2.6789 (2.8000)	Loss 0.35091 (0.35989)
2022-05-07 06:29:52,928 INFO 	Epoch: [44][210/309]	Batch time 2.7569 (2.7999)	Loss 0.37269 (0.36018)
2022-05-07 06:30:21,068 INFO 	Epoch: [44][220/309]	Batch time 3.1300 (2.8006)	Loss 0.32769 (0.35997)
2022-05-07 06:30:49,840 INFO 	Epoch: [44][230/309]	Batch time 2.4275 (2.8039)	Loss 0.39580 (0.35925)
2022-05-07 06:31:17,026 INFO 	Epoch: [44][240/309]	Batch time 2.8392 (2.8003)	Loss 0.35370 (0.35916)
2022-05-07 06:31:44,555 INFO 	Epoch: [44][250/309]	Batch time 2.8477 (2.7985)	Loss 0.33511 (0.35894)
2022-05-07 06:32:10,434 INFO 	Epoch: [44][260/309]	Batch time 2.5767 (2.7904)	Loss 0.38124 (0.35967)
2022-05-07 06:32:38,507 INFO 	Epoch: [44][270/309]	Batch time 2.9532 (2.7910)	Loss 0.35800 (0.35957)
2022-05-07 06:33:05,902 INFO 	Epoch: [44][280/309]	Batch time 2.9048 (2.7892)	Loss 0.31209 (0.35908)
2022-05-07 06:33:34,133 INFO 	Epoch: [44][290/309]	Batch time 2.6294 (2.7903)	Loss 0.37611 (0.35924)
2022-05-07 06:34:01,399 INFO 	Epoch: [44][300/309]	Batch time 2.5852 (2.7882)	Loss 0.36970 (0.35989)

Learning rate: 0.0008689236649327766
Step num: 13905

2022-05-07 06:34:26,307 INFO 	
Validation Loss 0.37068 (0.34755)

save mel done
2022-05-07 06:34:38,257 INFO 	Epoch: [45][0/309]	Batch time 4.7064 (4.7064)	Loss 0.34697 (0.34697)
2022-05-07 06:35:03,488 INFO 	Epoch: [45][10/309]	Batch time 2.7208 (2.7216)	Loss 0.37625 (0.36763)
2022-05-07 06:35:29,759 INFO 	Epoch: [45][20/309]	Batch time 2.8373 (2.6766)	Loss 0.33137 (0.36434)
2022-05-07 06:35:57,042 INFO 	Epoch: [45][30/309]	Batch time 2.5459 (2.6933)	Loss 0.33307 (0.35850)
2022-05-07 06:36:24,579 INFO 	Epoch: [45][40/309]	Batch time 2.7921 (2.7080)	Loss 0.36845 (0.35848)
2022-05-07 06:36:52,298 INFO 	Epoch: [45][50/309]	Batch time 2.9992 (2.7205)	Loss 0.36841 (0.35805)
2022-05-07 06:37:18,663 INFO 	Epoch: [45][60/309]	Batch time 2.5827 (2.7068)	Loss 0.37568 (0.36050)
2022-05-07 06:37:45,973 INFO 	Epoch: [45][70/309]	Batch time 2.9831 (2.7102)	Loss 0.39949 (0.36264)
2022-05-07 06:38:13,425 INFO 	Epoch: [45][80/309]	Batch time 2.7663 (2.7145)	Loss 0.35064 (0.36359)
2022-05-07 06:38:41,611 INFO 	Epoch: [45][90/309]	Batch time 2.6858 (2.7259)	Loss 0.36013 (0.36182)
2022-05-07 06:39:09,798 INFO 	Epoch: [45][100/309]	Batch time 2.5571 (2.7351)	Loss 0.41885 (0.36156)
2022-05-07 06:39:37,141 INFO 	Epoch: [45][110/309]	Batch time 2.9911 (2.7350)	Loss 0.35739 (0.36198)
2022-05-07 06:40:05,121 INFO 	Epoch: [45][120/309]	Batch time 2.7434 (2.7402)	Loss 0.36943 (0.36197)
2022-05-07 06:40:33,154 INFO 	Epoch: [45][130/309]	Batch time 2.8385 (2.7451)	Loss 0.36019 (0.36137)
2022-05-07 06:41:01,257 INFO 	Epoch: [45][140/309]	Batch time 3.0222 (2.7497)	Loss 0.32227 (0.35975)
2022-05-07 06:41:29,754 INFO 	Epoch: [45][150/309]	Batch time 2.9199 (2.7563)	Loss 0.36564 (0.35951)
2022-05-07 06:41:59,201 INFO 	Epoch: [45][160/309]	Batch time 3.3513 (2.7680)	Loss 0.31922 (0.35832)
2022-05-07 06:42:27,840 INFO 	Epoch: [45][170/309]	Batch time 2.6853 (2.7736)	Loss 0.36122 (0.35803)
2022-05-07 06:42:55,068 INFO 	Epoch: [45][180/309]	Batch time 2.8977 (2.7708)	Loss 0.31807 (0.35762)
2022-05-07 06:43:22,559 INFO 	Epoch: [45][190/309]	Batch time 2.6879 (2.7697)	Loss 0.34370 (0.35794)
2022-05-07 06:43:49,665 INFO 	Epoch: [45][200/309]	Batch time 3.3258 (2.7667)	Loss 0.32526 (0.35805)
2022-05-07 06:44:17,703 INFO 	Epoch: [45][210/309]	Batch time 2.9664 (2.7685)	Loss 0.36005 (0.35798)
2022-05-07 06:44:45,298 INFO 	Epoch: [45][220/309]	Batch time 2.7579 (2.7681)	Loss 0.35972 (0.35852)
2022-05-07 06:45:12,443 INFO 	Epoch: [45][230/309]	Batch time 2.6427 (2.7658)	Loss 0.37498 (0.35884)
2022-05-07 06:45:42,406 INFO 	Epoch: [45][240/309]	Batch time 3.2764 (2.7753)	Loss 0.33249 (0.35843)
2022-05-07 06:46:11,928 INFO 	Epoch: [45][250/309]	Batch time 2.9871 (2.7824)	Loss 0.36736 (0.35822)
2022-05-07 06:46:38,106 INFO 	Epoch: [45][260/309]	Batch time 2.6589 (2.7761)	Loss 0.37129 (0.35845)
2022-05-07 06:47:05,658 INFO 	Epoch: [45][270/309]	Batch time 2.4772 (2.7753)	Loss 0.36808 (0.35792)
2022-05-07 06:47:33,095 INFO 	Epoch: [45][280/309]	Batch time 2.5751 (2.7742)	Loss 0.37596 (0.35782)
2022-05-07 06:48:00,255 INFO 	Epoch: [45][290/309]	Batch time 2.8571 (2.7722)	Loss 0.38998 (0.35814)
2022-05-07 06:48:28,424 INFO 	Epoch: [45][300/309]	Batch time 2.9332 (2.7737)	Loss 0.36666 (0.35820)

Learning rate: 0.0008424845122589679
Step num: 14214

2022-05-07 06:48:52,714 INFO 	
Validation Loss 0.37288 (0.35072)


Epochs since last improvement: 1

Warning! Reached max decoder steps
save mel done
2022-05-07 06:49:02,639 INFO 	Epoch: [46][0/309]	Batch time 5.1690 (5.1690)	Loss 0.32286 (0.32286)
2022-05-07 06:49:30,350 INFO 	Epoch: [46][10/309]	Batch time 2.7342 (2.9891)	Loss 0.35737 (0.34768)
2022-05-07 06:49:58,155 INFO 	Epoch: [46][20/309]	Batch time 3.0600 (2.8898)	Loss 0.35205 (0.36057)
2022-05-07 06:50:25,626 INFO 	Epoch: [46][30/309]	Batch time 2.6050 (2.8437)	Loss 0.38022 (0.35749)
2022-05-07 06:50:52,718 INFO 	Epoch: [46][40/309]	Batch time 2.7517 (2.8109)	Loss 0.33444 (0.35869)
2022-05-07 06:51:20,675 INFO 	Epoch: [46][50/309]	Batch time 2.5288 (2.8079)	Loss 0.33102 (0.35931)
2022-05-07 06:51:48,352 INFO 	Epoch: [46][60/309]	Batch time 2.7379 (2.8013)	Loss 0.36551 (0.35865)
2022-05-07 06:52:15,814 INFO 	Epoch: [46][70/309]	Batch time 2.7527 (2.7936)	Loss 0.35065 (0.35918)
2022-05-07 06:52:44,433 INFO 	Epoch: [46][80/309]	Batch time 3.2286 (2.8020)	Loss 0.32596 (0.35814)
2022-05-07 06:53:14,109 INFO 	Epoch: [46][90/309]	Batch time 3.2925 (2.8202)	Loss 0.30483 (0.35648)
2022-05-07 06:53:41,723 INFO 	Epoch: [46][100/309]	Batch time 2.6587 (2.8144)	Loss 0.39744 (0.35711)
2022-05-07 06:54:09,922 INFO 	Epoch: [46][110/309]	Batch time 2.7200 (2.8149)	Loss 0.39710 (0.35678)
2022-05-07 06:54:37,916 INFO 	Epoch: [46][120/309]	Batch time 2.5132 (2.8136)	Loss 0.35775 (0.35734)
2022-05-07 06:55:06,014 INFO 	Epoch: [46][130/309]	Batch time 3.3403 (2.8133)	Loss 0.30036 (0.35789)
2022-05-07 06:55:35,309 INFO 	Epoch: [46][140/309]	Batch time 3.4255 (2.8216)	Loss 0.31451 (0.35752)
2022-05-07 06:56:04,135 INFO 	Epoch: [46][150/309]	Batch time 2.9006 (2.8256)	Loss 0.37936 (0.35739)
2022-05-07 06:56:32,517 INFO 	Epoch: [46][160/309]	Batch time 2.8493 (2.8264)	Loss 0.32373 (0.35734)
2022-05-07 06:56:59,503 INFO 	Epoch: [46][170/309]	Batch time 2.4159 (2.8189)	Loss 0.40234 (0.35684)
2022-05-07 06:57:27,955 INFO 	Epoch: [46][180/309]	Batch time 2.6841 (2.8204)	Loss 0.32657 (0.35597)
2022-05-07 06:57:54,567 INFO 	Epoch: [46][190/309]	Batch time 2.4542 (2.8120)	Loss 0.38159 (0.35619)
2022-05-07 06:58:23,865 INFO 	Epoch: [46][200/309]	Batch time 2.7876 (2.8179)	Loss 0.35486 (0.35545)
2022-05-07 06:58:52,070 INFO 	Epoch: [46][210/309]	Batch time 3.0838 (2.8180)	Loss 0.30622 (0.35506)
2022-05-07 06:59:21,393 INFO 	Epoch: [46][220/309]	Batch time 2.8290 (2.8232)	Loss 0.35529 (0.35499)
2022-05-07 06:59:49,697 INFO 	Epoch: [46][230/309]	Batch time 2.9556 (2.8235)	Loss 0.36721 (0.35516)
2022-05-07 07:00:15,609 INFO 	Epoch: [46][240/309]	Batch time 2.8600 (2.8139)	Loss 0.32336 (0.35590)
2022-05-07 07:00:43,111 INFO 	Epoch: [46][250/309]	Batch time 2.6013 (2.8113)	Loss 0.37842 (0.35587)
2022-05-07 07:01:11,112 INFO 	Epoch: [46][260/309]	Batch time 2.9538 (2.8109)	Loss 0.36681 (0.35570)
2022-05-07 07:01:38,443 INFO 	Epoch: [46][270/309]	Batch time 3.1075 (2.8080)	Loss 0.29825 (0.35550)
2022-05-07 07:02:04,683 INFO 	Epoch: [46][280/309]	Batch time 2.8978 (2.8015)	Loss 0.33000 (0.35627)
2022-05-07 07:02:32,216 INFO 	Epoch: [46][290/309]	Batch time 2.9058 (2.7998)	Loss 0.28859 (0.35587)
2022-05-07 07:03:00,391 INFO 	Epoch: [46][300/309]	Batch time 2.6744 (2.8004)	Loss 0.35760 (0.35562)

Learning rate: 0.0008168498362294491
Step num: 14523

2022-05-07 07:03:25,012 INFO 	
Validation Loss 0.36677 (0.34611)

save mel done
2022-05-07 07:03:36,573 INFO 	Epoch: [47][0/309]	Batch time 4.2406 (4.2406)	Loss 0.37834 (0.37834)
2022-05-07 07:04:04,692 INFO 	Epoch: [47][10/309]	Batch time 2.8259 (2.9418)	Loss 0.35099 (0.35361)
2022-05-07 07:04:31,560 INFO 	Epoch: [47][20/309]	Batch time 2.7615 (2.8204)	Loss 0.34408 (0.35102)
2022-05-07 07:04:58,153 INFO 	Epoch: [47][30/309]	Batch time 2.4623 (2.7684)	Loss 0.40078 (0.35398)
2022-05-07 07:05:26,768 INFO 	Epoch: [47][40/309]	Batch time 2.9014 (2.7911)	Loss 0.35764 (0.35166)
2022-05-07 07:05:54,706 INFO 	Epoch: [47][50/309]	Batch time 2.7712 (2.7916)	Loss 0.37173 (0.35175)
2022-05-07 07:06:22,230 INFO 	Epoch: [47][60/309]	Batch time 2.7134 (2.7852)	Loss 0.36824 (0.35208)
2022-05-07 07:06:49,573 INFO 	Epoch: [47][70/309]	Batch time 2.5243 (2.7780)	Loss 0.34297 (0.35124)
2022-05-07 07:07:18,195 INFO 	Epoch: [47][80/309]	Batch time 2.7639 (2.7884)	Loss 0.33643 (0.35268)
2022-05-07 07:07:44,560 INFO 	Epoch: [47][90/309]	Batch time 2.8839 (2.7717)	Loss 0.33032 (0.35372)
2022-05-07 07:08:11,510 INFO 	Epoch: [47][100/309]	Batch time 2.7305 (2.7641)	Loss 0.33858 (0.35358)
2022-05-07 07:08:37,765 INFO 	Epoch: [47][110/309]	Batch time 2.6066 (2.7516)	Loss 0.35826 (0.35427)
2022-05-07 07:09:04,638 INFO 	Epoch: [47][120/309]	Batch time 2.6537 (2.7463)	Loss 0.30421 (0.35443)
2022-05-07 07:09:31,787 INFO 	Epoch: [47][130/309]	Batch time 2.7766 (2.7439)	Loss 0.36606 (0.35414)
2022-05-07 07:09:58,497 INFO 	Epoch: [47][140/309]	Batch time 2.5107 (2.7388)	Loss 0.34267 (0.35410)
2022-05-07 07:10:26,500 INFO 	Epoch: [47][150/309]	Batch time 2.5745 (2.7428)	Loss 0.37853 (0.35365)
2022-05-07 07:10:53,127 INFO 	Epoch: [47][160/309]	Batch time 2.7362 (2.7378)	Loss 0.37222 (0.35449)
2022-05-07 07:11:20,767 INFO 	Epoch: [47][170/309]	Batch time 2.6580 (2.7394)	Loss 0.38884 (0.35498)
2022-05-07 07:11:47,462 INFO 	Epoch: [47][180/309]	Batch time 2.6514 (2.7355)	Loss 0.34238 (0.35562)
2022-05-07 07:12:15,746 INFO 	Epoch: [47][190/309]	Batch time 2.9313 (2.7404)	Loss 0.32166 (0.35537)
2022-05-07 07:12:44,120 INFO 	Epoch: [47][200/309]	Batch time 2.7947 (2.7452)	Loss 0.34807 (0.35550)
2022-05-07 07:13:11,590 INFO 	Epoch: [47][210/309]	Batch time 2.6061 (2.7453)	Loss 0.34645 (0.35568)
2022-05-07 07:13:38,538 INFO 	Epoch: [47][220/309]	Batch time 2.8340 (2.7430)	Loss 0.35859 (0.35594)
2022-05-07 07:14:08,034 INFO 	Epoch: [47][230/309]	Batch time 2.7347 (2.7520)	Loss 0.37528 (0.35585)
2022-05-07 07:14:36,098 INFO 	Epoch: [47][240/309]	Batch time 2.6111 (2.7542)	Loss 0.40595 (0.35651)
2022-05-07 07:15:05,341 INFO 	Epoch: [47][250/309]	Batch time 2.9696 (2.7610)	Loss 0.30303 (0.35601)
2022-05-07 07:15:34,851 INFO 	Epoch: [47][260/309]	Batch time 2.6726 (2.7683)	Loss 0.41536 (0.35596)
2022-05-07 07:16:03,039 INFO 	Epoch: [47][270/309]	Batch time 2.6646 (2.7701)	Loss 0.35474 (0.35583)
2022-05-07 07:16:31,119 INFO 	Epoch: [47][280/309]	Batch time 3.1374 (2.7715)	Loss 0.32739 (0.35553)
2022-05-07 07:16:59,194 INFO 	Epoch: [47][290/309]	Batch time 2.7645 (2.7727)	Loss 0.36282 (0.35562)
2022-05-07 07:17:28,293 INFO 	Epoch: [47][300/309]	Batch time 2.6183 (2.7773)	Loss 0.36704 (0.35535)

Learning rate: 0.0007919951586516244
Step num: 14832

2022-05-07 07:17:52,755 INFO 	
Validation Loss 0.37231 (0.35006)


Epochs since last improvement: 1

Warning! Reached max decoder steps
save mel done
2022-05-07 07:18:02,523 INFO 	Epoch: [48][0/309]	Batch time 5.0184 (5.0184)	Loss 0.34272 (0.34272)
2022-05-07 07:18:29,945 INFO 	Epoch: [48][10/309]	Batch time 2.4729 (2.9491)	Loss 0.39225 (0.36635)
2022-05-07 07:18:58,937 INFO 	Epoch: [48][20/309]	Batch time 3.0106 (2.9253)	Loss 0.35923 (0.36330)
2022-05-07 07:19:25,831 INFO 	Epoch: [48][30/309]	Batch time 2.6419 (2.8493)	Loss 0.35992 (0.36165)
2022-05-07 07:19:52,961 INFO 	Epoch: [48][40/309]	Batch time 2.7290 (2.8160)	Loss 0.32931 (0.36112)
2022-05-07 07:20:20,389 INFO 	Epoch: [48][50/309]	Batch time 2.9894 (2.8017)	Loss 0.33283 (0.35846)
2022-05-07 07:20:47,020 INFO 	Epoch: [48][60/309]	Batch time 2.7654 (2.7789)	Loss 0.35198 (0.36016)
2022-05-07 07:21:14,645 INFO 	Epoch: [48][70/309]	Batch time 2.8721 (2.7766)	Loss 0.32335 (0.35695)
2022-05-07 07:21:41,701 INFO 	Epoch: [48][80/309]	Batch time 2.7521 (2.7679)	Loss 0.33408 (0.35582)
2022-05-07 07:22:09,224 INFO 	Epoch: [48][90/309]	Batch time 2.6198 (2.7661)	Loss 0.36355 (0.35514)
2022-05-07 07:22:36,946 INFO 	Epoch: [48][100/309]	Batch time 2.6707 (2.7668)	Loss 0.37219 (0.35455)
2022-05-07 07:23:04,068 INFO 	Epoch: [48][110/309]	Batch time 2.6536 (2.7618)	Loss 0.34621 (0.35446)
2022-05-07 07:23:30,191 INFO 	Epoch: [48][120/309]	Batch time 2.7871 (2.7495)	Loss 0.33443 (0.35398)
2022-05-07 07:23:57,435 INFO 	Epoch: [48][130/309]	Batch time 2.8510 (2.7476)	Loss 0.34806 (0.35445)
2022-05-07 07:24:25,174 INFO 	Epoch: [48][140/309]	Batch time 2.6476 (2.7494)	Loss 0.33328 (0.35531)
2022-05-07 07:24:53,362 INFO 	Epoch: [48][150/309]	Batch time 2.9123 (2.7540)	Loss 0.33001 (0.35510)
2022-05-07 07:25:22,817 INFO 	Epoch: [48][160/309]	Batch time 2.7396 (2.7659)	Loss 0.35517 (0.35418)
2022-05-07 07:25:49,733 INFO 	Epoch: [48][170/309]	Batch time 2.8670 (2.7616)	Loss 0.35496 (0.35450)
2022-05-07 07:26:18,449 INFO 	Epoch: [48][180/309]	Batch time 2.6854 (2.7676)	Loss 0.36010 (0.35364)
2022-05-07 07:26:45,174 INFO 	Epoch: [48][190/309]	Batch time 2.4439 (2.7627)	Loss 0.35740 (0.35325)
2022-05-07 07:27:13,854 INFO 	Epoch: [48][200/309]	Batch time 2.9782 (2.7679)	Loss 0.33001 (0.35294)
2022-05-07 07:27:40,631 INFO 	Epoch: [48][210/309]	Batch time 2.3523 (2.7636)	Loss 0.35931 (0.35250)
2022-05-07 07:28:08,452 INFO 	Epoch: [48][220/309]	Batch time 2.6869 (2.7645)	Loss 0.37011 (0.35258)
2022-05-07 07:28:35,591 INFO 	Epoch: [48][230/309]	Batch time 2.4218 (2.7623)	Loss 0.41360 (0.35274)
2022-05-07 07:29:02,568 INFO 	Epoch: [48][240/309]	Batch time 2.6781 (2.7596)	Loss 0.37558 (0.35303)
2022-05-07 07:29:31,345 INFO 	Epoch: [48][250/309]	Batch time 3.0841 (2.7643)	Loss 0.31998 (0.35251)
2022-05-07 07:30:00,305 INFO 	Epoch: [48][260/309]	Batch time 2.8202 (2.7693)	Loss 0.32510 (0.35239)
2022-05-07 07:30:27,044 INFO 	Epoch: [48][270/309]	Batch time 2.5430 (2.7658)	Loss 0.37139 (0.35256)
2022-05-07 07:30:54,878 INFO 	Epoch: [48][280/309]	Batch time 2.5672 (2.7665)	Loss 0.37763 (0.35211)
2022-05-07 07:31:23,699 INFO 	Epoch: [48][290/309]	Batch time 2.9619 (2.7704)	Loss 0.35905 (0.35199)
2022-05-07 07:31:52,685 INFO 	Epoch: [48][300/309]	Batch time 2.8017 (2.7747)	Loss 0.36098 (0.35188)

Learning rate: 0.0007678967461424802
Step num: 15141

2022-05-07 07:32:17,770 INFO 	
Validation Loss 0.36614 (0.34314)

Warning! Reached max decoder steps
save mel done
2022-05-07 07:32:29,751 INFO 	Epoch: [49][0/309]	Batch time 4.4350 (4.4350)	Loss 0.39211 (0.39211)
2022-05-07 07:32:56,958 INFO 	Epoch: [49][10/309]	Batch time 2.7661 (2.8765)	Loss 0.39461 (0.34973)
2022-05-07 07:33:23,836 INFO 	Epoch: [49][20/309]	Batch time 2.6435 (2.7867)	Loss 0.39134 (0.35363)
2022-05-07 07:33:51,117 INFO 	Epoch: [49][30/309]	Batch time 2.4383 (2.7678)	Loss 0.41727 (0.35599)
2022-05-07 07:34:17,812 INFO 	Epoch: [49][40/309]	Batch time 2.6701 (2.7438)	Loss 0.33730 (0.35457)
2022-05-07 07:34:46,151 INFO 	Epoch: [49][50/309]	Batch time 2.9200 (2.7615)	Loss 0.34968 (0.35194)
2022-05-07 07:35:13,432 INFO 	Epoch: [49][60/309]	Batch time 2.5627 (2.7560)	Loss 0.32378 (0.35151)
2022-05-07 07:35:41,865 INFO 	Epoch: [49][70/309]	Batch time 2.6035 (2.7683)	Loss 0.36391 (0.35025)
2022-05-07 07:36:10,389 INFO 	Epoch: [49][80/309]	Batch time 2.6890 (2.7787)	Loss 0.39556 (0.35077)
2022-05-07 07:36:37,831 INFO 	Epoch: [49][90/309]	Batch time 2.7778 (2.7749)	Loss 0.39296 (0.35177)
2022-05-07 07:37:05,528 INFO 	Epoch: [49][100/309]	Batch time 2.5749 (2.7744)	Loss 0.36369 (0.35351)
2022-05-07 07:37:33,383 INFO 	Epoch: [49][110/309]	Batch time 2.7565 (2.7754)	Loss 0.39442 (0.35274)
2022-05-07 07:38:01,026 INFO 	Epoch: [49][120/309]	Batch time 2.5746 (2.7745)	Loss 0.40967 (0.35293)
2022-05-07 07:38:28,775 INFO 	Epoch: [49][130/309]	Batch time 2.5157 (2.7745)	Loss 0.37319 (0.35319)
2022-05-07 07:38:56,726 INFO 	Epoch: [49][140/309]	Batch time 3.0210 (2.7760)	Loss 0.30390 (0.35276)
2022-05-07 07:39:24,527 INFO 	Epoch: [49][150/309]	Batch time 2.5922 (2.7762)	Loss 0.41254 (0.35409)
2022-05-07 07:39:52,586 INFO 	Epoch: [49][160/309]	Batch time 2.7869 (2.7781)	Loss 0.33868 (0.35478)
2022-05-07 07:40:21,067 INFO 	Epoch: [49][170/309]	Batch time 2.6905 (2.7822)	Loss 0.38866 (0.35391)
2022-05-07 07:40:49,702 INFO 	Epoch: [49][180/309]	Batch time 3.1687 (2.7867)	Loss 0.29679 (0.35382)
2022-05-07 07:41:17,787 INFO 	Epoch: [49][190/309]	Batch time 2.6655 (2.7878)	Loss 0.36306 (0.35418)
2022-05-07 07:41:45,869 INFO 	Epoch: [49][200/309]	Batch time 2.7072 (2.7888)	Loss 0.38101 (0.35359)
2022-05-07 07:42:13,348 INFO 	Epoch: [49][210/309]	Batch time 2.5252 (2.7869)	Loss 0.36328 (0.35339)
2022-05-07 07:42:39,749 INFO 	Epoch: [49][220/309]	Batch time 2.6198 (2.7802)	Loss 0.35183 (0.35319)
2022-05-07 07:43:06,781 INFO 	Epoch: [49][230/309]	Batch time 2.5132 (2.7769)	Loss 0.38184 (0.35297)
2022-05-07 07:43:34,491 INFO 	Epoch: [49][240/309]	Batch time 2.4322 (2.7767)	Loss 0.37305 (0.35293)
2022-05-07 07:44:01,671 INFO 	Epoch: [49][250/309]	Batch time 2.7268 (2.7743)	Loss 0.34646 (0.35280)
2022-05-07 07:44:29,063 INFO 	Epoch: [49][260/309]	Batch time 2.6840 (2.7730)	Loss 0.37917 (0.35278)
2022-05-07 07:44:57,281 INFO 	Epoch: [49][270/309]	Batch time 2.2961 (2.7748)	Loss 0.41934 (0.35250)
2022-05-07 07:45:23,837 INFO 	Epoch: [49][280/309]	Batch time 2.7865 (2.7705)	Loss 0.36010 (0.35263)
2022-05-07 07:45:52,440 INFO 	Epoch: [49][290/309]	Batch time 3.2050 (2.7736)	Loss 0.32048 (0.35242)
2022-05-07 07:46:20,414 INFO 	Epoch: [49][300/309]	Batch time 2.6466 (2.7744)	Loss 0.38402 (0.35257)

Learning rate: 0.0007445315874659094
Step num: 15450

2022-05-07 07:46:45,412 INFO 	
Validation Loss 0.36264 (0.34055)

save mel done
2022-05-07 07:46:57,380 INFO 	Epoch: [50][0/309]	Batch time 4.5371 (4.5371)	Loss 0.33643 (0.33643)
2022-05-07 07:47:23,846 INFO 	Epoch: [50][10/309]	Batch time 2.6662 (2.8185)	Loss 0.37836 (0.34743)
2022-05-07 07:47:53,016 INFO 	Epoch: [50][20/309]	Batch time 2.8961 (2.8654)	Loss 0.36683 (0.34089)
2022-05-07 07:48:20,281 INFO 	Epoch: [50][30/309]	Batch time 2.5678 (2.8206)	Loss 0.35265 (0.34332)
2022-05-07 07:48:47,424 INFO 	Epoch: [50][40/309]	Batch time 2.7195 (2.7947)	Loss 0.33274 (0.34475)
2022-05-07 07:49:14,656 INFO 	Epoch: [50][50/309]	Batch time 2.6124 (2.7806)	Loss 0.37769 (0.34681)
2022-05-07 07:49:42,279 INFO 	Epoch: [50][60/309]	Batch time 2.8747 (2.7776)	Loss 0.33333 (0.34762)
2022-05-07 07:50:10,630 INFO 	Epoch: [50][70/309]	Batch time 2.7064 (2.7857)	Loss 0.42165 (0.34914)
2022-05-07 07:50:39,005 INFO 	Epoch: [50][80/309]	Batch time 2.5085 (2.7921)	Loss 0.34444 (0.34845)
2022-05-07 07:51:06,333 INFO 	Epoch: [50][90/309]	Batch time 2.5447 (2.7856)	Loss 0.33075 (0.34786)
2022-05-07 07:51:32,544 INFO 	Epoch: [50][100/309]	Batch time 2.8580 (2.7693)	Loss 0.35322 (0.35045)
2022-05-07 07:52:00,126 INFO 	Epoch: [50][110/309]	Batch time 2.5835 (2.7683)	Loss 0.35083 (0.35070)
2022-05-07 07:52:27,134 INFO 	Epoch: [50][120/309]	Batch time 2.5901 (2.7627)	Loss 0.34974 (0.35071)
2022-05-07 07:52:54,461 INFO 	Epoch: [50][130/309]	Batch time 2.6759 (2.7604)	Loss 0.36494 (0.35103)
2022-05-07 07:53:23,493 INFO 	Epoch: [50][140/309]	Batch time 3.0172 (2.7706)	Loss 0.28283 (0.35139)
2022-05-07 07:53:49,769 INFO 	Epoch: [50][150/309]	Batch time 2.5652 (2.7611)	Loss 0.37048 (0.35148)
2022-05-07 07:54:17,056 INFO 	Epoch: [50][160/309]	Batch time 2.5783 (2.7591)	Loss 0.36770 (0.35161)
2022-05-07 07:54:43,859 INFO 	Epoch: [50][170/309]	Batch time 2.5731 (2.7545)	Loss 0.39037 (0.35247)
2022-05-07 07:55:11,598 INFO 	Epoch: [50][180/309]	Batch time 2.8228 (2.7556)	Loss 0.31672 (0.35189)
2022-05-07 07:55:39,110 INFO 	Epoch: [50][190/309]	Batch time 2.9027 (2.7553)	Loss 0.36029 (0.35182)
2022-05-07 07:56:06,584 INFO 	Epoch: [50][200/309]	Batch time 2.6810 (2.7549)	Loss 0.34508 (0.35164)
2022-05-07 07:56:34,449 INFO 	Epoch: [50][210/309]	Batch time 2.4532 (2.7564)	Loss 0.37712 (0.35140)
2022-05-07 07:57:02,091 INFO 	Epoch: [50][220/309]	Batch time 2.7236 (2.7568)	Loss 0.38414 (0.35108)
2022-05-07 07:57:28,983 INFO 	Epoch: [50][230/309]	Batch time 2.8432 (2.7539)	Loss 0.34014 (0.35117)
2022-05-07 07:57:56,814 INFO 	Epoch: [50][240/309]	Batch time 2.8440 (2.7551)	Loss 0.35024 (0.35073)
2022-05-07 07:58:24,471 INFO 	Epoch: [50][250/309]	Batch time 3.0854 (2.7555)	Loss 0.32939 (0.35116)
2022-05-07 07:58:52,075 INFO 	Epoch: [50][260/309]	Batch time 3.1475 (2.7557)	Loss 0.33001 (0.35111)
2022-05-07 07:59:20,477 INFO 	Epoch: [50][270/309]	Batch time 2.7518 (2.7588)	Loss 0.34934 (0.35117)
2022-05-07 07:59:48,892 INFO 	Epoch: [50][280/309]	Batch time 2.7777 (2.7617)	Loss 0.38275 (0.35126)
2022-05-07 08:00:19,510 INFO 	Epoch: [50][290/309]	Batch time 2.5737 (2.7721)	Loss 0.35334 (0.35016)
2022-05-07 08:00:46,821 INFO 	Epoch: [50][300/309]	Batch time 3.0889 (2.7707)	Loss 0.32283 (0.35046)

Learning rate: 0.0007218773715596052
Step num: 15759

2022-05-07 08:01:11,857 INFO 	
Validation Loss 0.36124 (0.33984)

save mel done
2022-05-07 08:01:23,960 INFO 	Epoch: [51][0/309]	Batch time 4.6541 (4.6541)	Loss 0.32613 (0.32613)
2022-05-07 08:01:52,624 INFO 	Epoch: [51][10/309]	Batch time 2.8222 (3.0290)	Loss 0.33042 (0.32825)
2022-05-07 08:02:19,836 INFO 	Epoch: [51][20/309]	Batch time 2.8727 (2.8824)	Loss 0.34991 (0.34333)
2022-05-07 08:02:48,042 INFO 	Epoch: [51][30/309]	Batch time 2.8302 (2.8625)	Loss 0.31519 (0.34552)
2022-05-07 08:03:14,939 INFO 	Epoch: [51][40/309]	Batch time 2.7976 (2.8203)	Loss 0.33521 (0.34852)
2022-05-07 08:03:42,838 INFO 	Epoch: [51][50/309]	Batch time 2.8732 (2.8144)	Loss 0.39722 (0.35075)
2022-05-07 08:04:11,718 INFO 	Epoch: [51][60/309]	Batch time 3.2975 (2.8264)	Loss 0.29861 (0.35027)
2022-05-07 08:04:39,813 INFO 	Epoch: [51][70/309]	Batch time 2.6614 (2.8241)	Loss 0.38127 (0.35235)
2022-05-07 08:05:09,065 INFO 	Epoch: [51][80/309]	Batch time 2.6886 (2.8365)	Loss 0.35054 (0.34976)
2022-05-07 08:05:36,706 INFO 	Epoch: [51][90/309]	Batch time 2.6395 (2.8286)	Loss 0.35623 (0.34936)
2022-05-07 08:06:04,088 INFO 	Epoch: [51][100/309]	Batch time 2.6777 (2.8196)	Loss 0.38053 (0.35028)
2022-05-07 08:06:31,579 INFO 	Epoch: [51][110/309]	Batch time 2.8219 (2.8133)	Loss 0.35561 (0.35116)
2022-05-07 08:07:00,360 INFO 	Epoch: [51][120/309]	Batch time 2.8373 (2.8186)	Loss 0.36809 (0.35078)
2022-05-07 08:07:28,703 INFO 	Epoch: [51][130/309]	Batch time 2.7769 (2.8198)	Loss 0.33418 (0.35052)
2022-05-07 08:07:56,617 INFO 	Epoch: [51][140/309]	Batch time 2.8524 (2.8178)	Loss 0.35401 (0.35008)
2022-05-07 08:08:24,486 INFO 	Epoch: [51][150/309]	Batch time 2.4402 (2.8158)	Loss 0.36633 (0.34972)
2022-05-07 08:08:50,988 INFO 	Epoch: [51][160/309]	Batch time 2.6078 (2.8055)	Loss 0.37903 (0.35021)
2022-05-07 08:09:17,426 INFO 	Epoch: [51][170/309]	Batch time 2.8742 (2.7960)	Loss 0.35452 (0.35054)
2022-05-07 08:09:46,708 INFO 	Epoch: [51][180/309]	Batch time 3.0713 (2.8033)	Loss 0.31545 (0.35002)
2022-05-07 08:10:13,854 INFO 	Epoch: [51][190/309]	Batch time 2.8085 (2.7987)	Loss 0.33382 (0.35019)
2022-05-07 08:10:42,342 INFO 	Epoch: [51][200/309]	Batch time 3.0368 (2.8012)	Loss 0.28798 (0.34965)
2022-05-07 08:11:09,558 INFO 	Epoch: [51][210/309]	Batch time 2.7223 (2.7974)	Loss 0.35737 (0.34988)
2022-05-07 08:11:37,014 INFO 	Epoch: [51][220/309]	Batch time 2.7624 (2.7951)	Loss 0.33616 (0.34978)
2022-05-07 08:12:04,190 INFO 	Epoch: [51][230/309]	Batch time 2.6224 (2.7917)	Loss 0.35534 (0.34948)
2022-05-07 08:12:32,057 INFO 	Epoch: [51][240/309]	Batch time 2.7952 (2.7915)	Loss 0.33639 (0.34921)
2022-05-07 08:12:58,592 INFO 	Epoch: [51][250/309]	Batch time 2.4916 (2.7860)	Loss 0.32590 (0.34940)
2022-05-07 08:13:25,959 INFO 	Epoch: [51][260/309]	Batch time 2.9788 (2.7841)	Loss 0.33190 (0.34953)
2022-05-07 08:13:54,243 INFO 	Epoch: [51][270/309]	Batch time 3.1000 (2.7857)	Loss 0.32108 (0.34941)
2022-05-07 08:14:22,033 INFO 	Epoch: [51][280/309]	Batch time 2.8861 (2.7855)	Loss 0.31932 (0.34918)
2022-05-07 08:14:50,049 INFO 	Epoch: [51][290/309]	Batch time 3.1116 (2.7861)	Loss 0.37170 (0.34948)
2022-05-07 08:15:19,508 INFO 	Epoch: [51][300/309]	Batch time 3.0816 (2.7914)	Loss 0.31649 (0.34906)

Learning rate: 0.000699912466230541
Step num: 16068

2022-05-07 08:15:44,580 INFO 	
Validation Loss 0.36135 (0.33724)

save mel done
2022-05-07 08:15:56,642 INFO 	Epoch: [52][0/309]	Batch time 4.9801 (4.9801)	Loss 0.32784 (0.32784)
2022-05-07 08:16:23,437 INFO 	Epoch: [52][10/309]	Batch time 2.7539 (2.8887)	Loss 0.32843 (0.34460)
2022-05-07 08:16:51,123 INFO 	Epoch: [52][20/309]	Batch time 2.7287 (2.8315)	Loss 0.34569 (0.34769)
2022-05-07 08:17:18,526 INFO 	Epoch: [52][30/309]	Batch time 2.5277 (2.8021)	Loss 0.33803 (0.34785)
2022-05-07 08:17:45,666 INFO 	Epoch: [52][40/309]	Batch time 2.6291 (2.7806)	Loss 0.31172 (0.34908)
2022-05-07 08:18:14,258 INFO 	Epoch: [52][50/309]	Batch time 3.0073 (2.7960)	Loss 0.32767 (0.34578)
2022-05-07 08:18:42,072 INFO 	Epoch: [52][60/309]	Batch time 2.9158 (2.7936)	Loss 0.34716 (0.34685)
2022-05-07 08:19:09,381 INFO 	Epoch: [52][70/309]	Batch time 2.3172 (2.7848)	Loss 0.40544 (0.34789)
2022-05-07 08:19:36,995 INFO 	Epoch: [52][80/309]	Batch time 2.6851 (2.7819)	Loss 0.34253 (0.34837)
2022-05-07 08:20:04,667 INFO 	Epoch: [52][90/309]	Batch time 2.7254 (2.7803)	Loss 0.39476 (0.34862)
2022-05-07 08:20:30,927 INFO 	Epoch: [52][100/309]	Batch time 2.7765 (2.7650)	Loss 0.32349 (0.34979)
2022-05-07 08:20:58,670 INFO 	Epoch: [52][110/309]	Batch time 2.7190 (2.7658)	Loss 0.36889 (0.34971)
2022-05-07 08:21:25,462 INFO 	Epoch: [52][120/309]	Batch time 2.7364 (2.7587)	Loss 0.36428 (0.35023)
2022-05-07 08:21:52,109 INFO 	Epoch: [52][130/309]	Batch time 2.5519 (2.7515)	Loss 0.37174 (0.35013)
2022-05-07 08:22:21,482 INFO 	Epoch: [52][140/309]	Batch time 3.1732 (2.7647)	Loss 0.32184 (0.34981)
2022-05-07 08:22:49,490 INFO 	Epoch: [52][150/309]	Batch time 2.6903 (2.7671)	Loss 0.35180 (0.34949)
2022-05-07 08:23:19,403 INFO 	Epoch: [52][160/309]	Batch time 2.8466 (2.7810)	Loss 0.34392 (0.34782)
2022-05-07 08:23:48,223 INFO 	Epoch: [52][170/309]	Batch time 2.7077 (2.7869)	Loss 0.36922 (0.34683)
2022-05-07 08:24:16,324 INFO 	Epoch: [52][180/309]	Batch time 2.6840 (2.7882)	Loss 0.37038 (0.34707)
2022-05-07 08:24:45,470 INFO 	Epoch: [52][190/309]	Batch time 3.2155 (2.7948)	Loss 0.30809 (0.34677)
2022-05-07 08:25:13,387 INFO 	Epoch: [52][200/309]	Batch time 2.6561 (2.7947)	Loss 0.37390 (0.34742)
2022-05-07 08:25:41,619 INFO 	Epoch: [52][210/309]	Batch time 2.6433 (2.7960)	Loss 0.39720 (0.34683)
2022-05-07 08:26:09,377 INFO 	Epoch: [52][220/309]	Batch time 3.0863 (2.7951)	Loss 0.32468 (0.34625)
2022-05-07 08:26:36,243 INFO 	Epoch: [52][230/309]	Batch time 3.0276 (2.7904)	Loss 0.30143 (0.34623)
2022-05-07 08:27:03,853 INFO 	Epoch: [52][240/309]	Batch time 2.6686 (2.7892)	Loss 0.33188 (0.34593)
2022-05-07 08:27:32,600 INFO 	Epoch: [52][250/309]	Batch time 2.8299 (2.7926)	Loss 0.34770 (0.34598)
2022-05-07 08:28:01,615 INFO 	Epoch: [52][260/309]	Batch time 2.7446 (2.7968)	Loss 0.34072 (0.34601)
2022-05-07 08:28:28,502 INFO 	Epoch: [52][270/309]	Batch time 3.1173 (2.7928)	Loss 0.31733 (0.34627)
2022-05-07 08:28:55,795 INFO 	Epoch: [52][280/309]	Batch time 2.7166 (2.7905)	Loss 0.32448 (0.34613)
2022-05-07 08:29:22,471 INFO 	Epoch: [52][290/309]	Batch time 2.8118 (2.7863)	Loss 0.32604 (0.34613)
2022-05-07 08:29:51,052 INFO 	Epoch: [52][300/309]	Batch time 2.6748 (2.7887)	Loss 0.37194 (0.34614)

Learning rate: 0.0006786158974986918
Step num: 16377

2022-05-07 08:30:16,283 INFO 	
Validation Loss 0.35925 (0.33769)


Epochs since last improvement: 1

save mel done
2022-05-07 08:30:25,772 INFO 	Epoch: [53][0/309]	Batch time 4.7005 (4.7005)	Loss 0.36522 (0.36522)
2022-05-07 08:30:53,259 INFO 	Epoch: [53][10/309]	Batch time 2.5159 (2.9262)	Loss 0.33880 (0.34648)
2022-05-07 08:31:21,763 INFO 	Epoch: [53][20/309]	Batch time 3.1583 (2.8901)	Loss 0.29907 (0.34499)
2022-05-07 08:31:49,769 INFO 	Epoch: [53][30/309]	Batch time 3.0077 (2.8612)	Loss 0.32454 (0.34668)
2022-05-07 08:32:17,327 INFO 	Epoch: [53][40/309]	Batch time 2.5409 (2.8355)	Loss 0.37205 (0.34628)
2022-05-07 08:32:44,914 INFO 	Epoch: [53][50/309]	Batch time 2.8945 (2.8205)	Loss 0.32319 (0.34992)
2022-05-07 08:33:13,193 INFO 	Epoch: [53][60/309]	Batch time 3.1669 (2.8217)	Loss 0.31387 (0.35058)
2022-05-07 08:33:41,831 INFO 	Epoch: [53][70/309]	Batch time 2.7858 (2.8276)	Loss 0.35803 (0.35102)
2022-05-07 08:34:09,994 INFO 	Epoch: [53][80/309]	Batch time 2.9281 (2.8262)	Loss 0.33706 (0.34999)
2022-05-07 08:34:38,763 INFO 	Epoch: [53][90/309]	Batch time 3.0557 (2.8318)	Loss 0.35075 (0.34884)
2022-05-07 08:35:07,128 INFO 	Epoch: [53][100/309]	Batch time 2.7860 (2.8322)	Loss 0.34049 (0.34818)
2022-05-07 08:35:35,119 INFO 	Epoch: [53][110/309]	Batch time 2.7901 (2.8293)	Loss 0.32476 (0.34844)
2022-05-07 08:36:02,936 INFO 	Epoch: [53][120/309]	Batch time 2.6978 (2.8253)	Loss 0.35697 (0.34866)
2022-05-07 08:36:30,398 INFO 	Epoch: [53][130/309]	Batch time 2.7293 (2.8193)	Loss 0.34966 (0.34821)
2022-05-07 08:36:57,827 INFO 	Epoch: [53][140/309]	Batch time 2.7508 (2.8139)	Loss 0.36920 (0.34861)
2022-05-07 08:37:25,818 INFO 	Epoch: [53][150/309]	Batch time 2.8117 (2.8129)	Loss 0.33531 (0.34827)
2022-05-07 08:37:55,500 INFO 	Epoch: [53][160/309]	Batch time 3.1564 (2.8225)	Loss 0.33594 (0.34721)
2022-05-07 08:38:23,473 INFO 	Epoch: [53][170/309]	Batch time 2.5992 (2.8211)	Loss 0.36302 (0.34728)
2022-05-07 08:38:51,255 INFO 	Epoch: [53][180/309]	Batch time 2.9105 (2.8187)	Loss 0.32838 (0.34724)
2022-05-07 08:39:19,684 INFO 	Epoch: [53][190/309]	Batch time 2.8585 (2.8200)	Loss 0.34143 (0.34724)
2022-05-07 08:39:48,263 INFO 	Epoch: [53][200/309]	Batch time 2.8543 (2.8219)	Loss 0.35228 (0.34690)
2022-05-07 08:40:15,607 INFO 	Epoch: [53][210/309]	Batch time 2.7957 (2.8177)	Loss 0.32468 (0.34744)
2022-05-07 08:40:44,408 INFO 	Epoch: [53][220/309]	Batch time 2.6583 (2.8205)	Loss 0.36866 (0.34686)
2022-05-07 08:41:13,125 INFO 	Epoch: [53][230/309]	Batch time 2.4523 (2.8227)	Loss 0.38730 (0.34644)
2022-05-07 08:41:40,512 INFO 	Epoch: [53][240/309]	Batch time 2.5831 (2.8193)	Loss 0.35697 (0.34666)
2022-05-07 08:42:08,580 INFO 	Epoch: [53][250/309]	Batch time 3.3169 (2.8188)	Loss 0.31153 (0.34659)
2022-05-07 08:42:36,764 INFO 	Epoch: [53][260/309]	Batch time 2.7180 (2.8187)	Loss 0.34438 (0.34682)
2022-05-07 08:43:04,699 INFO 	Epoch: [53][270/309]	Batch time 3.2095 (2.8178)	Loss 0.31336 (0.34670)
2022-05-07 08:43:33,420 INFO 	Epoch: [53][280/309]	Batch time 2.6355 (2.8197)	Loss 0.35890 (0.34680)
2022-05-07 08:44:01,835 INFO 	Epoch: [53][290/309]	Batch time 2.6262 (2.8205)	Loss 0.32930 (0.34647)
2022-05-07 08:44:30,235 INFO 	Epoch: [53][300/309]	Batch time 2.8394 (2.8211)	Loss 0.33351 (0.34652)

Learning rate: 0.0006579673295692758
Step num: 16686

2022-05-07 08:44:56,132 INFO 	
Validation Loss 0.36114 (0.33946)


Epochs since last improvement: 2

save mel done
2022-05-07 08:45:06,007 INFO 	Epoch: [54][0/309]	Batch time 5.1688 (5.1688)	Loss 0.31393 (0.31393)
2022-05-07 08:45:32,472 INFO 	Epoch: [54][10/309]	Batch time 2.8559 (2.8758)	Loss 0.34049 (0.34131)
2022-05-07 08:46:00,036 INFO 	Epoch: [54][20/309]	Batch time 3.0803 (2.8189)	Loss 0.30905 (0.33786)
2022-05-07 08:46:28,152 INFO 	Epoch: [54][30/309]	Batch time 2.7069 (2.8166)	Loss 0.34918 (0.33949)
2022-05-07 08:46:56,493 INFO 	Epoch: [54][40/309]	Batch time 3.1180 (2.8208)	Loss 0.35113 (0.34081)
2022-05-07 08:47:23,350 INFO 	Epoch: [54][50/309]	Batch time 2.7798 (2.7943)	Loss 0.32700 (0.34303)
2022-05-07 08:47:50,432 INFO 	Epoch: [54][60/309]	Batch time 2.6139 (2.7802)	Loss 0.35317 (0.34337)
2022-05-07 08:48:17,511 INFO 	Epoch: [54][70/309]	Batch time 2.9903 (2.7700)	Loss 0.28501 (0.34263)
2022-05-07 08:48:44,650 INFO 	Epoch: [54][80/309]	Batch time 2.5609 (2.7631)	Loss 0.37932 (0.34373)
2022-05-07 08:49:11,753 INFO 	Epoch: [54][90/309]	Batch time 2.5820 (2.7573)	Loss 0.38306 (0.34533)
2022-05-07 08:49:39,189 INFO 	Epoch: [54][100/309]	Batch time 2.6225 (2.7559)	Loss 0.33960 (0.34522)
2022-05-07 08:50:05,334 INFO 	Epoch: [54][110/309]	Batch time 2.6706 (2.7432)	Loss 0.31056 (0.34519)
2022-05-07 08:50:32,792 INFO 	Epoch: [54][120/309]	Batch time 2.8250 (2.7434)	Loss 0.36097 (0.34469)
2022-05-07 08:51:00,543 INFO 	Epoch: [54][130/309]	Batch time 2.5931 (2.7458)	Loss 0.30880 (0.34488)
2022-05-07 08:51:26,272 INFO 	Epoch: [54][140/309]	Batch time 2.5951 (2.7336)	Loss 0.34338 (0.34539)
2022-05-07 08:51:54,552 INFO 	Epoch: [54][150/309]	Batch time 2.7256 (2.7398)	Loss 0.37303 (0.34569)
2022-05-07 08:52:22,884 INFO 	Epoch: [54][160/309]	Batch time 2.7971 (2.7456)	Loss 0.36037 (0.34536)
2022-05-07 08:52:51,227 INFO 	Epoch: [54][170/309]	Batch time 2.8454 (2.7508)	Loss 0.32368 (0.34467)
2022-05-07 08:53:19,429 INFO 	Epoch: [54][180/309]	Batch time 2.7305 (2.7546)	Loss 0.34524 (0.34462)
2022-05-07 08:53:47,691 INFO 	Epoch: [54][190/309]	Batch time 2.5093 (2.7584)	Loss 0.39492 (0.34469)
2022-05-07 08:54:15,553 INFO 	Epoch: [54][200/309]	Batch time 3.3614 (2.7598)	Loss 0.28226 (0.34488)
2022-05-07 08:54:43,278 INFO 	Epoch: [54][210/309]	Batch time 2.8632 (2.7604)	Loss 0.32915 (0.34510)
2022-05-07 08:55:10,910 INFO 	Epoch: [54][220/309]	Batch time 2.9596 (2.7605)	Loss 0.32980 (0.34579)
2022-05-07 08:55:36,795 INFO 	Epoch: [54][230/309]	Batch time 2.3653 (2.7531)	Loss 0.38802 (0.34642)
2022-05-07 08:56:04,091 INFO 	Epoch: [54][240/309]	Batch time 2.6596 (2.7521)	Loss 0.32430 (0.34604)
2022-05-07 08:56:30,956 INFO 	Epoch: [54][250/309]	Batch time 2.8832 (2.7495)	Loss 0.37405 (0.34625)
2022-05-07 08:56:59,823 INFO 	Epoch: [54][260/309]	Batch time 2.5970 (2.7547)	Loss 0.35930 (0.34563)
2022-05-07 08:57:29,109 INFO 	Epoch: [54][270/309]	Batch time 2.5734 (2.7611)	Loss 0.34318 (0.34530)
2022-05-07 08:57:55,970 INFO 	Epoch: [54][280/309]	Batch time 2.7243 (2.7585)	Loss 0.33968 (0.34534)
2022-05-07 08:58:22,719 INFO 	Epoch: [54][290/309]	Batch time 2.8050 (2.7556)	Loss 0.32609 (0.34535)
2022-05-07 08:58:51,850 INFO 	Epoch: [54][300/309]	Batch time 2.5630 (2.7608)	Loss 0.35391 (0.34535)

Learning rate: 0.0006379470454143888
Step num: 16995

2022-05-07 08:59:16,570 INFO 	
Validation Loss 0.36028 (0.33816)


Epochs since last improvement: 3

save mel done
2022-05-07 08:59:26,054 INFO 	Epoch: [55][0/309]	Batch time 4.6597 (4.6597)	Loss 0.34730 (0.34730)
2022-05-07 08:59:52,765 INFO 	Epoch: [55][10/309]	Batch time 2.7632 (2.8519)	Loss 0.35581 (0.35319)
2022-05-07 09:00:20,325 INFO 	Epoch: [55][20/309]	Batch time 2.8055 (2.8062)	Loss 0.34282 (0.34480)
2022-05-07 09:00:47,696 INFO 	Epoch: [55][30/309]	Batch time 2.5197 (2.7839)	Loss 0.36899 (0.34853)
2022-05-07 09:01:14,749 INFO 	Epoch: [55][40/309]	Batch time 2.5624 (2.7648)	Loss 0.33830 (0.34839)
2022-05-07 09:01:42,488 INFO 	Epoch: [55][50/309]	Batch time 2.8048 (2.7665)	Loss 0.35984 (0.34672)
2022-05-07 09:02:10,653 INFO 	Epoch: [55][60/309]	Batch time 3.2086 (2.7747)	Loss 0.30299 (0.34469)
2022-05-07 09:02:38,581 INFO 	Epoch: [55][70/309]	Batch time 2.8701 (2.7773)	Loss 0.32226 (0.34591)
2022-05-07 09:03:07,114 INFO 	Epoch: [55][80/309]	Batch time 3.0040 (2.7867)	Loss 0.29622 (0.34530)
2022-05-07 09:03:33,845 INFO 	Epoch: [55][90/309]	Batch time 2.5341 (2.7742)	Loss 0.33622 (0.34625)
2022-05-07 09:04:01,339 INFO 	Epoch: [55][100/309]	Batch time 3.0365 (2.7717)	Loss 0.31379 (0.34556)
2022-05-07 09:04:28,956 INFO 	Epoch: [55][110/309]	Batch time 3.4418 (2.7708)	Loss 0.30680 (0.34551)
2022-05-07 09:04:55,413 INFO 	Epoch: [55][120/309]	Batch time 2.6562 (2.7605)	Loss 0.40344 (0.34575)
2022-05-07 09:05:21,616 INFO 	Epoch: [55][130/309]	Batch time 2.6239 (2.7498)	Loss 0.35482 (0.34580)
2022-05-07 09:05:47,767 INFO 	Epoch: [55][140/309]	Batch time 2.7280 (2.7402)	Loss 0.38591 (0.34724)
2022-05-07 09:06:14,853 INFO 	Epoch: [55][150/309]	Batch time 2.7171 (2.7381)	Loss 0.33212 (0.34692)
2022-05-07 09:06:41,482 INFO 	Epoch: [55][160/309]	Batch time 2.4625 (2.7335)	Loss 0.37695 (0.34667)
2022-05-07 09:07:07,962 INFO 	Epoch: [55][170/309]	Batch time 2.5901 (2.7285)	Loss 0.34402 (0.34654)
2022-05-07 09:07:34,523 INFO 	Epoch: [55][180/309]	Batch time 2.5652 (2.7245)	Loss 0.31816 (0.34613)
2022-05-07 09:08:02,189 INFO 	Epoch: [55][190/309]	Batch time 2.8309 (2.7267)	Loss 0.31183 (0.34554)
2022-05-07 09:08:28,622 INFO 	Epoch: [55][200/309]	Batch time 2.7665 (2.7225)	Loss 0.33918 (0.34580)
2022-05-07 09:08:55,061 INFO 	Epoch: [55][210/309]	Batch time 2.5334 (2.7188)	Loss 0.33013 (0.34594)
2022-05-07 09:09:21,562 INFO 	Epoch: [55][220/309]	Batch time 2.9077 (2.7157)	Loss 0.33927 (0.34616)
2022-05-07 09:09:48,198 INFO 	Epoch: [55][230/309]	Batch time 2.9418 (2.7134)	Loss 0.36152 (0.34601)
2022-05-07 09:10:13,966 INFO 	Epoch: [55][240/309]	Batch time 2.5662 (2.7078)	Loss 0.35843 (0.34635)
2022-05-07 09:10:41,827 INFO 	Epoch: [55][250/309]	Batch time 3.4598 (2.7109)	Loss 0.27020 (0.34623)
2022-05-07 09:11:10,803 INFO 	Epoch: [55][260/309]	Batch time 3.0205 (2.7180)	Loss 0.33084 (0.34570)
2022-05-07 09:11:37,899 INFO 	Epoch: [55][270/309]	Batch time 2.8329 (2.7177)	Loss 0.31061 (0.34578)
2022-05-07 09:12:05,609 INFO 	Epoch: [55][280/309]	Batch time 2.5914 (2.7196)	Loss 0.37242 (0.34526)
2022-05-07 09:12:32,796 INFO 	Epoch: [55][290/309]	Batch time 2.5285 (2.7196)	Loss 0.35430 (0.34517)
2022-05-07 09:12:59,658 INFO 	Epoch: [55][300/309]	Batch time 2.5304 (2.7185)	Loss 0.36066 (0.34530)

Learning rate: 0.0006185359279454901
Step num: 17304

2022-05-07 09:13:24,429 INFO 	
Validation Loss 0.35736 (0.33554)

Warning! Reached max decoder steps
save mel done
2022-05-07 09:13:35,666 INFO 	Epoch: [56][0/309]	Batch time 3.9872 (3.9872)	Loss 0.36232 (0.36232)
2022-05-07 09:14:02,357 INFO 	Epoch: [56][10/309]	Batch time 2.7651 (2.7890)	Loss 0.31463 (0.33798)
2022-05-07 09:14:29,128 INFO 	Epoch: [56][20/309]	Batch time 2.5031 (2.7357)	Loss 0.37260 (0.33857)
2022-05-07 09:14:57,134 INFO 	Epoch: [56][30/309]	Batch time 2.5740 (2.7566)	Loss 0.36217 (0.34203)
2022-05-07 09:15:24,520 INFO 	Epoch: [56][40/309]	Batch time 2.5494 (2.7522)	Loss 0.35433 (0.34194)
2022-05-07 09:15:51,820 INFO 	Epoch: [56][50/309]	Batch time 3.1116 (2.7479)	Loss 0.28963 (0.34004)
2022-05-07 09:16:19,046 INFO 	Epoch: [56][60/309]	Batch time 2.5175 (2.7437)	Loss 0.38031 (0.34028)
2022-05-07 09:16:45,798 INFO 	Epoch: [56][70/309]	Batch time 2.7377 (2.7341)	Loss 0.35339 (0.34089)
2022-05-07 09:17:13,262 INFO 	Epoch: [56][80/309]	Batch time 2.6090 (2.7356)	Loss 0.37168 (0.34160)
2022-05-07 09:17:41,852 INFO 	Epoch: [56][90/309]	Batch time 2.7661 (2.7492)	Loss 0.34048 (0.34053)
2022-05-07 09:18:09,381 INFO 	Epoch: [56][100/309]	Batch time 2.9404 (2.7495)	Loss 0.34288 (0.34136)
2022-05-07 09:18:36,983 INFO 	Epoch: [56][110/309]	Batch time 2.5583 (2.7505)	Loss 0.34357 (0.34155)
2022-05-07 09:19:04,629 INFO 	Epoch: [56][120/309]	Batch time 2.6100 (2.7517)	Loss 0.32394 (0.34104)
2022-05-07 09:19:32,900 INFO 	Epoch: [56][130/309]	Batch time 2.6469 (2.7574)	Loss 0.35593 (0.34026)
2022-05-07 09:19:59,634 INFO 	Epoch: [56][140/309]	Batch time 2.5337 (2.7515)	Loss 0.34541 (0.34087)
2022-05-07 09:20:25,878 INFO 	Epoch: [56][150/309]	Batch time 2.6913 (2.7430)	Loss 0.32636 (0.34047)
2022-05-07 09:20:52,166 INFO 	Epoch: [56][160/309]	Batch time 2.5078 (2.7359)	Loss 0.33001 (0.33992)
2022-05-07 09:21:17,861 INFO 	Epoch: [56][170/309]	Batch time 2.4819 (2.7262)	Loss 0.35601 (0.34047)
2022-05-07 09:21:44,661 INFO 	Epoch: [56][180/309]	Batch time 2.6907 (2.7237)	Loss 0.29942 (0.34083)
2022-05-07 09:22:10,875 INFO 	Epoch: [56][190/309]	Batch time 2.6414 (2.7183)	Loss 0.36042 (0.34150)
2022-05-07 09:22:37,410 INFO 	Epoch: [56][200/309]	Batch time 2.8618 (2.7151)	Loss 0.28125 (0.34120)
2022-05-07 09:23:04,897 INFO 	Epoch: [56][210/309]	Batch time 2.5521 (2.7167)	Loss 0.39724 (0.34136)
2022-05-07 09:23:31,696 INFO 	Epoch: [56][220/309]	Batch time 2.2907 (2.7150)	Loss 0.32442 (0.34042)
2022-05-07 09:23:58,237 INFO 	Epoch: [56][230/309]	Batch time 2.7847 (2.7124)	Loss 0.35219 (0.34041)
2022-05-07 09:24:24,300 INFO 	Epoch: [56][240/309]	Batch time 2.6390 (2.7080)	Loss 0.30450 (0.34058)
2022-05-07 09:24:52,095 INFO 	Epoch: [56][250/309]	Batch time 2.5543 (2.7108)	Loss 0.37523 (0.34088)
2022-05-07 09:25:21,029 INFO 	Epoch: [56][260/309]	Batch time 3.0303 (2.7178)	Loss 0.31394 (0.34050)
2022-05-07 09:25:47,196 INFO 	Epoch: [56][270/309]	Batch time 2.7240 (2.7141)	Loss 0.38795 (0.34080)
2022-05-07 09:26:13,902 INFO 	Epoch: [56][280/309]	Batch time 2.7974 (2.7125)	Loss 0.29642 (0.34097)
2022-05-07 09:26:40,297 INFO 	Epoch: [56][290/309]	Batch time 2.4267 (2.7100)	Loss 0.36746 (0.34111)
2022-05-07 09:27:06,078 INFO 	Epoch: [56][300/309]	Batch time 2.7596 (2.7056)	Loss 0.31166 (0.34131)

Learning rate: 0.0005997154417587641
Step num: 17613

2022-05-07 09:27:30,193 INFO 	
Validation Loss 0.35619 (0.33484)

Warning! Reached max decoder steps
save mel done
2022-05-07 09:27:41,981 INFO 	Epoch: [57][0/309]	Batch time 4.5859 (4.5859)	Loss 0.32918 (0.32918)
2022-05-07 09:28:09,311 INFO 	Epoch: [57][10/309]	Batch time 3.1744 (2.9015)	Loss 0.30560 (0.32771)
2022-05-07 09:28:37,015 INFO 	Epoch: [57][20/309]	Batch time 3.0331 (2.8390)	Loss 0.34406 (0.33429)
2022-05-07 09:29:04,610 INFO 	Epoch: [57][30/309]	Batch time 2.6406 (2.8134)	Loss 0.34655 (0.33610)
2022-05-07 09:29:29,696 INFO 	Epoch: [57][40/309]	Batch time 2.4937 (2.7390)	Loss 0.37998 (0.34024)
2022-05-07 09:29:57,875 INFO 	Epoch: [57][50/309]	Batch time 2.6237 (2.7545)	Loss 0.33948 (0.33828)
2022-05-07 09:30:25,440 INFO 	Epoch: [57][60/309]	Batch time 2.4384 (2.7548)	Loss 0.38651 (0.33776)
2022-05-07 09:30:51,118 INFO 	Epoch: [57][70/309]	Batch time 2.8306 (2.7285)	Loss 0.31735 (0.33894)
2022-05-07 09:31:19,376 INFO 	Epoch: [57][80/309]	Batch time 2.5884 (2.7405)	Loss 0.32842 (0.33766)
2022-05-07 09:31:46,520 INFO 	Epoch: [57][90/309]	Batch time 3.0463 (2.7376)	Loss 0.29451 (0.33661)
2022-05-07 09:32:13,561 INFO 	Epoch: [57][100/309]	Batch time 2.7175 (2.7343)	Loss 0.32087 (0.33685)
2022-05-07 09:32:40,678 INFO 	Epoch: [57][110/309]	Batch time 2.5197 (2.7323)	Loss 0.36574 (0.33760)
2022-05-07 09:33:08,419 INFO 	Epoch: [57][120/309]	Batch time 2.7098 (2.7357)	Loss 0.34425 (0.33757)
2022-05-07 09:33:36,386 INFO 	Epoch: [57][130/309]	Batch time 2.9663 (2.7404)	Loss 0.32633 (0.33697)
2022-05-07 09:34:04,029 INFO 	Epoch: [57][140/309]	Batch time 2.5877 (2.7421)	Loss 0.38599 (0.33728)
2022-05-07 09:34:31,323 INFO 	Epoch: [57][150/309]	Batch time 2.8443 (2.7412)	Loss 0.31221 (0.33728)
2022-05-07 09:34:58,723 INFO 	Epoch: [57][160/309]	Batch time 2.4708 (2.7412)	Loss 0.35384 (0.33712)
2022-05-07 09:35:25,496 INFO 	Epoch: [57][170/309]	Batch time 2.4750 (2.7374)	Loss 0.34608 (0.33726)
2022-05-07 09:35:52,317 INFO 	Epoch: [57][180/309]	Batch time 2.4224 (2.7344)	Loss 0.39212 (0.33790)
2022-05-07 09:36:19,113 INFO 	Epoch: [57][190/309]	Batch time 2.5237 (2.7315)	Loss 0.37901 (0.33855)
2022-05-07 09:36:45,833 INFO 	Epoch: [57][200/309]	Batch time 2.8331 (2.7285)	Loss 0.34216 (0.33940)
2022-05-07 09:37:11,820 INFO 	Epoch: [57][210/309]	Batch time 2.8638 (2.7224)	Loss 0.34259 (0.34025)
2022-05-07 09:37:38,982 INFO 	Epoch: [57][220/309]	Batch time 2.8878 (2.7221)	Loss 0.33069 (0.34037)
2022-05-07 09:38:05,789 INFO 	Epoch: [57][230/309]	Batch time 2.6449 (2.7203)	Loss 0.35441 (0.34052)
2022-05-07 09:38:31,660 INFO 	Epoch: [57][240/309]	Batch time 2.5835 (2.7148)	Loss 0.37679 (0.34117)
2022-05-07 09:38:58,685 INFO 	Epoch: [57][250/309]	Batch time 3.0018 (2.7143)	Loss 0.29178 (0.34143)
2022-05-07 09:39:25,078 INFO 	Epoch: [57][260/309]	Batch time 2.8544 (2.7114)	Loss 0.32509 (0.34173)
2022-05-07 09:39:50,915 INFO 	Epoch: [57][270/309]	Batch time 2.7440 (2.7067)	Loss 0.33208 (0.34188)
2022-05-07 09:40:16,863 INFO 	Epoch: [57][280/309]	Batch time 2.4818 (2.7027)	Loss 0.35449 (0.34174)
2022-05-07 09:40:42,668 INFO 	Epoch: [57][290/309]	Batch time 2.6312 (2.6985)	Loss 0.34071 (0.34150)
2022-05-07 09:41:10,163 INFO 	Epoch: [57][300/309]	Batch time 2.7799 (2.7002)	Loss 0.34773 (0.34130)

Learning rate: 0.0005814676154359223
Step num: 17922

2022-05-07 09:41:34,370 INFO 	
Validation Loss 0.35697 (0.33611)


Epochs since last improvement: 1

Warning! Reached max decoder steps
save mel done
2022-05-07 09:41:44,062 INFO 	Epoch: [58][0/309]	Batch time 4.6788 (4.6788)	Loss 0.34820 (0.34820)
2022-05-07 09:42:10,607 INFO 	Epoch: [58][10/309]	Batch time 2.3089 (2.8385)	Loss 0.39428 (0.33939)
2022-05-07 09:42:37,196 INFO 	Epoch: [58][20/309]	Batch time 2.7575 (2.7530)	Loss 0.35669 (0.34614)
2022-05-07 09:43:04,469 INFO 	Epoch: [58][30/309]	Batch time 2.6934 (2.7447)	Loss 0.33022 (0.34282)
2022-05-07 09:43:32,124 INFO 	Epoch: [58][40/309]	Batch time 2.5984 (2.7498)	Loss 0.37023 (0.33994)
2022-05-07 09:44:00,098 INFO 	Epoch: [58][50/309]	Batch time 3.0970 (2.7591)	Loss 0.32198 (0.33666)
2022-05-07 09:44:26,887 INFO 	Epoch: [58][60/309]	Batch time 2.6958 (2.7460)	Loss 0.33582 (0.33734)
2022-05-07 09:44:54,372 INFO 	Epoch: [58][70/309]	Batch time 2.4850 (2.7463)	Loss 0.35292 (0.33685)
2022-05-07 09:45:20,161 INFO 	Epoch: [58][80/309]	Batch time 2.4087 (2.7256)	Loss 0.38821 (0.33988)
2022-05-07 09:45:47,030 INFO 	Epoch: [58][90/309]	Batch time 2.7408 (2.7214)	Loss 0.34590 (0.33992)
2022-05-07 09:46:14,611 INFO 	Epoch: [58][100/309]	Batch time 2.7017 (2.7250)	Loss 0.36210 (0.33992)
2022-05-07 09:46:41,999 INFO 	Epoch: [58][110/309]	Batch time 3.0809 (2.7263)	Loss 0.28389 (0.33993)
2022-05-07 09:47:09,520 INFO 	Epoch: [58][120/309]	Batch time 2.9315 (2.7284)	Loss 0.30345 (0.33932)
2022-05-07 09:47:34,823 INFO 	Epoch: [58][130/309]	Batch time 2.4393 (2.7133)	Loss 0.39871 (0.34036)
2022-05-07 09:48:01,969 INFO 	Epoch: [58][140/309]	Batch time 2.9509 (2.7134)	Loss 0.30680 (0.33970)
2022-05-07 09:48:29,494 INFO 	Epoch: [58][150/309]	Batch time 2.9929 (2.7160)	Loss 0.30863 (0.33949)
2022-05-07 09:48:55,727 INFO 	Epoch: [58][160/309]	Batch time 2.5466 (2.7102)	Loss 0.37346 (0.34033)
2022-05-07 09:49:23,482 INFO 	Epoch: [58][170/309]	Batch time 2.9915 (2.7140)	Loss 0.28308 (0.33981)
2022-05-07 09:49:49,988 INFO 	Epoch: [58][180/309]	Batch time 2.5414 (2.7105)	Loss 0.37859 (0.33941)
2022-05-07 09:50:16,665 INFO 	Epoch: [58][190/309]	Batch time 2.7085 (2.7083)	Loss 0.34747 (0.34005)
2022-05-07 09:50:44,152 INFO 	Epoch: [58][200/309]	Batch time 2.6948 (2.7103)	Loss 0.32409 (0.33951)
2022-05-07 09:51:11,030 INFO 	Epoch: [58][210/309]	Batch time 2.6773 (2.7092)	Loss 0.31171 (0.33950)
2022-05-07 09:51:37,373 INFO 	Epoch: [58][220/309]	Batch time 2.5941 (2.7058)	Loss 0.34465 (0.33983)
2022-05-07 09:52:04,188 INFO 	Epoch: [58][230/309]	Batch time 2.4959 (2.7048)	Loss 0.35961 (0.33937)
2022-05-07 09:52:30,970 INFO 	Epoch: [58][240/309]	Batch time 2.7236 (2.7037)	Loss 0.34887 (0.33962)
2022-05-07 09:52:57,388 INFO 	Epoch: [58][250/309]	Batch time 2.5085 (2.7012)	Loss 0.32273 (0.33964)
2022-05-07 09:53:23,975 INFO 	Epoch: [58][260/309]	Batch time 2.4555 (2.6996)	Loss 0.37927 (0.33952)
2022-05-07 09:53:50,864 INFO 	Epoch: [58][270/309]	Batch time 2.5894 (2.6992)	Loss 0.33260 (0.33954)
2022-05-07 09:54:16,995 INFO 	Epoch: [58][280/309]	Batch time 2.5447 (2.6961)	Loss 0.33092 (0.33943)
2022-05-07 09:54:43,478 INFO 	Epoch: [58][290/309]	Batch time 2.6060 (2.6945)	Loss 0.34581 (0.33966)
2022-05-07 09:55:08,899 INFO 	Epoch: [58][300/309]	Batch time 2.1690 (2.6894)	Loss 0.39455 (0.34026)

Learning rate: 0.0005637750243835482
Step num: 18231

2022-05-07 09:55:34,448 INFO 	
Validation Loss 0.35309 (0.33200)

save mel done
2022-05-07 09:55:46,578 INFO 	Epoch: [59][0/309]	Batch time 4.5039 (4.5039)	Loss 0.37414 (0.37414)
2022-05-07 09:56:12,930 INFO 	Epoch: [59][10/309]	Batch time 2.7469 (2.8050)	Loss 0.31323 (0.34537)
2022-05-07 09:56:39,727 INFO 	Epoch: [59][20/309]	Batch time 2.7019 (2.7454)	Loss 0.33017 (0.34076)
2022-05-07 09:57:05,635 INFO 	Epoch: [59][30/309]	Batch time 2.4200 (2.6955)	Loss 0.37607 (0.34212)
2022-05-07 09:57:32,758 INFO 	Epoch: [59][40/309]	Batch time 2.6258 (2.6996)	Loss 0.32864 (0.34046)
2022-05-07 09:57:59,467 INFO 	Epoch: [59][50/309]	Batch time 2.9826 (2.6940)	Loss 0.30198 (0.33963)
2022-05-07 09:58:26,208 INFO 	Epoch: [59][60/309]	Batch time 3.0298 (2.6907)	Loss 0.31448 (0.33918)
2022-05-07 09:58:54,263 INFO 	Epoch: [59][70/309]	Batch time 2.6597 (2.7069)	Loss 0.34285 (0.33697)
2022-05-07 09:59:20,687 INFO 	Epoch: [59][80/309]	Batch time 3.1610 (2.6989)	Loss 0.26156 (0.33633)
2022-05-07 09:59:46,547 INFO 	Epoch: [59][90/309]	Batch time 2.5283 (2.6865)	Loss 0.37127 (0.33736)
2022-05-07 10:00:12,840 INFO 	Epoch: [59][100/309]	Batch time 2.9773 (2.6808)	Loss 0.28482 (0.33775)
2022-05-07 10:00:39,419 INFO 	Epoch: [59][110/309]	Batch time 2.5563 (2.6788)	Loss 0.31336 (0.33840)
2022-05-07 10:01:05,091 INFO 	Epoch: [59][120/309]	Batch time 2.6143 (2.6696)	Loss 0.30495 (0.33936)
2022-05-07 10:01:31,678 INFO 	Epoch: [59][130/309]	Batch time 2.3683 (2.6687)	Loss 0.33075 (0.33916)
2022-05-07 10:01:57,572 INFO 	Epoch: [59][140/309]	Batch time 2.5144 (2.6631)	Loss 0.34596 (0.33898)
2022-05-07 10:02:25,813 INFO 	Epoch: [59][150/309]	Batch time 2.5946 (2.6738)	Loss 0.35791 (0.33870)
2022-05-07 10:02:54,163 INFO 	Epoch: [59][160/309]	Batch time 3.0573 (2.6838)	Loss 0.29329 (0.33841)
2022-05-07 10:03:20,925 INFO 	Epoch: [59][170/309]	Batch time 2.6951 (2.6833)	Loss 0.29661 (0.33832)
2022-05-07 10:03:48,082 INFO 	Epoch: [59][180/309]	Batch time 2.5513 (2.6851)	Loss 0.35976 (0.33816)
2022-05-07 10:04:14,767 INFO 	Epoch: [59][190/309]	Batch time 2.8725 (2.6843)	Loss 0.32953 (0.33814)
2022-05-07 10:04:41,253 INFO 	Epoch: [59][200/309]	Batch time 2.5844 (2.6825)	Loss 0.36688 (0.33839)
2022-05-07 10:05:08,432 INFO 	Epoch: [59][210/309]	Batch time 2.6750 (2.6842)	Loss 0.36658 (0.33863)
2022-05-07 10:05:35,777 INFO 	Epoch: [59][220/309]	Batch time 2.5935 (2.6864)	Loss 0.36087 (0.33856)
2022-05-07 10:06:01,751 INFO 	Epoch: [59][230/309]	Batch time 3.1243 (2.6826)	Loss 0.26470 (0.33865)
2022-05-07 10:06:27,430 INFO 	Epoch: [59][240/309]	Batch time 2.4795 (2.6778)	Loss 0.35429 (0.33848)
2022-05-07 10:06:53,482 INFO 	Epoch: [59][250/309]	Batch time 2.7367 (2.6749)	Loss 0.33281 (0.33886)
2022-05-07 10:07:18,808 INFO 	Epoch: [59][260/309]	Batch time 2.6443 (2.6695)	Loss 0.33862 (0.33873)
2022-05-07 10:07:44,976 INFO 	Epoch: [59][270/309]	Batch time 2.7302 (2.6675)	Loss 0.30694 (0.33852)
2022-05-07 10:08:10,946 INFO 	Epoch: [59][280/309]	Batch time 2.5309 (2.6650)	Loss 0.36992 (0.33860)
2022-05-07 10:08:39,030 INFO 	Epoch: [59][290/309]	Batch time 2.6826 (2.6700)	Loss 0.31758 (0.33802)
2022-05-07 10:09:06,773 INFO 	Epoch: [59][300/309]	Batch time 2.5448 (2.6734)	Loss 0.37707 (0.33810)

Learning rate: 0.0005466207741945977
Step num: 18540

2022-05-07 10:09:32,198 INFO 	
Validation Loss 0.35153 (0.33013)

save mel done
2022-05-07 10:09:43,733 INFO 	Epoch: [60][0/309]	Batch time 4.3051 (4.3051)	Loss 0.37294 (0.37294)
2022-05-07 10:10:10,626 INFO 	Epoch: [60][10/309]	Batch time 2.3970 (2.8361)	Loss 0.35473 (0.33635)
2022-05-07 10:10:36,392 INFO 	Epoch: [60][20/309]	Batch time 2.5510 (2.7125)	Loss 0.31328 (0.33696)
2022-05-07 10:11:03,075 INFO 	Epoch: [60][30/309]	Batch time 2.9500 (2.6983)	Loss 0.30726 (0.33422)
2022-05-07 10:11:29,586 INFO 	Epoch: [60][40/309]	Batch time 2.7447 (2.6868)	Loss 0.34612 (0.33721)
2022-05-07 10:11:56,982 INFO 	Epoch: [60][50/309]	Batch time 2.6741 (2.6971)	Loss 0.35406 (0.33805)
2022-05-07 10:12:23,553 INFO 	Epoch: [60][60/309]	Batch time 2.6978 (2.6906)	Loss 0.32285 (0.34183)
2022-05-07 10:12:53,117 INFO 	Epoch: [60][70/309]	Batch time 2.7119 (2.7280)	Loss 0.35438 (0.33826)
2022-05-07 10:13:22,533 INFO 	Epoch: [60][80/309]	Batch time 2.9099 (2.7544)	Loss 0.35860 (0.34022)
2022-05-07 10:13:53,549 INFO 	Epoch: [60][90/309]	Batch time 2.9484 (2.7925)	Loss 0.34433 (0.33986)
2022-05-07 10:14:24,066 INFO 	Epoch: [60][100/309]	Batch time 3.1200 (2.8182)	Loss 0.31638 (0.34085)
2022-05-07 10:14:55,060 INFO 	Epoch: [60][110/309]	Batch time 3.0055 (2.8435)	Loss 0.34642 (0.34111)
2022-05-07 10:15:25,395 INFO 	Epoch: [60][120/309]	Batch time 2.8211 (2.8592)	Loss 0.36362 (0.34083)
2022-05-07 10:16:00,391 INFO 	Epoch: [60][130/309]	Batch time 3.2589 (2.9081)	Loss 0.37796 (0.34029)
2022-05-07 10:16:34,767 INFO 	Epoch: [60][140/309]	Batch time 3.5014 (2.9457)	Loss 0.31345 (0.34105)
2022-05-07 10:17:10,512 INFO 	Epoch: [60][150/309]	Batch time 3.8299 (2.9873)	Loss 0.30585 (0.33952)
2022-05-07 10:17:45,333 INFO 	Epoch: [60][160/309]	Batch time 3.4140 (3.0180)	Loss 0.32200 (0.33904)
2022-05-07 10:18:21,013 INFO 	Epoch: [60][170/309]	Batch time 3.6207 (3.0502)	Loss 0.31970 (0.33841)
2022-05-07 10:18:57,654 INFO 	Epoch: [60][180/309]	Batch time 4.0074 (3.0841)	Loss 0.30603 (0.33780)
2022-05-07 10:19:33,591 INFO 	Epoch: [60][190/309]	Batch time 3.4302 (3.1108)	Loss 0.34015 (0.33785)
2022-05-07 10:20:09,925 INFO 	Epoch: [60][200/309]	Batch time 3.4465 (3.1368)	Loss 0.34357 (0.33734)
2022-05-07 10:20:46,208 INFO 	Epoch: [60][210/309]	Batch time 3.5323 (3.1601)	Loss 0.33313 (0.33690)
2022-05-07 10:21:21,846 INFO 	Epoch: [60][220/309]	Batch time 3.5175 (3.1784)	Loss 0.33867 (0.33692)
2022-05-07 10:21:58,846 INFO 	Epoch: [60][230/309]	Batch time 3.4975 (3.2009)	Loss 0.34822 (0.33661)
2022-05-07 10:22:36,113 INFO 	Epoch: [60][240/309]	Batch time 3.9258 (3.2228)	Loss 0.33370 (0.33647)
2022-05-07 10:23:13,080 INFO 	Epoch: [60][250/309]	Batch time 3.9030 (3.2416)	Loss 0.31040 (0.33646)
2022-05-07 10:23:49,262 INFO 	Epoch: [60][260/309]	Batch time 3.5834 (3.2561)	Loss 0.35153 (0.33677)
2022-05-07 10:24:26,704 INFO 	Epoch: [60][270/309]	Batch time 3.6727 (3.2741)	Loss 0.29878 (0.33662)
2022-05-07 10:25:04,732 INFO 	Epoch: [60][280/309]	Batch time 3.5488 (3.2929)	Loss 0.36420 (0.33677)
2022-05-07 10:25:41,358 INFO 	Epoch: [60][290/309]	Batch time 3.4326 (3.3056)	Loss 0.35603 (0.33656)
2022-05-07 10:26:16,396 INFO 	Epoch: [60][300/309]	Batch time 3.3684 (3.3122)	Loss 0.36251 (0.33708)

Learning rate: 0.000529988484516166
Step num: 18849

2022-05-07 10:26:49,866 INFO 	
Validation Loss 0.35444 (0.33340)


Epochs since last improvement: 1

Warning! Reached max decoder steps
save mel done
2022-05-07 10:27:01,440 INFO 	Epoch: [61][0/309]	Batch time 5.8743 (5.8743)	Loss 0.38348 (0.38348)
2022-05-07 10:27:38,325 INFO 	Epoch: [61][10/309]	Batch time 3.8423 (3.8872)	Loss 0.32827 (0.35326)
2022-05-07 10:28:16,576 INFO 	Epoch: [61][20/309]	Batch time 3.8332 (3.8576)	Loss 0.34916 (0.33929)
2022-05-07 10:28:53,910 INFO 	Epoch: [61][30/309]	Batch time 3.4599 (3.8176)	Loss 0.37990 (0.34073)
2022-05-07 10:29:32,572 INFO 	Epoch: [61][40/309]	Batch time 3.9641 (3.8294)	Loss 0.34385 (0.33822)
2022-05-07 10:30:09,907 INFO 	Epoch: [61][50/309]	Batch time 3.4836 (3.8106)	Loss 0.36722 (0.33853)
2022-05-07 10:30:47,265 INFO 	Epoch: [61][60/309]	Batch time 3.4068 (3.7983)	Loss 0.37912 (0.33910)
2022-05-07 10:31:23,716 INFO 	Epoch: [61][70/309]	Batch time 3.6515 (3.7768)	Loss 0.28791 (0.33807)
2022-05-07 10:31:57,745 INFO 	Epoch: [61][80/309]	Batch time 3.2790 (3.7306)	Loss 0.32524 (0.33824)
2022-05-07 10:32:33,191 INFO 	Epoch: [61][90/309]	Batch time 3.4324 (3.7102)	Loss 0.38247 (0.33767)
2022-05-07 10:33:08,290 INFO 	Epoch: [61][100/309]	Batch time 3.5152 (3.6903)	Loss 0.39372 (0.33732)
2022-05-07 10:33:45,723 INFO 	Epoch: [61][110/309]	Batch time 3.9839 (3.6951)	Loss 0.31289 (0.33641)
2022-05-07 10:34:22,539 INFO 	Epoch: [61][120/309]	Batch time 4.0813 (3.6940)	Loss 0.29673 (0.33594)
2022-05-07 10:34:53,549 INFO 	Epoch: [61][130/309]	Batch time 3.5408 (3.6487)	Loss 0.26985 (0.33504)
2022-05-07 10:35:24,985 INFO 	Epoch: [61][140/309]	Batch time 3.4281 (3.6129)	Loss 0.28619 (0.33409)
2022-05-07 10:35:54,913 INFO 	Epoch: [61][150/309]	Batch time 3.6743 (3.5718)	Loss 0.31180 (0.33452)
2022-05-07 10:36:24,989 INFO 	Epoch: [61][160/309]	Batch time 2.8665 (3.5368)	Loss 0.37289 (0.33543)
2022-05-07 10:36:54,919 INFO 	Epoch: [61][170/309]	Batch time 2.8980 (3.5050)	Loss 0.33601 (0.33541)
2022-05-07 10:37:23,531 INFO 	Epoch: [61][180/309]	Batch time 3.1998 (3.4694)	Loss 0.28667 (0.33566)
2022-05-07 10:37:51,700 INFO 	Epoch: [61][190/309]	Batch time 2.7417 (3.4353)	Loss 0.37341 (0.33665)
2022-05-07 10:38:23,737 INFO 	Epoch: [61][200/309]	Batch time 3.2592 (3.4237)	Loss 0.34437 (0.33632)
2022-05-07 10:38:55,557 INFO 	Epoch: [61][210/309]	Batch time 3.0947 (3.4123)	Loss 0.34615 (0.33628)
2022-05-07 10:39:25,326 INFO 	Epoch: [61][220/309]	Batch time 2.6238 (3.3926)	Loss 0.32900 (0.33617)
2022-05-07 10:39:55,107 INFO 	Epoch: [61][230/309]	Batch time 3.2396 (3.3746)	Loss 0.36212 (0.33620)
2022-05-07 10:40:24,775 INFO 	Epoch: [61][240/309]	Batch time 3.0774 (3.3577)	Loss 0.36317 (0.33643)
2022-05-07 10:40:54,837 INFO 	Epoch: [61][250/309]	Batch time 2.8379 (3.3437)	Loss 0.35008 (0.33589)
2022-05-07 10:41:23,113 INFO 	Epoch: [61][260/309]	Batch time 2.5667 (3.3239)	Loss 0.40862 (0.33626)
2022-05-07 10:41:51,975 INFO 	Epoch: [61][270/309]	Batch time 3.3436 (3.3078)	Loss 0.33653 (0.33657)
2022-05-07 10:42:19,266 INFO 	Epoch: [61][280/309]	Batch time 2.8361 (3.2872)	Loss 0.36152 (0.33697)
2022-05-07 10:42:48,164 INFO 	Epoch: [61][290/309]	Batch time 2.9436 (3.2735)	Loss 0.29185 (0.33654)
2022-05-07 10:43:17,046 INFO 	Epoch: [61][300/309]	Batch time 2.8047 (3.2607)	Loss 0.32096 (0.33629)

Learning rate: 0.0005138622734081196
Step num: 19158

2022-05-07 10:43:42,651 INFO 	
Validation Loss 0.34978 (0.32822)

Warning! Reached max decoder steps
save mel done
2022-05-07 10:43:55,818 INFO 	Epoch: [62][0/309]	Batch time 5.4831 (5.4831)	Loss 0.36438 (0.36438)
2022-05-07 10:44:29,164 INFO 	Epoch: [62][10/309]	Batch time 3.2268 (3.5299)	Loss 0.35401 (0.33152)
2022-05-07 10:45:03,283 INFO 	Epoch: [62][20/309]	Batch time 3.3525 (3.4737)	Loss 0.30398 (0.33345)
2022-05-07 10:45:37,119 INFO 	Epoch: [62][30/309]	Batch time 3.4241 (3.4446)	Loss 0.33394 (0.33607)
2022-05-07 10:46:11,237 INFO 	Epoch: [62][40/309]	Batch time 3.1344 (3.4366)	Loss 0.33942 (0.33477)
2022-05-07 10:46:43,913 INFO 	Epoch: [62][50/309]	Batch time 3.0921 (3.4035)	Loss 0.30804 (0.33622)
2022-05-07 10:47:15,835 INFO 	Epoch: [62][60/309]	Batch time 3.1024 (3.3689)	Loss 0.35458 (0.33690)
2022-05-07 10:47:47,287 INFO 	Epoch: [62][70/309]	Batch time 3.2463 (3.3374)	Loss 0.35723 (0.33730)
2022-05-07 10:48:22,716 INFO 	Epoch: [62][80/309]	Batch time 3.5338 (3.3627)	Loss 0.35154 (0.33808)
2022-05-07 10:48:59,212 INFO 	Epoch: [62][90/309]	Batch time 4.0329 (3.3942)	Loss 0.31587 (0.33738)
2022-05-07 10:49:37,445 INFO 	Epoch: [62][100/309]	Batch time 3.5580 (3.4367)	Loss 0.37888 (0.33737)
2022-05-07 10:50:14,711 INFO 	Epoch: [62][110/309]	Batch time 3.3564 (3.4628)	Loss 0.36651 (0.33727)
2022-05-07 10:50:52,356 INFO 	Epoch: [62][120/309]	Batch time 3.8009 (3.4878)	Loss 0.32692 (0.33645)
2022-05-07 10:51:30,150 INFO 	Epoch: [62][130/309]	Batch time 3.9850 (3.5100)	Loss 0.32541 (0.33659)
2022-05-07 10:52:06,924 INFO 	Epoch: [62][140/309]	Batch time 3.8644 (3.5219)	Loss 0.34419 (0.33702)
2022-05-07 10:52:45,225 INFO 	Epoch: [62][150/309]	Batch time 4.0800 (3.5423)	Loss 0.30425 (0.33635)
2022-05-07 10:53:21,784 INFO 	Epoch: [62][160/309]	Batch time 3.4689 (3.5494)	Loss 0.37900 (0.33699)
2022-05-07 10:54:00,919 INFO 	Epoch: [62][170/309]	Batch time 3.7476 (3.5707)	Loss 0.31087 (0.33622)
2022-05-07 10:54:41,109 INFO 	Epoch: [62][180/309]	Batch time 3.8434 (3.5954)	Loss 0.32065 (0.33551)
2022-05-07 10:55:18,435 INFO 	Epoch: [62][190/309]	Batch time 3.4323 (3.6026)	Loss 0.34756 (0.33561)
2022-05-07 10:55:54,080 INFO 	Epoch: [62][200/309]	Batch time 4.3196 (3.6007)	Loss 0.26571 (0.33554)
2022-05-07 10:56:28,163 INFO 	Epoch: [62][210/309]	Batch time 3.6808 (3.5916)	Loss 0.27653 (0.33477)
2022-05-07 10:57:00,370 INFO 	Epoch: [62][220/309]	Batch time 3.0417 (3.5748)	Loss 0.35295 (0.33453)
2022-05-07 10:57:31,701 INFO 	Epoch: [62][230/309]	Batch time 3.1525 (3.5557)	Loss 0.37265 (0.33499)
2022-05-07 10:58:04,713 INFO 	Epoch: [62][240/309]	Batch time 3.4443 (3.5451)	Loss 0.35321 (0.33474)
2022-05-07 10:58:37,676 INFO 	Epoch: [62][250/309]	Batch time 3.7267 (3.5352)	Loss 0.31177 (0.33441)
2022-05-07 10:59:15,360 INFO 	Epoch: [62][260/309]	Batch time 3.6094 (3.5442)	Loss 0.35197 (0.33465)
2022-05-07 10:59:52,690 INFO 	Epoch: [62][270/309]	Batch time 4.5187 (3.5511)	Loss 0.27740 (0.33458)
2022-05-07 11:00:28,801 INFO 	Epoch: [62][280/309]	Batch time 3.7433 (3.5533)	Loss 0.33658 (0.33493)
2022-05-07 11:01:06,144 INFO 	Epoch: [62][290/309]	Batch time 4.0073 (3.5595)	Loss 0.32489 (0.33490)
2022-05-07 11:01:45,728 INFO 	Epoch: [62][300/309]	Batch time 3.7966 (3.5727)	Loss 0.34355 (0.33460)

Learning rate: 0.0004982267421776534
Step num: 19467

2022-05-07 11:02:23,053 INFO 	
Validation Loss 0.35225 (0.33004)


Epochs since last improvement: 1

Warning! Reached max decoder steps
save mel done
2022-05-07 11:02:34,984 INFO 	Epoch: [63][0/309]	Batch time 6.3827 (6.3827)	Loss 0.38981 (0.38981)
2022-05-07 11:03:12,786 INFO 	Epoch: [63][10/309]	Batch time 3.6622 (4.0168)	Loss 0.32276 (0.33616)
2022-05-07 11:03:51,683 INFO 	Epoch: [63][20/309]	Batch time 3.6656 (3.9563)	Loss 0.35922 (0.33662)
2022-05-07 11:04:30,441 INFO 	Epoch: [63][30/309]	Batch time 4.1385 (3.9303)	Loss 0.32878 (0.33327)
2022-05-07 11:05:10,922 INFO 	Epoch: [63][40/309]	Batch time 4.0404 (3.9590)	Loss 0.33415 (0.33216)
2022-05-07 11:05:52,199 INFO 	Epoch: [63][50/309]	Batch time 4.1195 (3.9921)	Loss 0.31820 (0.33061)
2022-05-07 11:06:32,391 INFO 	Epoch: [63][60/309]	Batch time 4.1304 (3.9966)	Loss 0.36060 (0.33442)
2022-05-07 11:07:12,493 INFO 	Epoch: [63][70/309]	Batch time 4.4945 (3.9985)	Loss 0.29920 (0.33403)
2022-05-07 11:07:52,497 INFO 	Epoch: [63][80/309]	Batch time 3.9491 (3.9987)	Loss 0.33999 (0.33394)
2022-05-07 11:08:32,473 INFO 	Epoch: [63][90/309]	Batch time 3.8346 (3.9986)	Loss 0.37127 (0.33465)
2022-05-07 11:09:11,485 INFO 	Epoch: [63][100/309]	Batch time 3.6538 (3.9889)	Loss 0.35602 (0.33484)
2022-05-07 11:09:51,154 INFO 	Epoch: [63][110/309]	Batch time 3.4654 (3.9870)	Loss 0.31284 (0.33342)
2022-05-07 11:10:22,953 INFO 	Epoch: [63][120/309]	Batch time 3.2353 (3.9203)	Loss 0.37263 (0.33370)
2022-05-07 11:10:53,654 INFO 	Epoch: [63][130/309]	Batch time 3.1814 (3.8554)	Loss 0.37133 (0.33502)
2022-05-07 11:11:25,916 INFO 	Epoch: [63][140/309]	Batch time 3.2726 (3.8107)	Loss 0.34281 (0.33526)
2022-05-07 11:11:56,960 INFO 	Epoch: [63][150/309]	Batch time 2.6244 (3.7640)	Loss 0.37179 (0.33533)
2022-05-07 11:12:29,798 INFO 	Epoch: [63][160/309]	Batch time 3.1149 (3.7341)	Loss 0.32405 (0.33511)
2022-05-07 11:13:04,378 INFO 	Epoch: [63][170/309]	Batch time 3.5624 (3.7180)	Loss 0.31313 (0.33476)
2022-05-07 11:13:34,692 INFO 	Epoch: [63][180/309]	Batch time 3.2709 (3.6801)	Loss 0.30782 (0.33538)
2022-05-07 11:14:05,155 INFO 	Epoch: [63][190/309]	Batch time 2.9191 (3.6469)	Loss 0.30719 (0.33469)
2022-05-07 11:14:32,949 INFO 	Epoch: [63][200/309]	Batch time 2.8507 (3.6037)	Loss 0.32181 (0.33472)
2022-05-07 11:15:02,404 INFO 	Epoch: [63][210/309]	Batch time 3.0970 (3.5725)	Loss 0.31713 (0.33431)
2022-05-07 11:15:32,103 INFO 	Epoch: [63][220/309]	Batch time 3.1390 (3.5453)	Loss 0.32549 (0.33393)
2022-05-07 11:15:59,437 INFO 	Epoch: [63][230/309]	Batch time 2.5552 (3.5101)	Loss 0.33265 (0.33402)
2022-05-07 11:16:27,945 INFO 	Epoch: [63][240/309]	Batch time 2.7511 (3.4828)	Loss 0.34491 (0.33394)
2022-05-07 11:16:55,783 INFO 	Epoch: [63][250/309]	Batch time 2.6723 (3.4549)	Loss 0.33862 (0.33352)
2022-05-07 11:17:23,434 INFO 	Epoch: [63][260/309]	Batch time 2.9856 (3.4285)	Loss 0.32677 (0.33349)
2022-05-07 11:17:51,655 INFO 	Epoch: [63][270/309]	Batch time 3.1414 (3.4061)	Loss 0.31774 (0.33334)
2022-05-07 11:18:18,448 INFO 	Epoch: [63][280/309]	Batch time 2.6053 (3.3802)	Loss 0.34306 (0.33325)
2022-05-07 11:18:44,648 INFO 	Epoch: [63][290/309]	Batch time 2.8466 (3.3541)	Loss 0.32437 (0.33360)
2022-05-07 11:19:11,337 INFO 	Epoch: [63][300/309]	Batch time 3.2506 (3.3313)	Loss 0.26758 (0.33381)

Learning rate: 0.0004830669606752952
Step num: 19776

2022-05-07 11:19:36,373 INFO 	
Validation Loss 0.35222 (0.32838)


Epochs since last improvement: 2

Warning! Reached max decoder steps
save mel done
2022-05-07 11:19:45,880 INFO 	Epoch: [64][0/309]	Batch time 4.7335 (4.7335)	Loss 0.32604 (0.32604)
2022-05-07 11:20:12,614 INFO 	Epoch: [64][10/309]	Batch time 2.5963 (2.8606)	Loss 0.33538 (0.32792)
2022-05-07 11:20:39,218 INFO 	Epoch: [64][20/309]	Batch time 2.5375 (2.7653)	Loss 0.36557 (0.33812)
2022-05-07 11:21:05,456 INFO 	Epoch: [64][30/309]	Batch time 2.6793 (2.7197)	Loss 0.31880 (0.33621)
2022-05-07 11:21:32,210 INFO 	Epoch: [64][40/309]	Batch time 2.0372 (2.7089)	Loss 0.37240 (0.33311)
2022-05-07 11:22:00,231 INFO 	Epoch: [64][50/309]	Batch time 2.8239 (2.7272)	Loss 0.36892 (0.33265)
2022-05-07 11:22:28,349 INFO 	Epoch: [64][60/309]	Batch time 2.9661 (2.7410)	Loss 0.32350 (0.33111)
2022-05-07 11:22:54,732 INFO 	Epoch: [64][70/309]	Batch time 2.3650 (2.7266)	Loss 0.36910 (0.33213)
2022-05-07 11:23:20,314 INFO 	Epoch: [64][80/309]	Batch time 2.5440 (2.7058)	Loss 0.31530 (0.33336)
2022-05-07 11:23:46,836 INFO 	Epoch: [64][90/309]	Batch time 2.4367 (2.6999)	Loss 0.30976 (0.33286)
2022-05-07 11:24:12,655 INFO 	Epoch: [64][100/309]	Batch time 2.2770 (2.6882)	Loss 0.37600 (0.33436)
2022-05-07 11:24:37,910 INFO 	Epoch: [64][110/309]	Batch time 2.4107 (2.6735)	Loss 0.38511 (0.33442)
2022-05-07 11:25:06,416 INFO 	Epoch: [64][120/309]	Batch time 3.1091 (2.6882)	Loss 0.26883 (0.33314)
2022-05-07 11:25:33,412 INFO 	Epoch: [64][130/309]	Batch time 2.5700 (2.6891)	Loss 0.35156 (0.33338)
2022-05-07 11:26:00,036 INFO 	Epoch: [64][140/309]	Batch time 2.5270 (2.6872)	Loss 0.33060 (0.33328)
2022-05-07 11:26:26,579 INFO 	Epoch: [64][150/309]	Batch time 2.7763 (2.6850)	Loss 0.33564 (0.33335)
2022-05-07 11:26:52,923 INFO 	Epoch: [64][160/309]	Batch time 2.8382 (2.6818)	Loss 0.32199 (0.33305)
2022-05-07 11:27:20,209 INFO 	Epoch: [64][170/309]	Batch time 2.6666 (2.6846)	Loss 0.34355 (0.33302)
2022-05-07 11:27:46,262 INFO 	Epoch: [64][180/309]	Batch time 2.4533 (2.6802)	Loss 0.35654 (0.33333)
2022-05-07 11:28:12,165 INFO 	Epoch: [64][190/309]	Batch time 2.8737 (2.6755)	Loss 0.32341 (0.33349)
2022-05-07 11:28:39,821 INFO 	Epoch: [64][200/309]	Batch time 2.4740 (2.6800)	Loss 0.35921 (0.33316)
2022-05-07 11:29:05,945 INFO 	Epoch: [64][210/309]	Batch time 2.6140 (2.6768)	Loss 0.30104 (0.33285)
2022-05-07 11:29:32,447 INFO 	Epoch: [64][220/309]	Batch time 2.4580 (2.6756)	Loss 0.31390 (0.33224)
2022-05-07 11:30:00,616 INFO 	Epoch: [64][230/309]	Batch time 2.6061 (2.6817)	Loss 0.34078 (0.33160)
2022-05-07 11:30:27,228 INFO 	Epoch: [64][240/309]	Batch time 2.6432 (2.6808)	Loss 0.35098 (0.33128)
2022-05-07 11:30:51,762 INFO 	Epoch: [64][250/309]	Batch time 2.5072 (2.6718)	Loss 0.36434 (0.33237)
2022-05-07 11:31:17,081 INFO 	Epoch: [64][260/309]	Batch time 3.0356 (2.6664)	Loss 0.26429 (0.33265)
2022-05-07 11:31:42,212 INFO 	Epoch: [64][270/309]	Batch time 2.4238 (2.6608)	Loss 0.35139 (0.33302)
2022-05-07 11:32:08,481 INFO 	Epoch: [64][280/309]	Batch time 2.3527 (2.6596)	Loss 0.36337 (0.33344)
2022-05-07 11:32:34,100 INFO 	Epoch: [64][290/309]	Batch time 2.5469 (2.6562)	Loss 0.31859 (0.33347)
2022-05-07 11:33:01,714 INFO 	Epoch: [64][300/309]	Batch time 2.6558 (2.6597)	Loss 0.33346 (0.33329)

Learning rate: 0.0004683684530383155
Step num: 20085

2022-05-07 11:33:25,386 INFO 	
Validation Loss 0.34880 (0.32653)

Warning! Reached max decoder steps
save mel done
2022-05-07 11:33:37,402 INFO 	Epoch: [65][0/309]	Batch time 4.3857 (4.3857)	Loss 0.36057 (0.36057)
2022-05-07 11:34:03,356 INFO 	Epoch: [65][10/309]	Batch time 2.9488 (2.7582)	Loss 0.27824 (0.32850)
2022-05-07 11:34:29,164 INFO 	Epoch: [65][20/309]	Batch time 2.8567 (2.6737)	Loss 0.30077 (0.32868)
2022-05-07 11:34:54,677 INFO 	Epoch: [65][30/309]	Batch time 2.3376 (2.6342)	Loss 0.37143 (0.33383)
2022-05-07 11:35:20,270 INFO 	Epoch: [65][40/309]	Batch time 2.8502 (2.6159)	Loss 0.31224 (0.33325)
2022-05-07 11:35:46,915 INFO 	Epoch: [65][50/309]	Batch time 2.6036 (2.6255)	Loss 0.32062 (0.33325)
2022-05-07 11:36:12,192 INFO 	Epoch: [65][60/309]	Batch time 2.4719 (2.6094)	Loss 0.31962 (0.33459)
2022-05-07 11:36:38,181 INFO 	Epoch: [65][70/309]	Batch time 2.5432 (2.6080)	Loss 0.29312 (0.33398)
2022-05-07 11:37:05,210 INFO 	Epoch: [65][80/309]	Batch time 2.7492 (2.6197)	Loss 0.29438 (0.33165)
2022-05-07 11:37:32,039 INFO 	Epoch: [65][90/309]	Batch time 3.1284 (2.6266)	Loss 0.30001 (0.33142)
2022-05-07 11:37:58,856 INFO 	Epoch: [65][100/309]	Batch time 2.6004 (2.6321)	Loss 0.35445 (0.33135)
2022-05-07 11:38:24,406 INFO 	Epoch: [65][110/309]	Batch time 2.5623 (2.6251)	Loss 0.34788 (0.33175)
2022-05-07 11:38:49,907 INFO 	Epoch: [65][120/309]	Batch time 2.4457 (2.6189)	Loss 0.34755 (0.33218)
2022-05-07 11:39:16,277 INFO 	Epoch: [65][130/309]	Batch time 2.4855 (2.6203)	Loss 0.36126 (0.33231)
2022-05-07 11:39:41,529 INFO 	Epoch: [65][140/309]	Batch time 2.6321 (2.6136)	Loss 0.29423 (0.33183)
2022-05-07 11:40:06,757 INFO 	Epoch: [65][150/309]	Batch time 2.5130 (2.6076)	Loss 0.35218 (0.33212)
2022-05-07 11:40:32,534 INFO 	Epoch: [65][160/309]	Batch time 2.5299 (2.6057)	Loss 0.35179 (0.33220)
2022-05-07 11:40:57,769 INFO 	Epoch: [65][170/309]	Batch time 2.5039 (2.6009)	Loss 0.34171 (0.33234)
2022-05-07 11:41:23,887 INFO 	Epoch: [65][180/309]	Batch time 2.4074 (2.6015)	Loss 0.33863 (0.33189)
2022-05-07 11:41:49,739 INFO 	Epoch: [65][190/309]	Batch time 2.3744 (2.6006)	Loss 0.36494 (0.33192)
2022-05-07 11:42:15,246 INFO 	Epoch: [65][200/309]	Batch time 2.3739 (2.5982)	Loss 0.36968 (0.33249)
2022-05-07 11:42:42,093 INFO 	Epoch: [65][210/309]	Batch time 2.6503 (2.6023)	Loss 0.35494 (0.33271)
2022-05-07 11:43:08,494 INFO 	Epoch: [65][220/309]	Batch time 2.6280 (2.6040)	Loss 0.30217 (0.33226)
2022-05-07 11:43:34,271 INFO 	Epoch: [65][230/309]	Batch time 2.5618 (2.6028)	Loss 0.28229 (0.33186)
2022-05-07 11:44:00,178 INFO 	Epoch: [65][240/309]	Batch time 2.8143 (2.6023)	Loss 0.29762 (0.33120)
2022-05-07 11:44:26,883 INFO 	Epoch: [65][250/309]	Batch time 2.9884 (2.6050)	Loss 0.29398 (0.33100)
2022-05-07 11:44:53,214 INFO 	Epoch: [65][260/309]	Batch time 2.4121 (2.6061)	Loss 0.36147 (0.33098)
2022-05-07 11:45:18,314 INFO 	Epoch: [65][270/309]	Batch time 2.5017 (2.6026)	Loss 0.33046 (0.33187)
2022-05-07 11:45:44,684 INFO 	Epoch: [65][280/309]	Batch time 2.9269 (2.6038)	Loss 0.30476 (0.33218)
2022-05-07 11:46:11,600 INFO 	Epoch: [65][290/309]	Batch time 2.7444 (2.6068)	Loss 0.35282 (0.33188)
2022-05-07 11:46:37,534 INFO 	Epoch: [65][300/309]	Batch time 2.4151 (2.6064)	Loss 0.31366 (0.33193)

Learning rate: 0.00045411718386792927
Step num: 20394

2022-05-07 11:47:01,515 INFO 	
Validation Loss 0.35168 (0.32853)


Epochs since last improvement: 1

save mel done
2022-05-07 11:47:10,646 INFO 	Epoch: [66][0/309]	Batch time 4.1024 (4.1024)	Loss 0.34658 (0.34658)
2022-05-07 11:47:36,385 INFO 	Epoch: [66][10/309]	Batch time 2.5324 (2.7129)	Loss 0.36477 (0.33980)
2022-05-07 11:48:01,950 INFO 	Epoch: [66][20/309]	Batch time 2.6008 (2.6384)	Loss 0.36489 (0.34152)
2022-05-07 11:48:27,098 INFO 	Epoch: [66][30/309]	Batch time 2.6762 (2.5985)	Loss 0.32667 (0.34136)
2022-05-07 11:48:52,795 INFO 	Epoch: [66][40/309]	Batch time 2.4872 (2.5915)	Loss 0.33511 (0.33803)
2022-05-07 11:49:18,972 INFO 	Epoch: [66][50/309]	Batch time 2.8274 (2.5966)	Loss 0.31384 (0.33564)
2022-05-07 11:49:44,839 INFO 	Epoch: [66][60/309]	Batch time 2.6272 (2.5950)	Loss 0.34526 (0.33480)
2022-05-07 11:50:11,183 INFO 	Epoch: [66][70/309]	Batch time 2.5372 (2.6006)	Loss 0.35619 (0.33378)
2022-05-07 11:50:37,012 INFO 	Epoch: [66][80/309]	Batch time 2.8704 (2.5984)	Loss 0.29193 (0.33291)
2022-05-07 11:51:02,978 INFO 	Epoch: [66][90/309]	Batch time 2.6071 (2.5982)	Loss 0.31741 (0.33307)
2022-05-07 11:51:28,287 INFO 	Epoch: [66][100/309]	Batch time 2.5974 (2.5915)	Loss 0.32003 (0.33208)
2022-05-07 11:51:54,678 INFO 	Epoch: [66][110/309]	Batch time 2.8502 (2.5958)	Loss 0.28748 (0.33150)
2022-05-07 11:52:21,525 INFO 	Epoch: [66][120/309]	Batch time 2.7031 (2.6032)	Loss 0.35308 (0.33128)
2022-05-07 11:52:47,039 INFO 	Epoch: [66][130/309]	Batch time 2.6196 (2.5992)	Loss 0.33377 (0.33254)
2022-05-07 11:53:14,268 INFO 	Epoch: [66][140/309]	Batch time 2.4357 (2.6080)	Loss 0.36430 (0.33198)
2022-05-07 11:53:40,583 INFO 	Epoch: [66][150/309]	Batch time 2.3265 (2.6095)	Loss 0.33460 (0.33132)
2022-05-07 11:54:06,148 INFO 	Epoch: [66][160/309]	Batch time 2.4667 (2.6062)	Loss 0.34882 (0.33167)
2022-05-07 11:54:31,501 INFO 	Epoch: [66][170/309]	Batch time 2.6016 (2.6021)	Loss 0.34509 (0.33183)
2022-05-07 11:54:55,373 INFO 	Epoch: [66][180/309]	Batch time 2.4232 (2.5902)	Loss 0.36371 (0.33238)
2022-05-07 11:55:21,356 INFO 	Epoch: [66][190/309]	Batch time 2.5768 (2.5906)	Loss 0.35650 (0.33223)
2022-05-07 11:55:47,885 INFO 	Epoch: [66][200/309]	Batch time 2.5314 (2.5937)	Loss 0.33701 (0.33190)
2022-05-07 11:56:13,510 INFO 	Epoch: [66][210/309]	Batch time 2.5805 (2.5923)	Loss 0.34453 (0.33175)
2022-05-07 11:56:38,619 INFO 	Epoch: [66][220/309]	Batch time 2.3729 (2.5886)	Loss 0.36040 (0.33168)
2022-05-07 11:57:05,280 INFO 	Epoch: [66][230/309]	Batch time 2.8181 (2.5919)	Loss 0.33976 (0.33150)
2022-05-07 11:57:30,775 INFO 	Epoch: [66][240/309]	Batch time 2.6162 (2.5902)	Loss 0.35006 (0.33193)
2022-05-07 11:57:57,773 INFO 	Epoch: [66][250/309]	Batch time 2.3549 (2.5945)	Loss 0.37287 (0.33151)
2022-05-07 11:58:24,108 INFO 	Epoch: [66][260/309]	Batch time 2.3555 (2.5960)	Loss 0.37424 (0.33157)
2022-05-07 11:58:50,830 INFO 	Epoch: [66][270/309]	Batch time 2.7738 (2.5988)	Loss 0.30600 (0.33110)
2022-05-07 11:59:18,241 INFO 	Epoch: [66][280/309]	Batch time 2.6548 (2.6039)	Loss 0.32084 (0.33059)
2022-05-07 11:59:44,436 INFO 	Epoch: [66][290/309]	Batch time 2.8005 (2.6044)	Loss 0.31249 (0.33031)
2022-05-07 12:00:10,812 INFO 	Epoch: [66][300/309]	Batch time 2.4964 (2.6055)	Loss 0.35456 (0.33024)

Learning rate: 0.00044029954482709013
Step num: 20703

2022-05-07 12:00:35,270 INFO 	
Validation Loss 0.34769 (0.32534)

save mel done
2022-05-07 12:00:46,517 INFO 	Epoch: [67][0/309]	Batch time 4.1558 (4.1558)	Loss 0.34457 (0.34457)
2022-05-07 12:01:11,393 INFO 	Epoch: [67][10/309]	Batch time 2.7089 (2.6393)	Loss 0.33019 (0.33415)
2022-05-07 12:01:36,889 INFO 	Epoch: [67][20/309]	Batch time 2.9076 (2.5966)	Loss 0.32792 (0.33902)
2022-05-07 12:02:02,334 INFO 	Epoch: [67][30/309]	Batch time 2.3850 (2.5798)	Loss 0.36326 (0.33877)
2022-05-07 12:02:27,102 INFO 	Epoch: [67][40/309]	Batch time 2.6615 (2.5547)	Loss 0.32612 (0.33612)
2022-05-07 12:02:52,605 INFO 	Epoch: [67][50/309]	Batch time 2.6542 (2.5538)	Loss 0.33320 (0.33849)
2022-05-07 12:03:18,781 INFO 	Epoch: [67][60/309]	Batch time 2.4225 (2.5643)	Loss 0.36169 (0.33827)
2022-05-07 12:03:45,519 INFO 	Epoch: [67][70/309]	Batch time 2.7827 (2.5797)	Loss 0.30470 (0.33687)
2022-05-07 12:04:11,841 INFO 	Epoch: [67][80/309]	Batch time 3.1165 (2.5862)	Loss 0.25727 (0.33558)
2022-05-07 12:04:38,147 INFO 	Epoch: [67][90/309]	Batch time 2.5858 (2.5911)	Loss 0.32558 (0.33467)
2022-05-07 12:05:04,064 INFO 	Epoch: [67][100/309]	Batch time 2.5041 (2.5911)	Loss 0.34221 (0.33421)
2022-05-07 12:05:29,371 INFO 	Epoch: [67][110/309]	Batch time 2.3526 (2.5857)	Loss 0.36390 (0.33443)
2022-05-07 12:05:54,777 INFO 	Epoch: [67][120/309]	Batch time 2.4596 (2.5819)	Loss 0.33252 (0.33479)
2022-05-07 12:06:20,626 INFO 	Epoch: [67][130/309]	Batch time 2.4726 (2.5822)	Loss 0.32364 (0.33447)
2022-05-07 12:06:46,816 INFO 	Epoch: [67][140/309]	Batch time 2.6124 (2.5848)	Loss 0.34338 (0.33415)
2022-05-07 12:07:11,978 INFO 	Epoch: [67][150/309]	Batch time 2.4972 (2.5802)	Loss 0.33226 (0.33437)
2022-05-07 12:07:36,647 INFO 	Epoch: [67][160/309]	Batch time 2.3859 (2.5732)	Loss 0.34690 (0.33477)
2022-05-07 12:08:02,524 INFO 	Epoch: [67][170/309]	Batch time 2.5678 (2.5741)	Loss 0.32686 (0.33461)
2022-05-07 12:08:28,491 INFO 	Epoch: [67][180/309]	Batch time 2.4194 (2.5753)	Loss 0.35123 (0.33439)
2022-05-07 12:08:54,135 INFO 	Epoch: [67][190/309]	Batch time 2.4988 (2.5747)	Loss 0.35293 (0.33432)
2022-05-07 12:09:21,012 INFO 	Epoch: [67][200/309]	Batch time 2.4206 (2.5804)	Loss 0.36541 (0.33409)
2022-05-07 12:09:46,754 INFO 	Epoch: [67][210/309]	Batch time 2.8612 (2.5801)	Loss 0.30582 (0.33385)
2022-05-07 12:10:11,955 INFO 	Epoch: [67][220/309]	Batch time 2.5369 (2.5773)	Loss 0.38881 (0.33423)
2022-05-07 12:10:38,257 INFO 	Epoch: [67][230/309]	Batch time 2.4279 (2.5796)	Loss 0.31095 (0.33349)
2022-05-07 12:11:04,100 INFO 	Epoch: [67][240/309]	Batch time 2.4597 (2.5798)	Loss 0.31936 (0.33290)
2022-05-07 12:11:29,312 INFO 	Epoch: [67][250/309]	Batch time 2.5168 (2.5775)	Loss 0.30287 (0.33264)
2022-05-07 12:11:55,159 INFO 	Epoch: [67][260/309]	Batch time 2.4964 (2.5778)	Loss 0.31778 (0.33259)
2022-05-07 12:12:22,107 INFO 	Epoch: [67][270/309]	Batch time 2.8266 (2.5821)	Loss 0.31169 (0.33276)
2022-05-07 12:12:49,462 INFO 	Epoch: [67][280/309]	Batch time 3.3466 (2.5875)	Loss 0.29793 (0.33212)
2022-05-07 12:13:16,406 INFO 	Epoch: [67][290/309]	Batch time 2.6605 (2.5912)	Loss 0.33950 (0.33179)
2022-05-07 12:13:42,814 INFO 	Epoch: [67][300/309]	Batch time 2.7282 (2.5929)	Loss 0.30742 (0.33172)

Learning rate: 0.0004269023416460806
Step num: 21012

2022-05-07 12:14:06,303 INFO 	
Validation Loss 0.34694 (0.32769)


Epochs since last improvement: 1

save mel done
2022-05-07 12:14:15,151 INFO 	Epoch: [68][0/309]	Batch time 4.2369 (4.2369)	Loss 0.34326 (0.34326)
2022-05-07 12:14:40,353 INFO 	Epoch: [68][10/309]	Batch time 2.5598 (2.6762)	Loss 0.31495 (0.33612)
2022-05-07 12:15:05,957 INFO 	Epoch: [68][20/309]	Batch time 2.2699 (2.6211)	Loss 0.33610 (0.33854)
2022-05-07 12:15:32,490 INFO 	Epoch: [68][30/309]	Batch time 2.8183 (2.6315)	Loss 0.30712 (0.33471)
2022-05-07 12:15:58,783 INFO 	Epoch: [68][40/309]	Batch time 2.7762 (2.6309)	Loss 0.32829 (0.33415)
2022-05-07 12:16:24,618 INFO 	Epoch: [68][50/309]	Batch time 2.2913 (2.6216)	Loss 0.36818 (0.33327)
2022-05-07 12:16:50,012 INFO 	Epoch: [68][60/309]	Batch time 2.4507 (2.6082)	Loss 0.32900 (0.33273)
2022-05-07 12:17:15,672 INFO 	Epoch: [68][70/309]	Batch time 2.4162 (2.6022)	Loss 0.35887 (0.33196)
2022-05-07 12:17:40,748 INFO 	Epoch: [68][80/309]	Batch time 2.6540 (2.5905)	Loss 0.34811 (0.33208)
2022-05-07 12:18:07,389 INFO 	Epoch: [68][90/309]	Batch time 2.3847 (2.5986)	Loss 0.36110 (0.33135)
2022-05-07 12:18:32,600 INFO 	Epoch: [68][100/309]	Batch time 2.6264 (2.5909)	Loss 0.28717 (0.33083)
2022-05-07 12:18:58,431 INFO 	Epoch: [68][110/309]	Batch time 2.3932 (2.5902)	Loss 0.35052 (0.33109)
2022-05-07 12:19:24,284 INFO 	Epoch: [68][120/309]	Batch time 2.5415 (2.5898)	Loss 0.29822 (0.33030)
2022-05-07 12:19:50,433 INFO 	Epoch: [68][130/309]	Batch time 2.7233 (2.5917)	Loss 0.30610 (0.32953)
2022-05-07 12:20:15,729 INFO 	Epoch: [68][140/309]	Batch time 2.5507 (2.5873)	Loss 0.34659 (0.33029)
2022-05-07 12:20:41,421 INFO 	Epoch: [68][150/309]	Batch time 2.4606 (2.5861)	Loss 0.34739 (0.33030)
2022-05-07 12:21:07,491 INFO 	Epoch: [68][160/309]	Batch time 2.4581 (2.5874)	Loss 0.36437 (0.33014)
2022-05-07 12:21:33,018 INFO 	Epoch: [68][170/309]	Batch time 2.6710 (2.5854)	Loss 0.35358 (0.33073)
2022-05-07 12:21:59,035 INFO 	Epoch: [68][180/309]	Batch time 2.5079 (2.5863)	Loss 0.35862 (0.33077)
2022-05-07 12:22:25,400 INFO 	Epoch: [68][190/309]	Batch time 2.3603 (2.5889)	Loss 0.34597 (0.33051)
2022-05-07 12:22:51,979 INFO 	Epoch: [68][200/309]	Batch time 2.6712 (2.5924)	Loss 0.32746 (0.33020)
2022-05-07 12:23:18,188 INFO 	Epoch: [68][210/309]	Batch time 2.5426 (2.5937)	Loss 0.33391 (0.33031)
2022-05-07 12:23:42,660 INFO 	Epoch: [68][220/309]	Batch time 2.6930 (2.5871)	Loss 0.28847 (0.33058)
2022-05-07 12:24:07,632 INFO 	Epoch: [68][230/309]	Batch time 2.4611 (2.5832)	Loss 0.33920 (0.33094)
2022-05-07 12:24:34,468 INFO 	Epoch: [68][240/309]	Batch time 3.2423 (2.5874)	Loss 0.28508 (0.33085)
2022-05-07 12:25:00,657 INFO 	Epoch: [68][250/309]	Batch time 2.6493 (2.5886)	Loss 0.31020 (0.33060)
2022-05-07 12:25:28,257 INFO 	Epoch: [68][260/309]	Batch time 2.5336 (2.5952)	Loss 0.33746 (0.33017)
2022-05-07 12:25:54,044 INFO 	Epoch: [68][270/309]	Batch time 2.6290 (2.5946)	Loss 0.33489 (0.33009)
2022-05-07 12:26:20,583 INFO 	Epoch: [68][280/309]	Batch time 2.5534 (2.5967)	Loss 0.31411 (0.32992)
2022-05-07 12:26:46,902 INFO 	Epoch: [68][290/309]	Batch time 2.6293 (2.5979)	Loss 0.32410 (0.32991)
2022-05-07 12:27:14,060 INFO 	Epoch: [68][300/309]	Batch time 2.9275 (2.6018)	Loss 0.28751 (0.32957)

Learning rate: 0.0004139127815234889
Step num: 21321

2022-05-07 12:27:39,596 INFO 	
Validation Loss 0.34567 (0.32417)

save mel done
2022-05-07 12:27:50,567 INFO 	Epoch: [69][0/309]	Batch time 3.7813 (3.7813)	Loss 0.39880 (0.39880)
2022-05-07 12:28:15,967 INFO 	Epoch: [69][10/309]	Batch time 2.7702 (2.6528)	Loss 0.26922 (0.33340)
2022-05-07 12:28:41,550 INFO 	Epoch: [69][20/309]	Batch time 2.5492 (2.6078)	Loss 0.34315 (0.33063)
2022-05-07 12:29:06,583 INFO 	Epoch: [69][30/309]	Batch time 2.4917 (2.5741)	Loss 0.33760 (0.33080)
2022-05-07 12:29:31,884 INFO 	Epoch: [69][40/309]	Batch time 2.6670 (2.5634)	Loss 0.32508 (0.32927)
2022-05-07 12:29:59,085 INFO 	Epoch: [69][50/309]	Batch time 2.3920 (2.5941)	Loss 0.32414 (0.32631)
2022-05-07 12:30:26,675 INFO 	Epoch: [69][60/309]	Batch time 2.5421 (2.6211)	Loss 0.33244 (0.32443)
2022-05-07 12:30:52,345 INFO 	Epoch: [69][70/309]	Batch time 2.4999 (2.6135)	Loss 0.33819 (0.32525)
2022-05-07 12:31:17,636 INFO 	Epoch: [69][80/309]	Batch time 2.5596 (2.6031)	Loss 0.29234 (0.32639)
2022-05-07 12:31:43,633 INFO 	Epoch: [69][90/309]	Batch time 2.9861 (2.6027)	Loss 0.30641 (0.32677)
2022-05-07 12:32:10,129 INFO 	Epoch: [69][100/309]	Batch time 2.7673 (2.6074)	Loss 0.32677 (0.32638)
2022-05-07 12:32:36,939 INFO 	Epoch: [69][110/309]	Batch time 2.9199 (2.6140)	Loss 0.30076 (0.32582)
2022-05-07 12:33:02,632 INFO 	Epoch: [69][120/309]	Batch time 2.6352 (2.6103)	Loss 0.36596 (0.33005)
2022-05-07 12:33:28,672 INFO 	Epoch: [69][130/309]	Batch time 2.3738 (2.6098)	Loss 0.36633 (0.33198)
2022-05-07 12:33:55,645 INFO 	Epoch: [69][140/309]	Batch time 2.5376 (2.6160)	Loss 0.32914 (0.33278)
2022-05-07 12:34:21,222 INFO 	Epoch: [69][150/309]	Batch time 2.4761 (2.6122)	Loss 0.34033 (0.33317)
2022-05-07 12:34:47,454 INFO 	Epoch: [69][160/309]	Batch time 2.5246 (2.6128)	Loss 0.34930 (0.33341)
2022-05-07 12:35:12,666 INFO 	Epoch: [69][170/309]	Batch time 2.2240 (2.6075)	Loss 0.35214 (0.33346)
2022-05-07 12:35:38,249 INFO 	Epoch: [69][180/309]	Batch time 2.6690 (2.6048)	Loss 0.30468 (0.33365)
2022-05-07 12:36:04,494 INFO 	Epoch: [69][190/309]	Batch time 2.3467 (2.6058)	Loss 0.39584 (0.33415)
2022-05-07 12:36:30,123 INFO 	Epoch: [69][200/309]	Batch time 2.6204 (2.6037)	Loss 0.33383 (0.33400)
2022-05-07 12:36:55,187 INFO 	Epoch: [69][210/309]	Batch time 2.7677 (2.5991)	Loss 0.30800 (0.33448)
2022-05-07 12:37:22,062 INFO 	Epoch: [69][220/309]	Batch time 2.3919 (2.6031)	Loss 0.36424 (0.33426)
2022-05-07 12:37:47,268 INFO 	Epoch: [69][230/309]	Batch time 2.5775 (2.5995)	Loss 0.30716 (0.33403)
2022-05-07 12:38:13,918 INFO 	Epoch: [69][240/309]	Batch time 3.1459 (2.6022)	Loss 0.28634 (0.33393)
2022-05-07 12:38:39,811 INFO 	Epoch: [69][250/309]	Batch time 2.5474 (2.6017)	Loss 0.31374 (0.33388)
2022-05-07 12:39:05,303 INFO 	Epoch: [69][260/309]	Batch time 2.8439 (2.5997)	Loss 0.33249 (0.33397)
2022-05-07 12:39:31,683 INFO 	Epoch: [69][270/309]	Batch time 2.6268 (2.6011)	Loss 0.34647 (0.33412)
2022-05-07 12:39:57,940 INFO 	Epoch: [69][280/309]	Batch time 2.4037 (2.6020)	Loss 0.34173 (0.33395)
2022-05-07 12:40:23,373 INFO 	Epoch: [69][290/309]	Batch time 2.6337 (2.6000)	Loss 0.32189 (0.33395)
2022-05-07 12:40:49,869 INFO 	Epoch: [69][300/309]	Batch time 2.4977 (2.6016)	Loss 0.37899 (0.33388)

Learning rate: 0.0004013184609105419
Step num: 21630

2022-05-07 12:41:14,108 INFO 	
Validation Loss 0.34898 (0.32653)


Epochs since last improvement: 1

save mel done
2022-05-07 12:41:23,135 INFO 	Epoch: [70][0/309]	Batch time 4.3213 (4.3213)	Loss 0.34830 (0.34830)
2022-05-07 12:41:48,599 INFO 	Epoch: [70][10/309]	Batch time 3.0487 (2.7078)	Loss 0.27666 (0.34305)
2022-05-07 12:42:14,338 INFO 	Epoch: [70][20/309]	Batch time 3.0743 (2.6440)	Loss 0.27978 (0.33945)
2022-05-07 12:42:40,427 INFO 	Epoch: [70][30/309]	Batch time 2.9997 (2.6327)	Loss 0.29295 (0.33670)
2022-05-07 12:43:05,620 INFO 	Epoch: [70][40/309]	Batch time 2.6090 (2.6050)	Loss 0.32392 (0.33760)
2022-05-07 12:43:31,156 INFO 	Epoch: [70][50/309]	Batch time 2.5655 (2.5950)	Loss 0.31695 (0.33708)
2022-05-07 12:43:57,041 INFO 	Epoch: [70][60/309]	Batch time 2.5058 (2.5939)	Loss 0.34994 (0.33789)
2022-05-07 12:44:23,610 INFO 	Epoch: [70][70/309]	Batch time 2.7466 (2.6028)	Loss 0.32301 (0.33659)
2022-05-07 12:44:48,863 INFO 	Epoch: [70][80/309]	Batch time 2.5618 (2.5932)	Loss 0.31898 (0.33642)
2022-05-07 12:45:15,963 INFO 	Epoch: [70][90/309]	Batch time 2.4833 (2.6060)	Loss 0.34171 (0.33523)
2022-05-07 12:45:41,780 INFO 	Epoch: [70][100/309]	Batch time 2.3864 (2.6036)	Loss 0.36259 (0.33413)
2022-05-07 12:46:07,758 INFO 	Epoch: [70][110/309]	Batch time 2.9279 (2.6031)	Loss 0.33191 (0.33390)
2022-05-07 12:46:33,522 INFO 	Epoch: [70][120/309]	Batch time 2.6396 (2.6009)	Loss 0.35926 (0.33356)
2022-05-07 12:47:00,948 INFO 	Epoch: [70][130/309]	Batch time 2.8162 (2.6117)	Loss 0.30353 (0.33197)
2022-05-07 12:47:26,392 INFO 	Epoch: [70][140/309]	Batch time 2.6052 (2.6069)	Loss 0.35386 (0.33240)
2022-05-07 12:47:52,469 INFO 	Epoch: [70][150/309]	Batch time 2.4988 (2.6070)	Loss 0.34864 (0.33230)
2022-05-07 12:48:18,167 INFO 	Epoch: [70][160/309]	Batch time 2.3868 (2.6047)	Loss 0.35568 (0.33258)
2022-05-07 12:48:45,281 INFO 	Epoch: [70][170/309]	Batch time 2.5710 (2.6109)	Loss 0.32247 (0.33142)
2022-05-07 12:49:10,599 INFO 	Epoch: [70][180/309]	Batch time 2.5252 (2.6066)	Loss 0.34413 (0.33118)
2022-05-07 12:49:36,257 INFO 	Epoch: [70][190/309]	Batch time 2.6362 (2.6044)	Loss 0.33060 (0.33069)
2022-05-07 12:50:03,298 INFO 	Epoch: [70][200/309]	Batch time 2.7784 (2.6094)	Loss 0.31744 (0.33073)
2022-05-07 12:50:29,607 INFO 	Epoch: [70][210/309]	Batch time 2.2493 (2.6104)	Loss 0.36796 (0.33066)
2022-05-07 12:50:56,308 INFO 	Epoch: [70][220/309]	Batch time 2.5642 (2.6131)	Loss 0.29837 (0.33023)
2022-05-07 12:51:22,128 INFO 	Epoch: [70][230/309]	Batch time 2.5262 (2.6118)	Loss 0.32174 (0.33001)
2022-05-07 12:51:48,923 INFO 	Epoch: [70][240/309]	Batch time 2.6419 (2.6146)	Loss 0.33643 (0.32969)
2022-05-07 12:52:13,829 INFO 	Epoch: [70][250/309]	Batch time 2.6423 (2.6096)	Loss 0.32032 (0.32982)
2022-05-07 12:52:40,642 INFO 	Epoch: [70][260/309]	Batch time 2.7901 (2.6124)	Loss 0.29882 (0.32975)
2022-05-07 12:53:06,366 INFO 	Epoch: [70][270/309]	Batch time 2.6446 (2.6109)	Loss 0.32778 (0.32980)
2022-05-07 12:53:32,638 INFO 	Epoch: [70][280/309]	Batch time 2.3740 (2.6115)	Loss 0.35549 (0.32959)
2022-05-07 12:53:58,754 INFO 	Epoch: [70][290/309]	Batch time 2.3619 (2.6115)	Loss 0.37463 (0.32961)
2022-05-07 12:54:24,413 INFO 	Epoch: [70][300/309]	Batch time 2.5029 (2.6100)	Loss 0.31895 (0.32946)

Learning rate: 0.00038910735366713114
Step num: 21939

2022-05-07 12:54:48,943 INFO 	
Validation Loss 0.34703 (0.32536)


Epochs since last improvement: 2

Warning! Reached max decoder steps
save mel done
2022-05-07 12:54:58,032 INFO 	Epoch: [71][0/309]	Batch time 4.4542 (4.4542)	Loss 0.34324 (0.34324)
2022-05-07 12:55:22,995 INFO 	Epoch: [71][10/309]	Batch time 2.2404 (2.6744)	Loss 0.35368 (0.33851)
2022-05-07 12:55:47,823 INFO 	Epoch: [71][20/309]	Batch time 2.4414 (2.5831)	Loss 0.33646 (0.33583)
2022-05-07 12:56:14,344 INFO 	Epoch: [71][30/309]	Batch time 2.5204 (2.6054)	Loss 0.31877 (0.33178)
2022-05-07 12:56:41,686 INFO 	Epoch: [71][40/309]	Batch time 2.6475 (2.6368)	Loss 0.32706 (0.33144)
2022-05-07 12:57:08,046 INFO 	Epoch: [71][50/309]	Batch time 2.5707 (2.6366)	Loss 0.31569 (0.33033)
2022-05-07 12:57:34,863 INFO 	Epoch: [71][60/309]	Batch time 2.7192 (2.6440)	Loss 0.33380 (0.33031)
2022-05-07 12:58:01,286 INFO 	Epoch: [71][70/309]	Batch time 3.0832 (2.6438)	Loss 0.30085 (0.32863)
2022-05-07 12:58:27,424 INFO 	Epoch: [71][80/309]	Batch time 2.1869 (2.6401)	Loss 0.35678 (0.32804)
2022-05-07 12:58:55,125 INFO 	Epoch: [71][90/309]	Batch time 2.5802 (2.6544)	Loss 0.29885 (0.32673)
2022-05-07 12:59:21,495 INFO 	Epoch: [71][100/309]	Batch time 2.4618 (2.6526)	Loss 0.31714 (0.32615)
2022-05-07 12:59:46,453 INFO 	Epoch: [71][110/309]	Batch time 2.3160 (2.6385)	Loss 0.36115 (0.32746)
2022-05-07 13:00:11,463 INFO 	Epoch: [71][120/309]	Batch time 2.5318 (2.6272)	Loss 0.32151 (0.32740)
2022-05-07 13:00:36,628 INFO 	Epoch: [71][130/309]	Batch time 2.5845 (2.6187)	Loss 0.35171 (0.32774)
2022-05-07 13:01:03,564 INFO 	Epoch: [71][140/309]	Batch time 2.5801 (2.6240)	Loss 0.33927 (0.32688)
2022-05-07 13:01:29,202 INFO 	Epoch: [71][150/309]	Batch time 2.6168 (2.6200)	Loss 0.30222 (0.32710)
2022-05-07 13:01:55,403 INFO 	Epoch: [71][160/309]	Batch time 2.6877 (2.6200)	Loss 0.31433 (0.32721)
2022-05-07 13:02:20,694 INFO 	Epoch: [71][170/309]	Batch time 2.3665 (2.6147)	Loss 0.35132 (0.32751)
2022-05-07 13:02:47,497 INFO 	Epoch: [71][180/309]	Batch time 2.5629 (2.6183)	Loss 0.33938 (0.32759)
2022-05-07 13:03:13,501 INFO 	Epoch: [71][190/309]	Batch time 2.7104 (2.6174)	Loss 0.30218 (0.32780)
2022-05-07 13:03:39,315 INFO 	Epoch: [71][200/309]	Batch time 2.6303 (2.6156)	Loss 0.33912 (0.32807)
2022-05-07 13:04:04,719 INFO 	Epoch: [71][210/309]	Batch time 2.6238 (2.6120)	Loss 0.33917 (0.32832)
2022-05-07 13:04:30,494 INFO 	Epoch: [71][220/309]	Batch time 2.6014 (2.6105)	Loss 0.31806 (0.32836)
2022-05-07 13:04:56,086 INFO 	Epoch: [71][230/309]	Batch time 2.3634 (2.6083)	Loss 0.34686 (0.32847)
2022-05-07 13:05:22,078 INFO 	Epoch: [71][240/309]	Batch time 2.8017 (2.6079)	Loss 0.29975 (0.32850)
2022-05-07 13:05:48,432 INFO 	Epoch: [71][250/309]	Batch time 2.3893 (2.6090)	Loss 0.35459 (0.32829)
2022-05-07 13:06:14,953 INFO 	Epoch: [71][260/309]	Batch time 2.4985 (2.6106)	Loss 0.34367 (0.32808)
2022-05-07 13:06:41,080 INFO 	Epoch: [71][270/309]	Batch time 2.8721 (2.6107)	Loss 0.27985 (0.32820)
2022-05-07 13:07:07,828 INFO 	Epoch: [71][280/309]	Batch time 3.2832 (2.6130)	Loss 0.27373 (0.32812)
2022-05-07 13:07:34,174 INFO 	Epoch: [71][290/309]	Batch time 2.7087 (2.6137)	Loss 0.33865 (0.32846)
2022-05-07 13:08:00,418 INFO 	Epoch: [71][300/309]	Batch time 2.7998 (2.6141)	Loss 0.30012 (0.32833)

Learning rate: 0.00037726779957821953
Step num: 22248

2022-05-07 13:08:24,873 INFO 	
Validation Loss 0.34817 (0.32699)


Epochs since last improvement: 3

save mel done
2022-05-07 13:08:33,679 INFO 	Epoch: [72][0/309]	Batch time 4.1080 (4.1080)	Loss 0.32730 (0.32730)
2022-05-07 13:08:59,853 INFO 	Epoch: [72][10/309]	Batch time 2.4298 (2.7529)	Loss 0.34747 (0.32185)
2022-05-07 13:09:25,483 INFO 	Epoch: [72][20/309]	Batch time 2.4900 (2.6624)	Loss 0.34031 (0.32822)
2022-05-07 13:09:52,556 INFO 	Epoch: [72][30/309]	Batch time 2.6899 (2.6769)	Loss 0.33425 (0.32373)
2022-05-07 13:10:18,991 INFO 	Epoch: [72][40/309]	Batch time 2.6852 (2.6688)	Loss 0.32028 (0.32454)
2022-05-07 13:10:45,367 INFO 	Epoch: [72][50/309]	Batch time 2.4036 (2.6627)	Loss 0.33708 (0.32506)
2022-05-07 13:11:10,760 INFO 	Epoch: [72][60/309]	Batch time 2.5011 (2.6424)	Loss 0.34393 (0.32885)
2022-05-07 13:11:38,614 INFO 	Epoch: [72][70/309]	Batch time 2.8061 (2.6626)	Loss 0.30683 (0.32765)
2022-05-07 13:12:04,746 INFO 	Epoch: [72][80/309]	Batch time 2.6311 (2.6565)	Loss 0.32225 (0.32666)
2022-05-07 13:12:30,625 INFO 	Epoch: [72][90/309]	Batch time 2.3407 (2.6489)	Loss 0.36930 (0.32764)
2022-05-07 13:12:57,015 INFO 	Epoch: [72][100/309]	Batch time 2.7012 (2.6480)	Loss 0.29804 (0.32727)
2022-05-07 13:13:22,664 INFO 	Epoch: [72][110/309]	Batch time 2.6863 (2.6405)	Loss 0.30979 (0.32767)
2022-05-07 13:13:48,818 INFO 	Epoch: [72][120/309]	Batch time 2.7820 (2.6384)	Loss 0.34036 (0.32731)
2022-05-07 13:14:14,519 INFO 	Epoch: [72][130/309]	Batch time 2.4137 (2.6332)	Loss 0.32976 (0.32812)
2022-05-07 13:14:40,976 INFO 	Epoch: [72][140/309]	Batch time 2.4132 (2.6341)	Loss 0.35060 (0.32754)
2022-05-07 13:15:06,504 INFO 	Epoch: [72][150/309]	Batch time 2.4766 (2.6287)	Loss 0.34947 (0.32711)
2022-05-07 13:15:33,338 INFO 	Epoch: [72][160/309]	Batch time 2.3619 (2.6321)	Loss 0.32491 (0.32713)
2022-05-07 13:15:58,408 INFO 	Epoch: [72][170/309]	Batch time 2.4425 (2.6248)	Loss 0.32478 (0.32729)
2022-05-07 13:16:24,581 INFO 	Epoch: [72][180/309]	Batch time 2.6897 (2.6244)	Loss 0.31342 (0.32664)
2022-05-07 13:16:50,695 INFO 	Epoch: [72][190/309]	Batch time 2.4738 (2.6237)	Loss 0.33623 (0.32639)
2022-05-07 13:17:16,637 INFO 	Epoch: [72][200/309]	Batch time 2.5187 (2.6222)	Loss 0.32574 (0.32657)
2022-05-07 13:17:43,960 INFO 	Epoch: [72][210/309]	Batch time 2.8789 (2.6274)	Loss 0.29299 (0.32601)
2022-05-07 13:18:09,158 INFO 	Epoch: [72][220/309]	Batch time 2.5772 (2.6226)	Loss 0.31612 (0.32638)
2022-05-07 13:18:34,936 INFO 	Epoch: [72][230/309]	Batch time 2.5687 (2.6206)	Loss 0.29702 (0.32592)
2022-05-07 13:19:00,947 INFO 	Epoch: [72][240/309]	Batch time 2.5574 (2.6198)	Loss 0.31881 (0.32554)
2022-05-07 13:19:26,144 INFO 	Epoch: [72][250/309]	Batch time 2.5895 (2.6158)	Loss 0.34765 (0.32569)
2022-05-07 13:19:52,597 INFO 	Epoch: [72][260/309]	Batch time 2.5264 (2.6170)	Loss 0.32278 (0.32569)
2022-05-07 13:20:18,694 INFO 	Epoch: [72][270/309]	Batch time 2.8226 (2.6167)	Loss 0.30777 (0.32608)
2022-05-07 13:20:43,547 INFO 	Epoch: [72][280/309]	Batch time 2.4103 (2.6120)	Loss 0.31596 (0.32597)
2022-05-07 13:21:09,190 INFO 	Epoch: [72][290/309]	Batch time 2.6222 (2.6104)	Loss 0.34080 (0.32613)
2022-05-07 13:21:35,709 INFO 	Epoch: [72][300/309]	Batch time 2.3811 (2.6118)	Loss 0.33538 (0.32600)

Learning rate: 0.0003657884932196662
Step num: 22557

2022-05-07 13:21:59,181 INFO 	
Validation Loss 0.34874 (0.32418)


Epochs since last improvement: 4

save mel done
2022-05-07 13:22:08,580 INFO 	Epoch: [73][0/309]	Batch time 4.8256 (4.8256)	Loss 0.29643 (0.29643)
2022-05-07 13:22:35,089 INFO 	Epoch: [73][10/309]	Batch time 2.7625 (2.8487)	Loss 0.29464 (0.31735)
2022-05-07 13:23:02,225 INFO 	Epoch: [73][20/309]	Batch time 2.9227 (2.7843)	Loss 0.30066 (0.31786)
2022-05-07 13:23:29,225 INFO 	Epoch: [73][30/309]	Batch time 2.5591 (2.7571)	Loss 0.34754 (0.31902)
2022-05-07 13:23:54,710 INFO 	Epoch: [73][40/309]	Batch time 2.4951 (2.7062)	Loss 0.34305 (0.32200)
2022-05-07 13:24:22,045 INFO 	Epoch: [73][50/309]	Batch time 2.5538 (2.7116)	Loss 0.33243 (0.32143)
2022-05-07 13:24:48,233 INFO 	Epoch: [73][60/309]	Batch time 2.9007 (2.6964)	Loss 0.29635 (0.32265)
2022-05-07 13:25:12,817 INFO 	Epoch: [73][70/309]	Batch time 2.4648 (2.6628)	Loss 0.33873 (0.32509)
2022-05-07 13:25:39,045 INFO 	Epoch: [73][80/309]	Batch time 2.5340 (2.6579)	Loss 0.34669 (0.32576)
2022-05-07 13:26:06,794 INFO 	Epoch: [73][90/309]	Batch time 2.6672 (2.6708)	Loss 0.33235 (0.32401)
2022-05-07 13:26:33,810 INFO 	Epoch: [73][100/309]	Batch time 2.7508 (2.6738)	Loss 0.30493 (0.32488)
2022-05-07 13:27:00,544 INFO 	Epoch: [73][110/309]	Batch time 2.7429 (2.6738)	Loss 0.30221 (0.32543)
2022-05-07 13:27:26,339 INFO 	Epoch: [73][120/309]	Batch time 2.6597 (2.6660)	Loss 0.29947 (0.32516)
2022-05-07 13:27:52,575 INFO 	Epoch: [73][130/309]	Batch time 2.6637 (2.6628)	Loss 0.32984 (0.32543)
2022-05-07 13:28:18,395 INFO 	Epoch: [73][140/309]	Batch time 2.3528 (2.6570)	Loss 0.33047 (0.32551)
2022-05-07 13:28:44,512 INFO 	Epoch: [73][150/309]	Batch time 2.4077 (2.6540)	Loss 0.35994 (0.32550)
2022-05-07 13:29:10,161 INFO 	Epoch: [73][160/309]	Batch time 2.6357 (2.6485)	Loss 0.33342 (0.32577)
2022-05-07 13:29:37,302 INFO 	Epoch: [73][170/309]	Batch time 2.6981 (2.6523)	Loss 0.31900 (0.32484)
2022-05-07 13:30:03,390 INFO 	Epoch: [73][180/309]	Batch time 2.4422 (2.6499)	Loss 0.33719 (0.32490)
2022-05-07 13:30:28,526 INFO 	Epoch: [73][190/309]	Batch time 2.6331 (2.6428)	Loss 0.35058 (0.32565)
2022-05-07 13:30:55,267 INFO 	Epoch: [73][200/309]	Batch time 2.6926 (2.6443)	Loss 0.30666 (0.32540)
2022-05-07 13:31:21,250 INFO 	Epoch: [73][210/309]	Batch time 2.7481 (2.6422)	Loss 0.31057 (0.32516)
2022-05-07 13:31:46,325 INFO 	Epoch: [73][220/309]	Batch time 2.4046 (2.6361)	Loss 0.36157 (0.32611)
2022-05-07 13:32:11,305 INFO 	Epoch: [73][230/309]	Batch time 2.3865 (2.6301)	Loss 0.35563 (0.32663)
2022-05-07 13:32:39,121 INFO 	Epoch: [73][240/309]	Batch time 2.8927 (2.6364)	Loss 0.31756 (0.32616)
2022-05-07 13:33:05,969 INFO 	Epoch: [73][250/309]	Batch time 3.0334 (2.6383)	Loss 0.28828 (0.32576)
2022-05-07 13:33:32,423 INFO 	Epoch: [73][260/309]	Batch time 2.2433 (2.6386)	Loss 0.37808 (0.32614)
2022-05-07 13:33:58,717 INFO 	Epoch: [73][270/309]	Batch time 2.5805 (2.6382)	Loss 0.32369 (0.32608)
2022-05-07 13:34:25,240 INFO 	Epoch: [73][280/309]	Batch time 2.7649 (2.6387)	Loss 0.33538 (0.32585)
2022-05-07 13:34:52,433 INFO 	Epoch: [73][290/309]	Batch time 2.7502 (2.6415)	Loss 0.31263 (0.32539)
2022-05-07 13:35:19,245 INFO 	Epoch: [73][300/309]	Batch time 2.6778 (2.6428)	Loss 0.34859 (0.32562)

Learning rate: 0.00035465847316283497
Step num: 22866

2022-05-07 13:35:44,323 INFO 	
Validation Loss 0.34287 (0.32244)

save mel done
2022-05-07 13:35:55,531 INFO 	Epoch: [74][0/309]	Batch time 4.2283 (4.2283)	Loss 0.32530 (0.32530)
2022-05-07 13:36:22,305 INFO 	Epoch: [74][10/309]	Batch time 2.8371 (2.8184)	Loss 0.35169 (0.32558)
2022-05-07 13:36:47,965 INFO 	Epoch: [74][20/309]	Batch time 2.4081 (2.6982)	Loss 0.35777 (0.32322)
2022-05-07 13:37:13,773 INFO 	Epoch: [74][30/309]	Batch time 2.4752 (2.6603)	Loss 0.35014 (0.32380)
2022-05-07 13:37:39,733 INFO 	Epoch: [74][40/309]	Batch time 2.8526 (2.6447)	Loss 0.32753 (0.32375)
2022-05-07 13:38:05,361 INFO 	Epoch: [74][50/309]	Batch time 2.4754 (2.6286)	Loss 0.34177 (0.32576)
2022-05-07 13:38:30,563 INFO 	Epoch: [74][60/309]	Batch time 2.4540 (2.6108)	Loss 0.34584 (0.32588)
2022-05-07 13:38:57,070 INFO 	Epoch: [74][70/309]	Batch time 2.6282 (2.6164)	Loss 0.36191 (0.32596)
2022-05-07 13:39:23,526 INFO 	Epoch: [74][80/309]	Batch time 2.5389 (2.6200)	Loss 0.32919 (0.32666)
2022-05-07 13:39:48,869 INFO 	Epoch: [74][90/309]	Batch time 2.4228 (2.6106)	Loss 0.35513 (0.32777)
2022-05-07 13:40:14,133 INFO 	Epoch: [74][100/309]	Batch time 2.8608 (2.6023)	Loss 0.28928 (0.32776)
2022-05-07 13:40:40,048 INFO 	Epoch: [74][110/309]	Batch time 2.6103 (2.6013)	Loss 0.34160 (0.32748)
2022-05-07 13:41:06,682 INFO 	Epoch: [74][120/309]	Batch time 2.5169 (2.6064)	Loss 0.31045 (0.32639)
2022-05-07 13:41:33,478 INFO 	Epoch: [74][130/309]	Batch time 3.0826 (2.6120)	Loss 0.26914 (0.32578)
2022-05-07 13:41:58,350 INFO 	Epoch: [74][140/309]	Batch time 2.4238 (2.6032)	Loss 0.32497 (0.32663)
2022-05-07 13:42:24,009 INFO 	Epoch: [74][150/309]	Batch time 2.8421 (2.6007)	Loss 0.28462 (0.32586)
2022-05-07 13:42:50,399 INFO 	Epoch: [74][160/309]	Batch time 2.4069 (2.6031)	Loss 0.34275 (0.32556)
2022-05-07 13:43:15,205 INFO 	Epoch: [74][170/309]	Batch time 2.6345 (2.5959)	Loss 0.34124 (0.32650)
2022-05-07 13:43:41,480 INFO 	Epoch: [74][180/309]	Batch time 2.6687 (2.5977)	Loss 0.29262 (0.32552)
2022-05-07 13:44:08,428 INFO 	Epoch: [74][190/309]	Batch time 2.6896 (2.6027)	Loss 0.31886 (0.32541)
2022-05-07 13:44:34,037 INFO 	Epoch: [74][200/309]	Batch time 2.3335 (2.6007)	Loss 0.31709 (0.32519)
2022-05-07 13:44:59,148 INFO 	Epoch: [74][210/309]	Batch time 2.7042 (2.5964)	Loss 0.33267 (0.32556)
2022-05-07 13:45:24,891 INFO 	Epoch: [74][220/309]	Batch time 2.6396 (2.5954)	Loss 0.31120 (0.32572)
2022-05-07 13:45:50,741 INFO 	Epoch: [74][230/309]	Batch time 2.8946 (2.5950)	Loss 0.27090 (0.32559)
2022-05-07 13:46:15,506 INFO 	Epoch: [74][240/309]	Batch time 2.1971 (2.5901)	Loss 0.33454 (0.32588)
2022-05-07 13:46:41,136 INFO 	Epoch: [74][250/309]	Batch time 2.4250 (2.5890)	Loss 0.31499 (0.32594)
2022-05-07 13:47:06,870 INFO 	Epoch: [74][260/309]	Batch time 2.4940 (2.5884)	Loss 0.36810 (0.32615)
2022-05-07 13:47:33,613 INFO 	Epoch: [74][270/309]	Batch time 2.5892 (2.5916)	Loss 0.34013 (0.32560)
2022-05-07 13:47:59,167 INFO 	Epoch: [74][280/309]	Batch time 2.5534 (2.5903)	Loss 0.32286 (0.32569)
2022-05-07 13:48:25,907 INFO 	Epoch: [74][290/309]	Batch time 2.7088 (2.5931)	Loss 0.28455 (0.32531)
2022-05-07 13:48:51,714 INFO 	Epoch: [74][300/309]	Batch time 2.4931 (2.5927)	Loss 0.32689 (0.32565)

Learning rate: 0.00034386711150768024
Step num: 23175

2022-05-07 13:49:15,587 INFO 	
Validation Loss 0.34423 (0.32254)


Epochs since last improvement: 1

Warning! Reached max decoder steps
save mel done
2022-05-07 13:49:24,344 INFO 	Epoch: [75][0/309]	Batch time 4.4094 (4.4094)	Loss 0.32289 (0.32289)
2022-05-07 13:49:51,013 INFO 	Epoch: [75][10/309]	Batch time 2.7797 (2.8253)	Loss 0.32571 (0.32493)
2022-05-07 13:50:17,662 INFO 	Epoch: [75][20/309]	Batch time 2.8142 (2.7489)	Loss 0.31218 (0.32847)
2022-05-07 13:50:43,107 INFO 	Epoch: [75][30/309]	Batch time 2.3858 (2.6830)	Loss 0.36241 (0.33035)
2022-05-07 13:51:09,431 INFO 	Epoch: [75][40/309]	Batch time 2.8597 (2.6706)	Loss 0.29947 (0.32775)
2022-05-07 13:51:37,037 INFO 	Epoch: [75][50/309]	Batch time 2.8873 (2.6883)	Loss 0.29786 (0.32507)
2022-05-07 13:52:04,097 INFO 	Epoch: [75][60/309]	Batch time 2.6015 (2.6912)	Loss 0.31743 (0.32430)
2022-05-07 13:52:32,261 INFO 	Epoch: [75][70/309]	Batch time 2.8092 (2.7088)	Loss 0.30291 (0.32278)
2022-05-07 13:52:58,415 INFO 	Epoch: [75][80/309]	Batch time 2.4434 (2.6973)	Loss 0.31019 (0.32285)
2022-05-07 13:53:24,497 INFO 	Epoch: [75][90/309]	Batch time 2.4886 (2.6875)	Loss 0.34455 (0.32238)
2022-05-07 13:53:50,318 INFO 	Epoch: [75][100/309]	Batch time 2.4093 (2.6771)	Loss 0.35209 (0.32316)
2022-05-07 13:54:15,734 INFO 	Epoch: [75][110/309]	Batch time 2.6990 (2.6649)	Loss 0.31075 (0.32302)
2022-05-07 13:54:42,711 INFO 	Epoch: [75][120/309]	Batch time 2.3601 (2.6676)	Loss 0.33025 (0.32297)
2022-05-07 13:55:08,968 INFO 	Epoch: [75][130/309]	Batch time 3.0202 (2.6644)	Loss 0.26818 (0.32286)
2022-05-07 13:55:34,735 INFO 	Epoch: [75][140/309]	Batch time 2.5008 (2.6582)	Loss 0.30815 (0.32324)
2022-05-07 13:56:00,916 INFO 	Epoch: [75][150/309]	Batch time 2.6546 (2.6555)	Loss 0.32043 (0.32344)
2022-05-07 13:56:27,653 INFO 	Epoch: [75][160/309]	Batch time 2.7324 (2.6566)	Loss 0.32075 (0.32298)
2022-05-07 13:56:54,065 INFO 	Epoch: [75][170/309]	Batch time 2.7470 (2.6557)	Loss 0.32605 (0.32360)
2022-05-07 13:57:20,924 INFO 	Epoch: [75][180/309]	Batch time 2.6937 (2.6574)	Loss 0.33501 (0.32364)
2022-05-07 13:57:46,762 INFO 	Epoch: [75][190/309]	Batch time 2.6830 (2.6535)	Loss 0.33067 (0.32378)
2022-05-07 13:58:13,282 INFO 	Epoch: [75][200/309]	Batch time 2.8894 (2.6535)	Loss 0.30519 (0.32392)
2022-05-07 13:58:39,221 INFO 	Epoch: [75][210/309]	Batch time 2.4633 (2.6506)	Loss 0.34259 (0.32389)
2022-05-07 13:59:04,028 INFO 	Epoch: [75][220/309]	Batch time 2.3634 (2.6430)	Loss 0.35906 (0.32384)
2022-05-07 13:59:29,545 INFO 	Epoch: [75][230/309]	Batch time 2.4478 (2.6390)	Loss 0.35195 (0.32389)
2022-05-07 13:59:56,864 INFO 	Epoch: [75][240/309]	Batch time 2.6820 (2.6429)	Loss 0.32037 (0.32363)
2022-05-07 14:00:22,539 INFO 	Epoch: [75][250/309]	Batch time 2.5832 (2.6399)	Loss 0.33299 (0.32393)
2022-05-07 14:00:48,230 INFO 	Epoch: [75][260/309]	Batch time 2.5195 (2.6371)	Loss 0.32220 (0.32443)
2022-05-07 14:01:14,627 INFO 	Epoch: [75][270/309]	Batch time 2.9498 (2.6372)	Loss 0.27384 (0.32398)
2022-05-07 14:01:40,948 INFO 	Epoch: [75][280/309]	Batch time 2.4834 (2.6371)	Loss 0.29825 (0.32359)
2022-05-07 14:02:06,111 INFO 	Epoch: [75][290/309]	Batch time 2.4214 (2.6329)	Loss 0.34870 (0.32406)
2022-05-07 14:02:31,250 INFO 	Epoch: [75][300/309]	Batch time 2.3122 (2.6290)	Loss 0.36133 (0.32421)

Learning rate: 0.0003334041037343145
Step num: 23484

2022-05-07 14:02:55,092 INFO 	
Validation Loss 0.34265 (0.32293)


Epochs since last improvement: 2

save mel done
2022-05-07 14:03:04,135 INFO 	Epoch: [76][0/309]	Batch time 4.4213 (4.4213)	Loss 0.31862 (0.31862)
2022-05-07 14:03:31,138 INFO 	Epoch: [76][10/309]	Batch time 2.6729 (2.8568)	Loss 0.34442 (0.32303)
2022-05-07 14:03:58,569 INFO 	Epoch: [76][20/309]	Batch time 2.7327 (2.8026)	Loss 0.31713 (0.31777)
2022-05-07 14:04:23,640 INFO 	Epoch: [76][30/309]	Batch time 2.6784 (2.7073)	Loss 0.33548 (0.32316)
2022-05-07 14:04:49,354 INFO 	Epoch: [76][40/309]	Batch time 2.4313 (2.6741)	Loss 0.37349 (0.32725)
2022-05-07 14:05:14,319 INFO 	Epoch: [76][50/309]	Batch time 2.5640 (2.6393)	Loss 0.33201 (0.32931)
2022-05-07 14:05:40,202 INFO 	Epoch: [76][60/309]	Batch time 2.5028 (2.6309)	Loss 0.29713 (0.32792)
2022-05-07 14:06:07,797 INFO 	Epoch: [76][70/309]	Batch time 2.5796 (2.6491)	Loss 0.35232 (0.32624)
2022-05-07 14:06:34,564 INFO 	Epoch: [76][80/309]	Batch time 2.6782 (2.6525)	Loss 0.29562 (0.32605)
2022-05-07 14:07:01,040 INFO 	Epoch: [76][90/309]	Batch time 2.4421 (2.6519)	Loss 0.35293 (0.32604)
2022-05-07 14:07:28,027 INFO 	Epoch: [76][100/309]	Batch time 3.2023 (2.6566)	Loss 0.27547 (0.32493)
2022-05-07 14:07:55,175 INFO 	Epoch: [76][110/309]	Batch time 2.9634 (2.6618)	Loss 0.29174 (0.32452)
2022-05-07 14:08:20,686 INFO 	Epoch: [76][120/309]	Batch time 2.6133 (2.6527)	Loss 0.33076 (0.32508)
2022-05-07 14:08:46,448 INFO 	Epoch: [76][130/309]	Batch time 2.5027 (2.6468)	Loss 0.31019 (0.32537)
2022-05-07 14:09:12,034 INFO 	Epoch: [76][140/309]	Batch time 2.6945 (2.6406)	Loss 0.31106 (0.32608)
2022-05-07 14:09:37,721 INFO 	Epoch: [76][150/309]	Batch time 2.6773 (2.6358)	Loss 0.33298 (0.32622)
2022-05-07 14:10:04,594 INFO 	Epoch: [76][160/309]	Batch time 3.0145 (2.6390)	Loss 0.28568 (0.32550)
2022-05-07 14:10:30,634 INFO 	Epoch: [76][170/309]	Batch time 2.3185 (2.6370)	Loss 0.36670 (0.32526)
2022-05-07 14:10:56,542 INFO 	Epoch: [76][180/309]	Batch time 2.6004 (2.6344)	Loss 0.36893 (0.32522)
2022-05-07 14:11:23,470 INFO 	Epoch: [76][190/309]	Batch time 2.5725 (2.6375)	Loss 0.33754 (0.32487)
2022-05-07 14:11:50,215 INFO 	Epoch: [76][200/309]	Batch time 2.7257 (2.6393)	Loss 0.31559 (0.32443)
2022-05-07 14:12:16,126 INFO 	Epoch: [76][210/309]	Batch time 2.7219 (2.6370)	Loss 0.29516 (0.32462)
2022-05-07 14:12:41,958 INFO 	Epoch: [76][220/309]	Batch time 2.3485 (2.6346)	Loss 0.36771 (0.32509)
2022-05-07 14:13:08,044 INFO 	Epoch: [76][230/309]	Batch time 2.6544 (2.6335)	Loss 0.33830 (0.32468)
2022-05-07 14:13:33,968 INFO 	Epoch: [76][240/309]	Batch time 2.6397 (2.6318)	Loss 0.32443 (0.32460)
2022-05-07 14:13:59,976 INFO 	Epoch: [76][250/309]	Batch time 2.7081 (2.6305)	Loss 0.29119 (0.32445)
2022-05-07 14:14:25,883 INFO 	Epoch: [76][260/309]	Batch time 2.6841 (2.6290)	Loss 0.32593 (0.32477)
2022-05-07 14:14:51,993 INFO 	Epoch: [76][270/309]	Batch time 2.2955 (2.6283)	Loss 0.38243 (0.32520)
2022-05-07 14:15:17,479 INFO 	Epoch: [76][280/309]	Batch time 2.3802 (2.6255)	Loss 0.35075 (0.32522)
2022-05-07 14:15:42,575 INFO 	Epoch: [76][290/309]	Batch time 2.5802 (2.6215)	Loss 0.33123 (0.32564)
2022-05-07 14:16:08,956 INFO 	Epoch: [76][300/309]	Batch time 2.7409 (2.6221)	Loss 0.30685 (0.32545)

Learning rate: 0.0003232594588633663
Step num: 23793

2022-05-07 14:16:33,277 INFO 	
Validation Loss 0.35274 (0.32764)


Epochs since last improvement: 3

save mel done
2022-05-07 14:16:42,111 INFO 	Epoch: [77][0/309]	Batch time 4.4117 (4.4117)	Loss 0.30681 (0.30681)
2022-05-07 14:17:07,875 INFO 	Epoch: [77][10/309]	Batch time 3.1025 (2.7432)	Loss 0.31414 (0.32820)
2022-05-07 14:17:34,526 INFO 	Epoch: [77][20/309]	Batch time 2.9706 (2.7061)	Loss 0.32659 (0.33041)
2022-05-07 14:18:02,764 INFO 	Epoch: [77][30/309]	Batch time 2.7139 (2.7440)	Loss 0.34634 (0.32715)
2022-05-07 14:18:28,549 INFO 	Epoch: [77][40/309]	Batch time 2.3843 (2.7036)	Loss 0.33772 (0.32763)
2022-05-07 14:18:54,891 INFO 	Epoch: [77][50/309]	Batch time 2.5036 (2.6900)	Loss 0.31648 (0.32577)
2022-05-07 14:19:21,044 INFO 	Epoch: [77][60/309]	Batch time 2.5857 (2.6778)	Loss 0.32622 (0.32496)
2022-05-07 14:19:47,045 INFO 	Epoch: [77][70/309]	Batch time 2.5346 (2.6668)	Loss 0.35569 (0.32637)
2022-05-07 14:20:12,687 INFO 	Epoch: [77][80/309]	Batch time 2.3567 (2.6542)	Loss 0.36985 (0.32672)
2022-05-07 14:20:38,672 INFO 	Epoch: [77][90/309]	Batch time 2.6526 (2.6481)	Loss 0.32948 (0.32721)
2022-05-07 14:21:05,199 INFO 	Epoch: [77][100/309]	Batch time 2.5274 (2.6485)	Loss 0.34898 (0.32657)
2022-05-07 14:21:31,635 INFO 	Epoch: [77][110/309]	Batch time 2.6484 (2.6481)	Loss 0.32957 (0.32617)
2022-05-07 14:21:57,789 INFO 	Epoch: [77][120/309]	Batch time 2.5894 (2.6454)	Loss 0.37481 (0.32624)
2022-05-07 14:22:23,858 INFO 	Epoch: [77][130/309]	Batch time 2.5480 (2.6424)	Loss 0.32362 (0.32650)
2022-05-07 14:22:51,030 INFO 	Epoch: [77][140/309]	Batch time 2.7827 (2.6477)	Loss 0.31766 (0.32569)
2022-05-07 14:23:16,932 INFO 	Epoch: [77][150/309]	Batch time 2.7322 (2.6439)	Loss 0.32774 (0.32576)
2022-05-07 14:23:44,036 INFO 	Epoch: [77][160/309]	Batch time 2.8260 (2.6481)	Loss 0.29466 (0.32480)
2022-05-07 14:24:10,337 INFO 	Epoch: [77][170/309]	Batch time 3.0353 (2.6470)	Loss 0.27306 (0.32473)
2022-05-07 14:24:36,643 INFO 	Epoch: [77][180/309]	Batch time 3.3135 (2.6461)	Loss 0.26635 (0.32488)
2022-05-07 14:25:02,639 INFO 	Epoch: [77][190/309]	Batch time 2.5080 (2.6437)	Loss 0.32837 (0.32562)
2022-05-07 14:25:28,581 INFO 	Epoch: [77][200/309]	Batch time 2.6942 (2.6412)	Loss 0.31894 (0.32542)
2022-05-07 14:25:54,953 INFO 	Epoch: [77][210/309]	Batch time 2.5520 (2.6410)	Loss 0.34199 (0.32568)
2022-05-07 14:26:21,006 INFO 	Epoch: [77][220/309]	Batch time 2.4123 (2.6394)	Loss 0.35693 (0.32563)
2022-05-07 14:26:47,496 INFO 	Epoch: [77][230/309]	Batch time 2.5187 (2.6398)	Loss 0.35250 (0.32524)
2022-05-07 14:27:13,450 INFO 	Epoch: [77][240/309]	Batch time 2.4076 (2.6380)	Loss 0.33862 (0.32528)
2022-05-07 14:27:38,972 INFO 	Epoch: [77][250/309]	Batch time 2.6130 (2.6346)	Loss 0.28977 (0.32509)
2022-05-07 14:28:04,960 INFO 	Epoch: [77][260/309]	Batch time 2.5055 (2.6332)	Loss 0.34054 (0.32481)
2022-05-07 14:28:31,051 INFO 	Epoch: [77][270/309]	Batch time 2.7130 (2.6323)	Loss 0.29427 (0.32491)
2022-05-07 14:28:57,571 INFO 	Epoch: [77][280/309]	Batch time 2.6379 (2.6330)	Loss 0.33181 (0.32514)
2022-05-07 14:29:24,623 INFO 	Epoch: [77][290/309]	Batch time 2.6454 (2.6355)	Loss 0.32761 (0.32499)
2022-05-07 14:29:51,107 INFO 	Epoch: [77][300/309]	Batch time 2.5432 (2.6359)	Loss 0.31710 (0.32489)

Learning rate: 0.0003134234899157344
Step num: 24102

2022-05-07 14:30:16,421 INFO 	
Validation Loss 0.34369 (0.32087)

save mel done
2022-05-07 14:30:27,933 INFO 	Epoch: [78][0/309]	Batch time 4.4864 (4.4864)	Loss 0.29342 (0.29342)
2022-05-07 14:30:54,231 INFO 	Epoch: [78][10/309]	Batch time 2.9750 (2.7986)	Loss 0.29046 (0.31819)
2022-05-07 14:31:20,867 INFO 	Epoch: [78][20/309]	Batch time 2.5496 (2.7343)	Loss 0.33648 (0.32065)
2022-05-07 14:31:46,702 INFO 	Epoch: [78][30/309]	Batch time 2.6483 (2.6857)	Loss 0.32112 (0.32208)
2022-05-07 14:32:12,060 INFO 	Epoch: [78][40/309]	Batch time 2.5350 (2.6491)	Loss 0.36335 (0.32625)
2022-05-07 14:32:40,295 INFO 	Epoch: [78][50/309]	Batch time 2.8648 (2.6833)	Loss 0.29231 (0.32250)
2022-05-07 14:33:04,813 INFO 	Epoch: [78][60/309]	Batch time 2.5411 (2.6454)	Loss 0.33966 (0.32417)
2022-05-07 14:33:32,440 INFO 	Epoch: [78][70/309]	Batch time 2.8471 (2.6619)	Loss 0.29128 (0.32353)
2022-05-07 14:33:57,995 INFO 	Epoch: [78][80/309]	Batch time 2.2947 (2.6488)	Loss 0.34501 (0.32390)
2022-05-07 14:34:23,350 INFO 	Epoch: [78][90/309]	Batch time 2.6414 (2.6363)	Loss 0.35024 (0.32480)
2022-05-07 14:34:49,945 INFO 	Epoch: [78][100/309]	Batch time 2.6030 (2.6386)	Loss 0.31030 (0.32429)
2022-05-07 14:35:15,900 INFO 	Epoch: [78][110/309]	Batch time 2.3750 (2.6347)	Loss 0.36040 (0.32393)
2022-05-07 14:35:42,539 INFO 	Epoch: [78][120/309]	Batch time 2.9371 (2.6371)	Loss 0.29845 (0.32263)
2022-05-07 14:36:08,046 INFO 	Epoch: [78][130/309]	Batch time 2.3960 (2.6305)	Loss 0.32896 (0.32329)
2022-05-07 14:36:33,314 INFO 	Epoch: [78][140/309]	Batch time 2.6040 (2.6232)	Loss 0.30735 (0.32400)
2022-05-07 14:37:00,389 INFO 	Epoch: [78][150/309]	Batch time 2.6232 (2.6288)	Loss 0.33812 (0.32247)
2022-05-07 14:37:27,150 INFO 	Epoch: [78][160/309]	Batch time 2.7744 (2.6317)	Loss 0.27878 (0.32201)
2022-05-07 14:37:53,647 INFO 	Epoch: [78][170/309]	Batch time 2.6800 (2.6328)	Loss 0.29943 (0.32134)
2022-05-07 14:38:19,031 INFO 	Epoch: [78][180/309]	Batch time 2.7551 (2.6275)	Loss 0.28067 (0.32130)
2022-05-07 14:38:45,549 INFO 	Epoch: [78][190/309]	Batch time 2.6098 (2.6288)	Loss 0.31295 (0.32075)
2022-05-07 14:39:10,777 INFO 	Epoch: [78][200/309]	Batch time 2.5786 (2.6235)	Loss 0.32007 (0.32121)
2022-05-07 14:39:36,401 INFO 	Epoch: [78][210/309]	Batch time 2.3829 (2.6206)	Loss 0.37197 (0.32130)
2022-05-07 14:40:02,225 INFO 	Epoch: [78][220/309]	Batch time 2.6601 (2.6189)	Loss 0.28940 (0.32122)
2022-05-07 14:40:28,229 INFO 	Epoch: [78][230/309]	Batch time 2.7285 (2.6181)	Loss 0.30122 (0.32140)
2022-05-07 14:40:54,237 INFO 	Epoch: [78][240/309]	Batch time 2.4862 (2.6174)	Loss 0.33757 (0.32103)
2022-05-07 14:41:20,057 INFO 	Epoch: [78][250/309]	Batch time 2.4313 (2.6160)	Loss 0.33976 (0.32086)
2022-05-07 14:41:44,971 INFO 	Epoch: [78][260/309]	Batch time 2.5833 (2.6112)	Loss 0.36885 (0.32120)
2022-05-07 14:42:11,099 INFO 	Epoch: [78][270/309]	Batch time 2.3837 (2.6113)	Loss 0.31635 (0.32130)
2022-05-07 14:42:36,564 INFO 	Epoch: [78][280/309]	Batch time 2.5944 (2.6090)	Loss 0.37428 (0.32217)
2022-05-07 14:43:01,851 INFO 	Epoch: [78][290/309]	Batch time 2.4968 (2.6062)	Loss 0.32750 (0.32237)
2022-05-07 14:43:27,662 INFO 	Epoch: [78][300/309]	Batch time 2.7483 (2.6054)	Loss 0.30547 (0.32281)

Learning rate: 0.00030388680466262756
Step num: 24411

2022-05-07 14:43:51,812 INFO 	
Validation Loss 0.34124 (0.31932)

save mel done
2022-05-07 14:44:03,000 INFO 	Epoch: [79][0/309]	Batch time 4.2650 (4.2650)	Loss 0.29936 (0.29936)
2022-05-07 14:44:29,474 INFO 	Epoch: [79][10/309]	Batch time 2.6141 (2.7944)	Loss 0.31654 (0.31618)
2022-05-07 14:44:54,757 INFO 	Epoch: [79][20/309]	Batch time 2.5509 (2.6678)	Loss 0.31609 (0.32088)
2022-05-07 14:45:20,762 INFO 	Epoch: [79][30/309]	Batch time 2.8949 (2.6460)	Loss 0.31339 (0.31962)
2022-05-07 14:45:46,391 INFO 	Epoch: [79][40/309]	Batch time 2.6523 (2.6258)	Loss 0.33001 (0.32360)
2022-05-07 14:46:12,984 INFO 	Epoch: [79][50/309]	Batch time 2.4510 (2.6323)	Loss 0.34753 (0.32365)
2022-05-07 14:46:38,426 INFO 	Epoch: [79][60/309]	Batch time 2.3671 (2.6179)	Loss 0.38705 (0.32252)
2022-05-07 14:47:04,649 INFO 	Epoch: [79][70/309]	Batch time 3.0172 (2.6185)	Loss 0.28890 (0.32271)
2022-05-07 14:47:30,896 INFO 	Epoch: [79][80/309]	Batch time 2.8317 (2.6193)	Loss 0.27766 (0.32168)
2022-05-07 14:47:57,458 INFO 	Epoch: [79][90/309]	Batch time 2.3636 (2.6233)	Loss 0.32788 (0.32028)
2022-05-07 14:48:23,784 INFO 	Epoch: [79][100/309]	Batch time 2.5224 (2.6242)	Loss 0.35721 (0.31915)
2022-05-07 14:48:49,981 INFO 	Epoch: [79][110/309]	Batch time 2.6775 (2.6238)	Loss 0.33729 (0.32089)
2022-05-07 14:49:16,016 INFO 	Epoch: [79][120/309]	Batch time 2.8403 (2.6222)	Loss 0.30598 (0.32070)
2022-05-07 14:49:41,968 INFO 	Epoch: [79][130/309]	Batch time 2.8892 (2.6201)	Loss 0.31666 (0.32038)
2022-05-07 14:50:09,022 INFO 	Epoch: [79][140/309]	Batch time 2.8606 (2.6262)	Loss 0.30019 (0.31968)
2022-05-07 14:50:34,914 INFO 	Epoch: [79][150/309]	Batch time 2.5240 (2.6237)	Loss 0.31526 (0.31953)
2022-05-07 14:51:01,114 INFO 	Epoch: [79][160/309]	Batch time 2.6422 (2.6235)	Loss 0.33563 (0.31950)
2022-05-07 14:51:28,075 INFO 	Epoch: [79][170/309]	Batch time 2.6194 (2.6277)	Loss 0.33869 (0.31953)
2022-05-07 14:51:53,213 INFO 	Epoch: [79][180/309]	Batch time 2.7447 (2.6214)	Loss 0.28740 (0.32022)
2022-05-07 14:52:19,298 INFO 	Epoch: [79][190/309]	Batch time 2.4932 (2.6207)	Loss 0.32790 (0.32034)
2022-05-07 14:52:45,959 INFO 	Epoch: [79][200/309]	Batch time 2.9112 (2.6230)	Loss 0.29762 (0.32064)
2022-05-07 14:53:13,110 INFO 	Epoch: [79][210/309]	Batch time 2.9981 (2.6274)	Loss 0.29254 (0.32024)
2022-05-07 14:53:39,094 INFO 	Epoch: [79][220/309]	Batch time 2.5620 (2.6261)	Loss 0.32807 (0.32079)
2022-05-07 14:54:07,234 INFO 	Epoch: [79][230/309]	Batch time 2.6963 (2.6342)	Loss 0.33475 (0.32034)
2022-05-07 14:54:32,602 INFO 	Epoch: [79][240/309]	Batch time 2.5133 (2.6302)	Loss 0.33979 (0.32097)
2022-05-07 14:54:58,841 INFO 	Epoch: [79][250/309]	Batch time 2.6457 (2.6299)	Loss 0.30106 (0.32098)
2022-05-07 14:55:24,959 INFO 	Epoch: [79][260/309]	Batch time 2.6145 (2.6292)	Loss 0.32775 (0.32072)
2022-05-07 14:55:51,918 INFO 	Epoch: [79][270/309]	Batch time 2.7674 (2.6317)	Loss 0.28773 (0.32044)
2022-05-07 14:56:18,407 INFO 	Epoch: [79][280/309]	Batch time 2.5839 (2.6323)	Loss 0.35233 (0.32052)
2022-05-07 14:56:45,626 INFO 	Epoch: [79][290/309]	Batch time 2.6169 (2.6354)	Loss 0.33092 (0.32047)
2022-05-07 14:57:11,567 INFO 	Epoch: [79][300/309]	Batch time 2.4800 (2.6340)	Loss 0.32051 (0.32071)

Learning rate: 0.00029464029665705654
Step num: 24720

2022-05-07 14:57:37,186 INFO 	
Validation Loss 0.34394 (0.32165)


Epochs since last improvement: 1

save mel done
2022-05-07 14:57:45,907 INFO 	Epoch: [80][0/309]	Batch time 4.2091 (4.2091)	Loss 0.29376 (0.29376)
2022-05-07 14:58:12,329 INFO 	Epoch: [80][10/309]	Batch time 2.6682 (2.7847)	Loss 0.32946 (0.31259)
2022-05-07 14:58:38,282 INFO 	Epoch: [80][20/309]	Batch time 2.6317 (2.6945)	Loss 0.31663 (0.31924)
2022-05-07 14:59:03,246 INFO 	Epoch: [80][30/309]	Batch time 2.6790 (2.6306)	Loss 0.30520 (0.32161)
2022-05-07 14:59:29,238 INFO 	Epoch: [80][40/309]	Batch time 2.6857 (2.6229)	Loss 0.32216 (0.32436)
2022-05-07 14:59:55,007 INFO 	Epoch: [80][50/309]	Batch time 2.4990 (2.6139)	Loss 0.33552 (0.32451)
2022-05-07 15:00:20,661 INFO 	Epoch: [80][60/309]	Batch time 2.4105 (2.6059)	Loss 0.31301 (0.32392)
2022-05-07 15:00:48,138 INFO 	Epoch: [80][70/309]	Batch time 2.3837 (2.6259)	Loss 0.33169 (0.32511)
2022-05-07 15:01:14,996 INFO 	Epoch: [80][80/309]	Batch time 2.5544 (2.6333)	Loss 0.31355 (0.32358)
2022-05-07 15:01:41,447 INFO 	Epoch: [80][90/309]	Batch time 2.6470 (2.6346)	Loss 0.30182 (0.32302)
2022-05-07 15:02:07,657 INFO 	Epoch: [80][100/309]	Batch time 3.1636 (2.6333)	Loss 0.30370 (0.32349)
2022-05-07 15:02:34,448 INFO 	Epoch: [80][110/309]	Batch time 3.0385 (2.6374)	Loss 0.27118 (0.32269)
2022-05-07 15:03:00,788 INFO 	Epoch: [80][120/309]	Batch time 3.1221 (2.6371)	Loss 0.30738 (0.32397)
2022-05-07 15:03:27,120 INFO 	Epoch: [80][130/309]	Batch time 2.9107 (2.6368)	Loss 0.28198 (0.32363)
2022-05-07 15:03:54,224 INFO 	Epoch: [80][140/309]	Batch time 2.4722 (2.6420)	Loss 0.33592 (0.32272)
2022-05-07 15:04:19,641 INFO 	Epoch: [80][150/309]	Batch time 2.4795 (2.6354)	Loss 0.34535 (0.32317)
2022-05-07 15:04:46,324 INFO 	Epoch: [80][160/309]	Batch time 2.7154 (2.6374)	Loss 0.29129 (0.32252)
2022-05-07 15:05:11,846 INFO 	Epoch: [80][170/309]	Batch time 2.6287 (2.6324)	Loss 0.30933 (0.32225)
2022-05-07 15:05:37,498 INFO 	Epoch: [80][180/309]	Batch time 2.3274 (2.6287)	Loss 0.34882 (0.32241)
2022-05-07 15:06:03,982 INFO 	Epoch: [80][190/309]	Batch time 2.4179 (2.6298)	Loss 0.33279 (0.32178)
2022-05-07 15:06:29,892 INFO 	Epoch: [80][200/309]	Batch time 2.6073 (2.6278)	Loss 0.32564 (0.32198)
2022-05-07 15:06:55,033 INFO 	Epoch: [80][210/309]	Batch time 2.6216 (2.6224)	Loss 0.31772 (0.32239)
2022-05-07 15:07:22,195 INFO 	Epoch: [80][220/309]	Batch time 2.8895 (2.6267)	Loss 0.30422 (0.32184)
2022-05-07 15:07:49,481 INFO 	Epoch: [80][230/309]	Batch time 3.0263 (2.6311)	Loss 0.28393 (0.32159)
2022-05-07 15:08:16,066 INFO 	Epoch: [80][240/309]	Batch time 2.8255 (2.6322)	Loss 0.30631 (0.32156)
2022-05-07 15:08:42,340 INFO 	Epoch: [80][250/309]	Batch time 2.4172 (2.6320)	Loss 0.35896 (0.32174)
2022-05-07 15:09:08,834 INFO 	Epoch: [80][260/309]	Batch time 2.6939 (2.6327)	Loss 0.32533 (0.32185)
2022-05-07 15:09:35,178 INFO 	Epoch: [80][270/309]	Batch time 2.6954 (2.6328)	Loss 0.29551 (0.32150)
2022-05-07 15:10:02,247 INFO 	Epoch: [80][280/309]	Batch time 2.8017 (2.6354)	Loss 0.31062 (0.32117)
2022-05-07 15:10:28,037 INFO 	Epoch: [80][290/309]	Batch time 2.6169 (2.6335)	Loss 0.31475 (0.32119)
2022-05-07 15:10:54,282 INFO 	Epoch: [80][300/309]	Batch time 2.4444 (2.6332)	Loss 0.31357 (0.32084)

Learning rate: 0.0002856751365382159
Step num: 25029

2022-05-07 15:11:19,110 INFO 	
Validation Loss 0.33862 (0.31819)

save mel done
2022-05-07 15:11:30,495 INFO 	Epoch: [81][0/309]	Batch time 4.5474 (4.5474)	Loss 0.28738 (0.28738)
2022-05-07 15:11:59,282 INFO 	Epoch: [81][10/309]	Batch time 2.8610 (3.0304)	Loss 0.33399 (0.31159)
